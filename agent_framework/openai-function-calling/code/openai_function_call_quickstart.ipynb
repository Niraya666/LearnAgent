{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMcE4Hjqex3A",
        "outputId": "a4407752-f79d-4806-b9c2-f13c1bc5f9b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install scipy --quiet\n",
        "!pip install tenacity --quiet\n",
        "!pip install tiktoken --quiet\n",
        "!pip install termcolor --quiet\n",
        "!pip install openai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "AZURE_OPENAI_API_KEY = userdata.get('AZURE_OPENAI_API_KEY')\n",
        "OPENAI_API_VERSION = userdata.get('AZURE_OPENAI_API_VERSION')\n",
        "AZURE_OPENAI_ENDPOINT = userdata.get('AZURE_OPENAI_API_BASE')\n",
        "deployment_name = userdata.get('fast_llm_model_deployment_id')\n",
        "\n",
        "import os\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = AZURE_OPENAI_API_KEY\n",
        "os.environ[\"OPENAI_API_VERSION\"] = OPENAI_API_VERSION\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = AZURE_OPENAI_ENDPOINT\n",
        "os.environ[\"OPENAI_API_TYPE\"] = 'azure'\n",
        "\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('AZURE_OPENAI_API_KEY')\n",
        "os.environ[\"OPENAI_API_BASE\"] = userdata.get('AZURE_OPENAI_API_BASE')\n",
        "\n"
      ],
      "metadata": {
        "id": "9PY4IgX7e6bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import AzureOpenAI\n",
        "client = AzureOpenAI()"
      ],
      "metadata": {
        "id": "JYxiuSZmft1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
        "from termcolor import colored"
      ],
      "metadata": {
        "id": "DYBmw2JHf3cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utilities"
      ],
      "metadata": {
        "id": "DVwepviigAVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##  define a few utilities for making calls to the Chat Completions API and for maintaining and keeping track of the conversation state.\n",
        "@retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3))\n",
        "def chat_completion_request(messages, tools=None, tool_choice=None, model=deployment_name):\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            tools=tools,\n",
        "            tool_choice=tool_choice,\n",
        "        )\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(\"Unable to generate ChatCompletion response\")\n",
        "        print(f\"Exception: {e}\")\n",
        "        return e\n"
      ],
      "metadata": {
        "id": "shgj1c92f8yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pretty_print_conversation(messages):\n",
        "    role_to_color = {\n",
        "        \"system\": \"red\",\n",
        "        \"user\": \"green\",\n",
        "        \"assistant\": \"blue\",\n",
        "        \"function\": \"magenta\",\n",
        "    }\n",
        "\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"system\":\n",
        "            print(colored(f\"system: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
        "        elif message[\"role\"] == \"user\":\n",
        "            print(colored(f\"user: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
        "        elif message[\"role\"] == \"assistant\" and message.get(\"function_call\"):\n",
        "            print(colored(f\"assistant: {message['function_call']}\\n\", role_to_color[message[\"role\"]]))\n",
        "        elif message[\"role\"] == \"assistant\" and not message.get(\"function_call\"):\n",
        "            print(colored(f\"assistant: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
        "        elif message[\"role\"] == \"function\":\n",
        "            print(colored(f\"function ({message['name']}): {message['content']}\\n\", role_to_color[message[\"role\"]]))\n"
      ],
      "metadata": {
        "id": "YWYLSiBsgrvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic concepts"
      ],
      "metadata": {
        "id": "NpkWYdZ3g1Ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## create some function specifications to interface with a hypothetical weather API\n",
        "\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_current_weather\",\n",
        "            \"description\": \"Get the current weather\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
        "                    },\n",
        "                    \"format\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
        "                        \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"location\", \"format\"],\n",
        "            },\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_n_day_weather_forecast\",\n",
        "            \"description\": \"Get an N-day weather forecast\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
        "                    },\n",
        "                    \"format\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
        "                        \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
        "                    },\n",
        "                    \"num_days\": {\n",
        "                        \"type\": \"integer\",\n",
        "                        \"description\": \"The number of days to forecast\",\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"location\", \"format\", \"num_days\"]\n",
        "            },\n",
        "        }\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "A7NAr7FDgx75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## If we prompt the model about the current weather, it will respond with some clarifying questions.\n",
        "messages = []\n",
        "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"What's the weather like today\"})\n",
        "chat_response = chat_completion_request(\n",
        "    messages, tools=tools\n",
        ")\n",
        "assistant_message = chat_response.choices[0].message\n",
        "messages.append(assistant_message)\n",
        "assistant_message\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nDfCHAsg0Yc",
        "outputId": "6816fd03-e56d-4821-d7d0-b0806986ea65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletionMessage(content='Sure, may I know your current location?', role='assistant', function_call=None, tool_calls=None)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Once we provide the missing information, it will generate the appropriate function arguments for us.\n",
        "messages.append({\"role\": \"user\", \"content\": \"I'm in Shanghai, China.\"})\n",
        "chat_response = chat_completion_request(\n",
        "    messages, tools=tools\n",
        ")\n",
        "assistant_message = chat_response.choices[0].message\n",
        "messages.append(assistant_message)\n",
        "assistant_message\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtnKYCsQhS3r",
        "outputId": "1bb3b8b9-2cbe-48b3-bf80-f7b857a78e29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_VdqOOMp9pagf5ho39Y2HmYV4', function=Function(arguments='{\\n  \"location\": \"San Francisco, CA\",\\n  \"format\": \"celsius\",\\n  \"num_days\": 4\\n}', name='get_n_day_weather_forecast'), type='function')])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## get it to target the other function we've told it about.\n",
        "\n",
        "messages = []\n",
        "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"what is the weather going to be like in Glasgow, Scotland over the next x days\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"in 5 days\"})\n",
        "\n",
        "chat_response = chat_completion_request(\n",
        "    messages, tools=tools\n",
        ")\n",
        "assistant_message = chat_response.choices[0].message\n",
        "messages.append(assistant_message)\n",
        "assistant_message\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjwmhIl7hao4",
        "outputId": "09500de7-e446-4481-f1d7-f2dd9827f7fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_jYEPycIK4RvjiYr4QX529Lso', function=Function(arguments='{\\n  \"location\": \"Glasgow, Scotland\",\\n  \"format\": \"celsius\",\\n  \"num_days\": 5\\n}', name='get_n_day_weather_forecast'), type='function')])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forcing the use of specific functions or no function"
      ],
      "metadata": {
        "id": "4gP1ubcZiGw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## We can force the model to use a specific function,\n",
        "## for example get_n_day_weather_forecast by using the function_call argument.\n",
        "## By doing so, we force the model to make assumptions about how to use it.\n",
        "\n",
        "# in this cell we force the model to use get_n_day_weather_forecast\n",
        "messages = []\n",
        "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"Give me a weather report for Toronto, Canada.\"})\n",
        "chat_response = chat_completion_request(\n",
        "    messages, tools=tools, tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_n_day_weather_forecast\"}}\n",
        ")\n",
        "chat_response.choices[0].message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUngVy9Uh4D-",
        "outputId": "774394bf-6c1d-492f-f7c6-c4f2da47651b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_0ldecDpV8Vdq8mGPoUewlue3', function=Function(arguments='{\\n  \"location\": \"Toronto, Canada\",\\n  \"format\": \"celsius\",\\n  \"num_days\": 1\\n}', name='get_n_day_weather_forecast'), type='function')])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## We can also force the model to not use a function at all.\n",
        "\n",
        "messages = []\n",
        "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"Give me the current weather (use Celcius) for Toronto, Canada.\"})\n",
        "chat_response = chat_completion_request(\n",
        "    messages, tools=tools, tool_choice=\"none\"\n",
        ")\n",
        "chat_response.choices[0].message\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llzxlPHciQh9",
        "outputId": "4ae58399-ae25-4699-88f0-f77279a47e42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletionMessage(content='{\\n  \"location\": \"Toronto, Canada\",\\n  \"format\": \"celsius\"\\n}', role='assistant', function_call=None, tool_calls=None)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallel Function Calling"
      ],
      "metadata": {
        "id": "25tT4v8JijCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##  call multiple functions in one turn.\n",
        "\n",
        "messages = []\n",
        "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"what is the weather going to be like in San Francisco and Glasgow over the next 4 days\"})\n",
        "chat_response = chat_completion_request(\n",
        "    messages, tools=tools, model = \"gpt4turbo\"\n",
        ")\n",
        "\n",
        "assistant_message = chat_response.choices[0].message.tool_calls\n",
        "assistant_message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "032Xstrjidx7",
        "outputId": "31f1119b-e494-4030-ec18-376835398d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ChatCompletionMessageToolCall(id='call_tfl8eTCW64sHvHjiiatoYzku', function=Function(arguments='{\"location\": \"San Francisco, CA\", \"format\": \"fahrenheit\", \"num_days\": 4}', name='get_n_day_weather_forecast'), type='function'),\n",
              " ChatCompletionMessageToolCall(id='call_bAqj55RygP2Y1T85RHqgskku', function=Function(arguments='{\"location\": \"Glasgow, UK\", \"format\": \"celsius\", \"num_days\": 4}', name='get_n_day_weather_forecast'), type='function')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z-LLfqOQi5vL"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "def get_n_day_weather_forecast(request):\n",
        "    \"\"\"\n",
        "    This function is for illustrative purposes.\n",
        "    The location and unit should be used to determine weather\n",
        "    instead of returning a hardcoded response.\n",
        "    \"\"\"\n",
        "    location = request.get(\"location\")\n",
        "    format = request.get(\"format\")\n",
        "    return {\"temperature\": \"22\", \"format\": format, \"description\": \"Sunny\"}\n",
        "\n",
        "def get_current_weather(location, format=\"fahrenheit\"):\n",
        "    \"\"\"Get the current weather in a given location\"\"\"\n",
        "    if \"tokyo\" in location.lower():\n",
        "        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"format\": format})\n",
        "    elif \"san francisco\" in location.lower():\n",
        "        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"format\": format})\n",
        "    elif \"paris\" in location.lower():\n",
        "        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"format\": format})\n",
        "    else:\n",
        "        return json.dumps({\"location\": location, \"temperature\": \"unknown\", \"format\": format})\n",
        "\n",
        "available_functions = {\n",
        "            \"get_current_weather\": get_current_weather,\n",
        "        }"
      ],
      "metadata": {
        "id": "nSVyew9OjW9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "\n",
        "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"What's the weather like today\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"I'm in Tokyo.\"})\n",
        "chat_response = chat_completion_request(\n",
        "    messages, tools=tools,model = \"gpt4turbo\"\n",
        ")\n",
        "# assistant_message = chat_response.choices[0].message\n",
        "# messages.append(assistant_message)\n",
        "# print(assistant_message)\n"
      ],
      "metadata": {
        "id": "3PPId3cvm4dU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_message = chat_response.choices[0].message\n",
        "tool_calls = response_message.tool_calls\n",
        "for tool_call in tool_calls:\n",
        "  function_name = tool_call.function.name\n",
        "  function_to_call = available_functions[function_name]\n",
        "  function_args = json.loads(tool_call.function.arguments)\n",
        "  function_response = function_to_call(\n",
        "      location=function_args.get(\"location\"),\n",
        "      format=function_args.get(\"format\"),\n",
        "  )\n",
        "  print(function_response)\n",
        "  messages.append(\n",
        "                {\n",
        "                    \"tool_call_id\": tool_call.id,\n",
        "                    \"role\": \"tool\",\n",
        "                    \"name\": function_name,\n",
        "                    \"tool_calls\": function_response,\n",
        "                }\n",
        "            )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYDKphUqjx_r",
        "outputId": "8a49500f-b2cf-422c-83ab-60e3c9e17514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"location\": \"Tokyo\", \"temperature\": \"10\", \"format\": \"celsius\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chat_completion_request(\n",
        "#     messages, tools=tools, model = \"gpt4turbo\"\n",
        "# )\n",
        "messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7dWIkD3pVnB",
        "outputId": "49f9ebd6-6d35-4411-b4be-162da4a1a666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"},\n",
              " {'role': 'user', 'content': \"What's the weather like today\"},\n",
              " {'role': 'user', 'content': \"I'm in Tokyo.\"},\n",
              " {'tool_call_id': 'call_NARYCqOR263Dk7eRENtCJrcM',\n",
              "  'role': 'tool',\n",
              "  'name': 'get_current_weather',\n",
              "  'tool_calls': '{\"location\": \"Tokyo\", \"temperature\": \"10\", \"format\": \"celsius\"}'}]"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCdeNIf-kqcQ",
        "outputId": "7ce77804-25de-419a-d642-c0528e411d13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_xv5EqQqroL8fDcxI0SuiiE8D', function=Function(arguments='{\"location\":\"Tokyo\",\"format\":\"celsius\"}', name='get_current_weather'), type='function')])"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion_request(\n",
        "    messages, tools=tools, model = \"gpt4turbo\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_y7T1MohnMUN",
        "outputId": "8a33f302-5d1c-4675-9e14-129e6875d090"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unable to generate ChatCompletion response\n",
            "Exception: Error code: 400 - {'error': {'message': '\\'{\"location\": \"Tokyo\", \"temperature\": \"10\", \"format\": \"celsius\"}\\' is not of type \\'array\\' - \\'messages.3.tool_calls\\'', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "openai.BadRequestError('Error code: 400 - {\\'error\\': {\\'message\\': \\'\\\\\\'{\"location\": \"Tokyo\", \"temperature\": \"10\", \"format\": \"celsius\"}\\\\\\' is not of type \\\\\\'array\\\\\\' - \\\\\\'messages.3.tool_calls\\\\\\'\\', \\'type\\': \\'invalid_request_error\\', \\'param\\': None, \\'code\\': None}}')"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u8uBaf4DktnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OeP7s1S5loIg",
        "outputId": "5f301060-202e-4da2-f9ab-cb870ce92978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"temperature\": \"22\", \"format\": \"celsius\", \"description\": \"Sunny\"}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "messages.append({\"role\": \"user\", \"content\": \"What's the weather like in Tokyo!\"})\n",
        "chat_response = chat_completion_request(messages, tools=tools, tool_choice=\"auto\")\n",
        "assistant_message = chat_response.choices[0].message\n",
        "assistant_message = json.loads(assistant_message.model_dump_json())\n",
        "assistant_message[\"content\"] = str(assistant_message[\"tool_calls\"][0][\"function\"])\n",
        "\n",
        "#a temporary patch but this should be handled differently\n",
        "# remove \"function_call\" from assistant message\n",
        "del assistant_message[\"function_call\"]"
      ],
      "metadata": {
        "id": "lZmtcq5ll0SH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assistant_message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rhkyt8Xs1M5",
        "outputId": "6bd3b4af-8d61-4cc8-9946-d6c4229f0249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'content': '{\\'arguments\\': \\'{\\\\n  \"location\": \"Tokyo\",\\\\n  \"format\": \"celsius\"\\\\n}\\', \\'name\\': \\'get_current_weather\\'}',\n",
              " 'role': 'assistant',\n",
              " 'tool_calls': [{'id': 'call_Tz8S1HgvnaBzf6CFZP1u4d1J',\n",
              "   'function': {'arguments': '{\\n  \"location\": \"Tokyo\",\\n  \"format\": \"celsius\"\\n}',\n",
              "    'name': 'get_current_weather'},\n",
              "   'type': 'function'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append(assistant_message)\n",
        "messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2q5Rd5xItA70",
        "outputId": "fa0b8c86-0ba9-4199-8278-b9d4ac7b53c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'user', 'content': \"What's the weather like in Tokyo!\"},\n",
              " {'content': '{\\'arguments\\': \\'{\\\\n  \"location\": \"Tokyo\",\\\\n  \"format\": \"celsius\"\\\\n}\\', \\'name\\': \\'get_current_weather\\'}',\n",
              "  'role': 'assistant',\n",
              "  'tool_calls': [{'id': 'call_Tz8S1HgvnaBzf6CFZP1u4d1J',\n",
              "    'function': {'arguments': '{\\n  \"location\": \"Tokyo\",\\n  \"format\": \"celsius\"\\n}',\n",
              "     'name': 'get_current_weather'},\n",
              "    'type': 'function'}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the weather information to pass back to the model\n",
        "weather = get_current_weather(messages[1][\"tool_calls\"][0][\"function\"][\"arguments\"])\n",
        "\n",
        "messages.append({\"role\": \"tool\",\n",
        "                 \"tool_call_id\": assistant_message[\"tool_calls\"][0][\"id\"],\n",
        "                 \"name\": assistant_message[\"tool_calls\"][0][\"function\"][\"name\"],\n",
        "                 \"content\": weather})"
      ],
      "metadata": {
        "id": "pZwNhfzrtD0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JvFHz-PtH4x",
        "outputId": "4ce37bf7-00a9-42b7-9a6f-0bc8a083cda7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'user', 'content': \"What's the weather like in Tokyo!\"},\n",
              " {'content': '{\\'arguments\\': \\'{\\\\n  \"location\": \"Tokyo\",\\\\n  \"format\": \"celsius\"\\\\n}\\', \\'name\\': \\'get_current_weather\\'}',\n",
              "  'role': 'assistant',\n",
              "  'tool_calls': [{'id': 'call_Tz8S1HgvnaBzf6CFZP1u4d1J',\n",
              "    'function': {'arguments': '{\\n  \"location\": \"Tokyo\",\\n  \"format\": \"celsius\"\\n}',\n",
              "     'name': 'get_current_weather'},\n",
              "    'type': 'function'}]},\n",
              " {'role': 'tool',\n",
              "  'tool_call_id': 'call_Tz8S1HgvnaBzf6CFZP1u4d1J',\n",
              "  'name': 'get_current_weather',\n",
              "  'content': '{\"location\": \"Tokyo\", \"temperature\": \"10\", \"format\": \"fahrenheit\"}'}]"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_response = chat_completion_request(messages, tools=tools)\n",
        "final_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKnY8noKtI4u",
        "outputId": "3ee58b3d-f324-4180-ec27-2ec8cf0cf22a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-8zG0XOGJm9rLukaKdlea9KnoXUdDP', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The current weather in Tokyo is 10 degrees Celsius.', role='assistant', function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1709610233, model='gpt-35-turbo-16k', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=12, prompt_tokens=263, total_tokens=275), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oioDUm88tett"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cLXQbfYOBfEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "31xmKB_5BfGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to call functions with model generated arguments\n",
        "\n",
        "\n",
        "how to execute functions whose inputs are model-generated, and use this to implement an agent that can answer questions for us about a database."
      ],
      "metadata": {
        "id": "4AgPaW6wBm8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.sqlitetutorial.net/wp-content/uploads/2018/03/chinook.zip -O chinook.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ToOOgpFBfIt",
        "outputId": "ee8195a5-24a4-4bfa-9e14-a4fc416491db"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-05 05:13:15--  https://www.sqlitetutorial.net/wp-content/uploads/2018/03/chinook.zip\n",
            "Resolving www.sqlitetutorial.net (www.sqlitetutorial.net)... 104.21.30.141, 172.67.172.250, 2606:4700:3037::6815:1e8d, ...\n",
            "Connecting to www.sqlitetutorial.net (www.sqlitetutorial.net)|104.21.30.141|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 305596 (298K) [application/zip]\n",
            "Saving to: ‘chinook.zip’\n",
            "\n",
            "chinook.zip         100%[===================>] 298.43K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-03-05 05:13:15 (4.02 MB/s) - ‘chinook.zip’ saved [305596/305596]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip chinook.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WndDNiVhBfXz",
        "outputId": "b7826157-63e5-45ab-e346-48d37cca7c99"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  chinook.zip\n",
            "  inflating: chinook.db              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect(\"/content/chinook.db\")\n",
        "print(\"Opened database successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDQw41AFCBwM",
        "outputId": "a91d3d59-914b-41b4-8330-25ca529a754c"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Opened database successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_table_names(conn):\n",
        "    \"\"\"Return a list of table names.\"\"\"\n",
        "    table_names = []\n",
        "    tables = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "    for table in tables.fetchall():\n",
        "        table_names.append(table[0])\n",
        "    return table_names\n",
        "\n",
        "\n",
        "def get_column_names(conn, table_name):\n",
        "    \"\"\"Return a list of column names.\"\"\"\n",
        "    column_names = []\n",
        "    columns = conn.execute(f\"PRAGMA table_info('{table_name}');\").fetchall()\n",
        "    for col in columns:\n",
        "        column_names.append(col[1])\n",
        "    return column_names\n",
        "\n",
        "\n",
        "def get_database_info(conn):\n",
        "    \"\"\"Return a list of dicts containing the table name and columns for each table in the database.\"\"\"\n",
        "    table_dicts = []\n",
        "    for table_name in get_table_names(conn):\n",
        "        columns_names = get_column_names(conn, table_name)\n",
        "        table_dicts.append({\"table_name\": table_name, \"column_names\": columns_names})\n",
        "    return table_dicts\n"
      ],
      "metadata": {
        "id": "k-4oxW2zCK_I"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##  use these utility functions to extract a representation of the database schema.\n",
        "\n",
        "database_schema_dict = get_database_info(conn)\n",
        "database_schema_string = \"\\n\".join(\n",
        "    [\n",
        "        f\"Table: {table['table_name']}\\nColumns: {', '.join(table['column_names'])}\"\n",
        "        for table in database_schema_dict\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "71RxXAOACOAW"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "database_schema_string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "Pywt2AvuCWxz",
        "outputId": "19674f5e-cae2-4861-f8ae-78e8d59427ce"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Table: albums\\nColumns: AlbumId, Title, ArtistId\\nTable: sqlite_sequence\\nColumns: name, seq\\nTable: artists\\nColumns: ArtistId, Name\\nTable: customers\\nColumns: CustomerId, FirstName, LastName, Company, Address, City, State, Country, PostalCode, Phone, Fax, Email, SupportRepId\\nTable: employees\\nColumns: EmployeeId, LastName, FirstName, Title, ReportsTo, BirthDate, HireDate, Address, City, State, Country, PostalCode, Phone, Fax, Email\\nTable: genres\\nColumns: GenreId, Name\\nTable: invoices\\nColumns: InvoiceId, CustomerId, InvoiceDate, BillingAddress, BillingCity, BillingState, BillingCountry, BillingPostalCode, Total\\nTable: invoice_items\\nColumns: InvoiceLineId, InvoiceId, TrackId, UnitPrice, Quantity\\nTable: media_types\\nColumns: MediaTypeId, Name\\nTable: playlists\\nColumns: PlaylistId, Name\\nTable: playlist_track\\nColumns: PlaylistId, TrackId\\nTable: tracks\\nColumns: TrackId, Name, AlbumId, MediaTypeId, GenreId, Composer, Milliseconds, Bytes, UnitPrice\\nTable: sqlite_stat1\\nColumns: tbl, idx, stat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### we'll define a function specification for the function we'd like the API to generate arguments for.\n",
        "### Notice that we are inserting the database schema into the function specification.\n",
        "### This will be important for the model to know about.\n",
        "\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"ask_database\",\n",
        "            \"description\": \"Use this function to answer user questions about music. Input should be a fully formed SQL query.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"query\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": f\"\"\"\n",
        "                                SQL query extracting info to answer the user's question.\n",
        "                                SQL should be written using this database schema:\n",
        "                                {database_schema_string}\n",
        "                                The query should be returned in plain text, not in JSON.\n",
        "                                \"\"\",\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"query\"],\n",
        "            },\n",
        "        }\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "rkWXInMfCZZT"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Executing SQL queries"
      ],
      "metadata": {
        "id": "GpdK0iSvDHJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_database(conn, query):\n",
        "    \"\"\"Function to query SQLite database with a provided SQL query.\"\"\"\n",
        "    try:\n",
        "        results = str(conn.execute(query).fetchall())\n",
        "    except Exception as e:\n",
        "        results = f\"query failed with error: {e}\"\n",
        "    return results\n",
        "\n",
        "def execute_function_call(message):\n",
        "    if message.tool_calls[0].function.name == \"ask_database\":\n",
        "        query = json.loads(message.tool_calls[0].function.arguments)[\"query\"]\n",
        "        results = ask_database(conn, query)\n",
        "    else:\n",
        "        results = f\"Error: function {message.tool_calls[0].function.name} does not exist\"\n",
        "    return results"
      ],
      "metadata": {
        "id": "-ZrURyBMC1Cg"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "messages.append({\"role\": \"system\", \"content\": \"Answer user questions by generating SQL queries against the Chinook Music Database.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"Hi, who are the top 5 artists by number of tracks?\"})\n",
        "chat_response = chat_completion_request(messages, tools)\n",
        "assistant_message = chat_response.choices[0].message\n",
        "assistant_message.content = str(assistant_message.tool_calls[0].function)\n",
        "messages.append({\"role\": assistant_message.role, \"content\": assistant_message.content})\n",
        "if assistant_message.tool_calls:\n",
        "    results = execute_function_call(assistant_message)\n",
        "    messages.append({\"role\": \"function\", \"tool_call_id\": assistant_message.tool_calls[0].id, \"name\": assistant_message.tool_calls[0].function.name, \"content\": results})\n",
        "pretty_print_conversation(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acVvotaNDNcN",
        "outputId": "9a83d2ba-3e5b-4640-bbde-727ac6859451"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "system: Answer user questions by generating SQL queries against the Chinook Music Database.\n",
            "\n",
            "user: Hi, who are the top 5 artists by number of tracks?\n",
            "\n",
            "assistant: Function(arguments='{\\n  \"query\": \"SELECT artists.Name, COUNT(tracks.TrackId) AS TrackCount FROM artists JOIN albums ON artists.ArtistId = albums.ArtistId JOIN tracks ON albums.AlbumId = tracks.AlbumId GROUP BY artists.Name ORDER BY TrackCount DESC LIMIT 5;\"\\n}', name='ask_database')\n",
            "\n",
            "function (ask_database): [('Iron Maiden', 213), ('U2', 135), ('Led Zeppelin', 114), ('Metallica', 112), ('Lost', 92)]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append({\"role\": \"user\", \"content\": \"What is the name of the album with the most tracks?\"})\n",
        "chat_response = chat_completion_request(messages, tools)\n",
        "assistant_message = chat_response.choices[0].message\n",
        "assistant_message.content = str(assistant_message.tool_calls[0].function)\n",
        "messages.append({\"role\": assistant_message.role, \"content\": assistant_message.content})\n",
        "if assistant_message.tool_calls:\n",
        "    results = execute_function_call(assistant_message)\n",
        "    messages.append({\"role\": \"function\", \"tool_call_id\": assistant_message.tool_calls[0].id, \"name\": assistant_message.tool_calls[0].function.name, \"content\": results})\n",
        "pretty_print_conversation(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jISHsso-DPGr",
        "outputId": "b160a568-b6a2-4504-9b6b-5df0853b4795"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "system: Answer user questions by generating SQL queries against the Chinook Music Database.\n",
            "\n",
            "user: Hi, who are the top 5 artists by number of tracks?\n",
            "\n",
            "assistant: Function(arguments='{\\n  \"query\": \"SELECT artists.Name, COUNT(tracks.TrackId) AS TrackCount FROM artists JOIN albums ON artists.ArtistId = albums.ArtistId JOIN tracks ON albums.AlbumId = tracks.AlbumId GROUP BY artists.Name ORDER BY TrackCount DESC LIMIT 5;\"\\n}', name='ask_database')\n",
            "\n",
            "function (ask_database): [('Iron Maiden', 213), ('U2', 135), ('Led Zeppelin', 114), ('Metallica', 112), ('Lost', 92)]\n",
            "\n",
            "user: What is the name of the album with the most tracks?\n",
            "\n",
            "assistant: Function(arguments='{\\n  \"query\": \"SELECT albums.Title, COUNT(tracks.TrackId) AS TrackCount FROM albums JOIN tracks ON albums.AlbumId = tracks.AlbumId GROUP BY albums.Title ORDER BY TrackCount DESC LIMIT 1;\"\\n}', name='ask_database')\n",
            "\n",
            "function (ask_database): [('Greatest Hits', 57)]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XN-_M78rDgGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to use functions with a knowledge base\n",
        "\n",
        "We'll create an agent that uses data from arXiv to answer questions about academic subjects. It has two functions at its disposal:\n",
        "\n",
        "- **get_articles**: A function that gets arXiv articles on a subject and summarizes them for the user with links.\n",
        "\n",
        "- **read_article_and_summarize**: This function takes one of the previously searched articles, reads it in its entirety and summarizes the core argument, evidence and conclusions.\n"
      ],
      "metadata": {
        "id": "4a_K-rxVEZ7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scipy --quiet\n",
        "!pip install tenacity --quiet\n",
        "!pip install tiktoken==0.3.3 --quiet\n",
        "!pip install termcolor --quiet\n",
        "!pip install openai --quiet\n",
        "!pip install arxiv --quiet\n",
        "!pip install pandas --quiet\n",
        "!pip install PyPDF2 --quiet\n",
        "!pip install tqdm --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzjMW6ewEaUx",
        "outputId": "306fc7dd-43bf-426b-a336-b6716496d70f"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import arxiv\n",
        "import ast\n",
        "import concurrent\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "from csv import writer\n",
        "from IPython.display import display, Markdown, Latex\n",
        "from openai import OpenAI\n",
        "from PyPDF2 import PdfReader\n",
        "from scipy import spatial\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
        "from tqdm import tqdm\n",
        "from termcolor import colored\n",
        "\n",
        "# GPT_MODEL = \"gpt-3.5-turbo-0613\"\n",
        "# EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
        "# client = OpenAI()"
      ],
      "metadata": {
        "id": "KOZw5_WdE9Rk"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_MODEL = \"gpt35turbo16k\"\n",
        "EMBEDDING_MODEL = \"embeddingada002\""
      ],
      "metadata": {
        "id": "YxH18m44FB-l"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Search utilities\n",
        "\n",
        "Downloaded papers will be stored in a directory (we use ./data/papers here). We create a file arxiv_library.csv to store the embeddings and details for downloaded papers to retrieve against using summarize_text."
      ],
      "metadata": {
        "id": "kgxzYyyzGBf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory = './data/papers'\n",
        "\n",
        "# Check if the directory already exists\n",
        "if not os.path.exists(directory):\n",
        "    # If the directory doesn't exist, create it and any necessary intermediate directories\n",
        "    os.makedirs(directory)\n",
        "    print(f\"Directory '{directory}' created successfully.\")\n",
        "else:\n",
        "    # If the directory already exists, print a message indicating it\n",
        "    print(f\"Directory '{directory}' already exists.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ex7uGIyRGCEk",
        "outputId": "1a5f037b-c85f-4f44-fbea-58591d14af5e"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory './data/papers' created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a directory to store downloaded papers\n",
        "data_dir = os.path.join(os.curdir, \"data\", \"papers\")\n",
        "paper_dir_filepath = \"./data/arxiv_library.csv\"\n",
        "\n",
        "# Generate a blank dataframe where we can store downloaded files\n",
        "df = pd.DataFrame(list())\n",
        "df.to_csv(paper_dir_filepath)"
      ],
      "metadata": {
        "id": "5s5FQCD3GGNe"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KjSQtEp6I0Hr"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
        "def embedding_request(text):\n",
        "    response = client.embeddings.create(input=text, model=EMBEDDING_MODEL)\n",
        "    return response\n",
        "\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
        "def get_articles(query, library=paper_dir_filepath, top_k=5):\n",
        "    \"\"\"This function gets the top_k articles based on a user's query, sorted by relevance.\n",
        "    It also downloads the files and stores them in arxiv_library.csv to be retrieved by the read_article_and_summarize.\n",
        "    \"\"\"\n",
        "    client = arxiv.Client()\n",
        "    search = arxiv.Search(\n",
        "        query = query,\n",
        "        max_results = top_k,\n",
        "        sort_by = arxiv.SortCriterion.SubmittedDate\n",
        "    )\n",
        "    result_list = []\n",
        "    for result in client.results(search):\n",
        "        result_dict = {}\n",
        "        result_dict.update({\"title\": result.title})\n",
        "        result_dict.update({\"summary\": result.summary})\n",
        "\n",
        "        # Taking the first url provided\n",
        "        result_dict.update({\"article_url\": [x.href for x in result.links][0]})\n",
        "        result_dict.update({\"pdf_url\": [x.href for x in result.links][1]})\n",
        "        result_list.append(result_dict)\n",
        "\n",
        "        # Store references in library file\n",
        "        response = embedding_request(text=result.title)\n",
        "        file_reference = [\n",
        "            result.title,\n",
        "            result.download_pdf(data_dir),\n",
        "            response.data[0].embedding,\n",
        "        ]\n",
        "\n",
        "        # Write to file\n",
        "        with open(library, \"a\") as f_object:\n",
        "            writer_object = writer(f_object)\n",
        "            writer_object.writerow(file_reference)\n",
        "            f_object.close()\n",
        "    return result_list\n"
      ],
      "metadata": {
        "id": "hvHTI3YxGMuz"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test that the search is working\n",
        "result_output = get_articles(\"attention is all you need\")\n",
        "result_output[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcQoIoaIGPQ1",
        "outputId": "56e3c0ae-93e2-4350-bb77-c7163c60ba04"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': 'Enhancing Retinal Vascular Structure Segmentation in Images With a Novel Design Two-Path Interactive Fusion Module Model',\n",
              " 'summary': \"Precision in identifying and differentiating micro and macro blood vessels in\\nthe retina is crucial for the diagnosis of retinal diseases, although it poses\\na significant challenge. Current autoencoding-based segmentation approaches\\nencounter limitations as they are constrained by the encoder and undergo a\\nreduction in resolution during the encoding stage. The inability to recover\\nlost information in the decoding phase further impedes these approaches.\\nConsequently, their capacity to extract the retinal microvascular structure is\\nrestricted. To address this issue, we introduce Swin-Res-Net, a specialized\\nmodule designed to enhance the precision of retinal vessel segmentation.\\nSwin-Res-Net utilizes the Swin transformer which uses shifted windows with\\ndisplacement for partitioning, to reduce network complexity and accelerate\\nmodel convergence. Additionally, the model incorporates interactive fusion with\\na functional module in the Res2Net architecture. The Res2Net leverages\\nmulti-scale techniques to enlarge the receptive field of the convolutional\\nkernel, enabling the extraction of additional semantic information from the\\nimage. This combination creates a new module that enhances the localization and\\nseparation of micro vessels in the retina. To improve the efficiency of\\nprocessing vascular information, we've added a module to eliminate redundant\\ninformation between the encoding and decoding steps.\\n  Our proposed architecture produces outstanding results, either meeting or\\nsurpassing those of other published models. The AUC reflects significant\\nenhancements, achieving values of 0.9956, 0.9931, and 0.9946 in pixel-wise\\nsegmentation of retinal vessels across three widely utilized datasets:\\nCHASE-DB1, DRIVE, and STARE, respectively. Moreover, Swin-Res-Net outperforms\\nalternative architectures, demonstrating superior performance in both IOU and\\nF1 measure metrics.\",\n",
              " 'article_url': 'http://arxiv.org/abs/2403.01362v1',\n",
              " 'pdf_url': 'http://arxiv.org/pdf/2403.01362v1'}"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def strings_ranked_by_relatedness(\n",
        "    query: str,\n",
        "    df: pd.DataFrame,\n",
        "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
        "    top_n: int = 100,\n",
        ") -> list[str]:\n",
        "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
        "    query_embedding_response = embedding_request(query)\n",
        "    query_embedding = query_embedding_response.data[0].embedding\n",
        "    strings_and_relatednesses = [\n",
        "        (row[\"filepath\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
        "        for i, row in df.iterrows()\n",
        "    ]\n",
        "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
        "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
        "    return strings[:top_n]\n"
      ],
      "metadata": {
        "id": "PJUENpMGGSXY"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pdf(filepath):\n",
        "    \"\"\"Takes a filepath to a PDF and returns a string of the PDF's contents\"\"\"\n",
        "    # creating a pdf reader object\n",
        "    reader = PdfReader(filepath)\n",
        "    pdf_text = \"\"\n",
        "    page_number = 0\n",
        "    for page in reader.pages:\n",
        "        page_number += 1\n",
        "        pdf_text += page.extract_text() + f\"\\nPage Number: {page_number}\"\n",
        "    return pdf_text\n",
        "\n",
        "\n",
        "# Split a text into smaller chunks of size n, preferably ending at the end of a sentence\n",
        "def create_chunks(text, n, tokenizer):\n",
        "    \"\"\"Returns successive n-sized chunks from provided text.\"\"\"\n",
        "    tokens = tokenizer.encode(text)\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
        "        j = min(i + int(1.5 * n), len(tokens))\n",
        "        while j > i + int(0.5 * n):\n",
        "            # Decode the tokens and check for full stop or newline\n",
        "            chunk = tokenizer.decode(tokens[i:j])\n",
        "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
        "                break\n",
        "            j -= 1\n",
        "        # If no end of sentence found, use n tokens as the chunk size\n",
        "        if j == i + int(0.5 * n):\n",
        "            j = min(i + n, len(tokens))\n",
        "        yield tokens[i:j]\n",
        "        i = j\n",
        "\n",
        "\n",
        "def extract_chunk(content, template_prompt):\n",
        "    \"\"\"This function applies a prompt to some input content. In this case it returns a summarized chunk of text\"\"\"\n",
        "    prompt = template_prompt + content\n",
        "    response = client.chat.completions.create(\n",
        "        model=GPT_MODEL, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "def summarize_text(query):\n",
        "    \"\"\"This function does the following:\n",
        "    - Reads in the arxiv_library.csv file in including the embeddings\n",
        "    - Finds the closest file to the user's query\n",
        "    - Scrapes the text out of the file and chunks it\n",
        "    - Summarizes each chunk in parallel\n",
        "    - Does one final summary and returns this to the user\"\"\"\n",
        "\n",
        "    # A prompt to dictate how the recursive summarizations should approach the input paper\n",
        "    summary_prompt = \"\"\"Summarize this text from an academic paper. Extract any key points with reasoning.\\n\\nContent:\"\"\"\n",
        "\n",
        "    # If the library is empty (no searches have been performed yet), we perform one and download the results\n",
        "    library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
        "    if len(library_df) == 0:\n",
        "        print(\"No papers searched yet, downloading first.\")\n",
        "        get_articles(query)\n",
        "        print(\"Papers downloaded, continuing\")\n",
        "        library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
        "    library_df.columns = [\"title\", \"filepath\", \"embedding\"]\n",
        "    library_df[\"embedding\"] = library_df[\"embedding\"].apply(ast.literal_eval)\n",
        "    strings = strings_ranked_by_relatedness(query, library_df, top_n=1)\n",
        "    print(\"Chunking text from paper\")\n",
        "    pdf_text = read_pdf(strings[0])\n",
        "\n",
        "    # Initialise tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    results = \"\"\n",
        "\n",
        "    # Chunk up the document into 1500 token chunks\n",
        "    chunks = create_chunks(pdf_text, 1500, tokenizer)\n",
        "    text_chunks = [tokenizer.decode(chunk) for chunk in chunks]\n",
        "    print(\"Summarizing each chunk of text\")\n",
        "\n",
        "    # Parallel process the summaries\n",
        "    with concurrent.futures.ThreadPoolExecutor(\n",
        "        max_workers=len(text_chunks)\n",
        "    ) as executor:\n",
        "        futures = [\n",
        "            executor.submit(extract_chunk, chunk, summary_prompt)\n",
        "            for chunk in text_chunks\n",
        "        ]\n",
        "        with tqdm(total=len(text_chunks)) as pbar:\n",
        "            for _ in concurrent.futures.as_completed(futures):\n",
        "                pbar.update(1)\n",
        "        for future in futures:\n",
        "            data = future.result()\n",
        "            results += data\n",
        "\n",
        "    # Final summary\n",
        "    print(\"Summarizing into overall summary\")\n",
        "    response = client.chat.completions.create(\n",
        "        model=GPT_MODEL,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"Write a summary collated from this collection of key points extracted from an academic paper.\n",
        "                        The summary should highlight the core argument, conclusions and evidence, and answer the user's query.\n",
        "                        User query: {query}\n",
        "                        The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions.\n",
        "                        Key points:\\n{results}\\nSummary:\\n\"\"\",\n",
        "            }\n",
        "        ],\n",
        "        temperature=0,\n",
        "    )\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "RtAiqTubG4YC"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the summarize_text function works\n",
        "chat_test_response = summarize_text(\"PPO reinforcement learning sequence generation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lB2Q4P5-HLsa",
        "outputId": "bab5251e-27f8-4ca7-defe-c9f5b0190f05"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunking text from paper\n",
            "Summarizing each chunk of text\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:04<00:00,  1.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarizing into overall summary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat_test_response.choices[0].message.content)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeV9uBMCHOra",
        "outputId": "f03b2a18-1fb5-44dc-9bd4-8d4711724b01"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Core Argument:\n",
            "- The paper discusses the potential of using a general-purpose large language model (LLM) to learn the structural biophysics of DNA.\n",
            "- The authors show that fine-tuning a LLM can enhance its ability to analyze and design DNA sequences and their structures.\n",
            "- The study focuses on the formation of secondary structures in DNA, which are governed by base pairing and stacking bonds.\n",
            "\n",
            "Evidence:\n",
            "- The authors use the NUPACK software suite to provide data for training and validation.\n",
            "- The models are fine-tuned using OpenAI's API and an error checking step is applied to the output of each expert.\n",
            "- The authors evaluate the performance of different models in predicting secondary structure and calculating minimum free energy (MFE) of DNA sequences.\n",
            "- The models perform better when they explicitly consider the nearest neighbor window and the reverse complement of the sequences.\n",
            "- The pipeline approach, where a separate model determines the reverse complement and feeds it to another model for secondary structure prediction, enhances the accuracy of the predictions.\n",
            "- Increasing the amount of training data improves the accuracy of MFE predictions.\n",
            "\n",
            "Conclusions:\n",
            "- The expert pipeline approach and breaking down the problem into smaller subtasks improve the performance of the models in predicting secondary structure and calculating MFE of DNA sequences.\n",
            "- The combination of the chain-of-thought (CoT) approach and model pipeline provides the best results in analysis tasks.\n",
            "- The CoT approach, combined with the reverse complement transformation, yields the highest accuracy in design tasks.\n",
            "- The addition of an error checking layer further improves accuracy in design tasks.\n",
            "- The paper suggests further exploration of chaining smaller models for performance improvement and the use of an LLM architecture involving both an encoder and decoder for direct sequence comparison.\n",
            "- The authors highlight the potential applications of LLMs in DNA nanotech and suggest using LLMs trained on experimental data for more accurate structural predictions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure Agent\n",
        "\n",
        " a Conversation class to support multiple turns with the API, and some Python functions to enable interaction between the ChatCompletion API and our knowledge base functions."
      ],
      "metadata": {
        "id": "sr6xYsHWHp79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
        "def chat_completion_request(messages, functions=None, model=GPT_MODEL):\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            functions=functions,\n",
        "        )\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(\"Unable to generate ChatCompletion response\")\n",
        "        print(f\"Exception: {e}\")\n",
        "        return e\n"
      ],
      "metadata": {
        "id": "hwEhHyK0HSgX"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Conversation:\n",
        "    def __init__(self):\n",
        "        self.conversation_history = []\n",
        "\n",
        "    def add_message(self, role, content):\n",
        "        message = {\"role\": role, \"content\": content}\n",
        "        self.conversation_history.append(message)\n",
        "\n",
        "    def display_conversation(self, detailed=False):\n",
        "        role_to_color = {\n",
        "            \"system\": \"red\",\n",
        "            \"user\": \"green\",\n",
        "            \"assistant\": \"blue\",\n",
        "            \"function\": \"magenta\",\n",
        "        }\n",
        "        for message in self.conversation_history:\n",
        "            print(\n",
        "                colored(\n",
        "                    f\"{message['role']}: {message['content']}\\n\\n\",\n",
        "                    role_to_color[message[\"role\"]],\n",
        "                )\n",
        "            )"
      ],
      "metadata": {
        "id": "T4frPLSgHwqi"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate our get_articles and read_article_and_summarize functions\n",
        "arxiv_functions = [\n",
        "    {\n",
        "        \"name\": \"get_articles\",\n",
        "        \"description\": \"\"\"Use this function to get academic papers from arXiv to answer user questions.\"\"\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"query\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": f\"\"\"\n",
        "                            User query in JSON. Responses should be summarized and should include the article URL reference\n",
        "                            \"\"\",\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"query\"],\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"read_article_and_summarize\",\n",
        "        \"description\": \"\"\"Use this function to read whole papers and provide a summary for users.\n",
        "        You should NEVER call this function before get_articles has been called in the conversation.\"\"\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"query\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": f\"\"\"\n",
        "                            Description of the article in plain text based on the user's query\n",
        "                            \"\"\",\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"query\"],\n",
        "        },\n",
        "    }\n",
        "]\n"
      ],
      "metadata": {
        "id": "UoXSoYx_Hz-j"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_completion_with_function_execution(messages, functions=[None]):\n",
        "    \"\"\"This function makes a ChatCompletion API call with the option of adding functions\"\"\"\n",
        "    response = chat_completion_request(messages, functions)\n",
        "    full_message = response.choices[0]\n",
        "    if full_message.finish_reason == \"function_call\":\n",
        "        print(f\"Function generation requested, calling function\")\n",
        "        return call_arxiv_function(messages, full_message)\n",
        "    else:\n",
        "        print(f\"Function not required, responding to user\")\n",
        "        return response\n",
        "\n",
        "\n",
        "def call_arxiv_function(messages, full_message):\n",
        "    \"\"\"Function calling function which executes function calls when the model believes it is necessary.\n",
        "    Currently extended by adding clauses to this if statement.\"\"\"\n",
        "\n",
        "    if full_message.message.function_call.name == \"get_articles\":\n",
        "        try:\n",
        "            parsed_output = json.loads(\n",
        "                full_message.message.function_call.arguments\n",
        "            )\n",
        "            print(\"Getting search results\")\n",
        "            results = get_articles(parsed_output[\"query\"])\n",
        "        except Exception as e:\n",
        "            print(parsed_output)\n",
        "            print(f\"Function execution failed\")\n",
        "            print(f\"Error message: {e}\")\n",
        "        messages.append(\n",
        "            {\n",
        "                \"role\": \"function\",\n",
        "                \"name\": full_message.message.function_call.name,\n",
        "                \"content\": str(results),\n",
        "            }\n",
        "        )\n",
        "        try:\n",
        "            print(\"Got search results, summarizing content\")\n",
        "            response = chat_completion_request(messages)\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            print(type(e))\n",
        "            raise Exception(\"Function chat request failed\")\n",
        "\n",
        "    elif (\n",
        "        full_message.message.function_call.name == \"read_article_and_summarize\"\n",
        "    ):\n",
        "        parsed_output = json.loads(\n",
        "            full_message.message.function_call.arguments\n",
        "        )\n",
        "        print(\"Finding and reading paper\")\n",
        "        summary = summarize_text(parsed_output[\"query\"])\n",
        "        return summary\n",
        "\n",
        "    else:\n",
        "        raise Exception(\"Function does not exist and cannot be called\")\n"
      ],
      "metadata": {
        "id": "j504sqceH175"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### arXiv conversation"
      ],
      "metadata": {
        "id": "z-XTWiTDH--D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start with a system message\n",
        "paper_system_message = \"\"\"You are arXivGPT, a helpful assistant pulls academic papers to answer user questions.\n",
        "You summarize the papers clearly so the customer can decide which to read to answer their question.\n",
        "You always provide the article_url and title so the user can understand the name of the paper and click through to access it.\n",
        "Begin!\"\"\"\n",
        "paper_conversation = Conversation()\n",
        "paper_conversation.add_message(\"system\", paper_system_message)\n"
      ],
      "metadata": {
        "id": "oJM-aSpJH7XF"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a user message\n",
        "paper_conversation.add_message(\"user\", \"Hi, how does PPO reinforcement learning work?\")\n",
        "chat_response = chat_completion_with_function_execution(\n",
        "    paper_conversation.conversation_history, functions=arxiv_functions\n",
        ")\n",
        "assistant_message = chat_response.choices[0].message.content\n",
        "paper_conversation.add_message(\"assistant\", assistant_message)\n",
        "display(Markdown(assistant_message))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "gfTaeYLOICQg",
        "outputId": "bdf7cc29-641f-4e50-dcc4-5dfc73fb8e2c"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function generation requested, calling function\n",
            "Getting search results\n",
            "Got search results, summarizing content\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I found several papers related to PPO reinforcement learning. Here are a few summaries:\n\n1. Title: \"Bandit Profit-maximization for Targeted Marketing\"\n   - Summary: This paper presents near-optimal algorithms for optimizing profit over multiple demand curves, which are dependent on different ancillary variables while maintaining the same price. It is relevant to PPO reinforcement learning as it tackles a sequential profit-maximization problem.\n   - Article URL: [Link](http://arxiv.org/abs/2403.01361v1)\n\n2. Title: \"Inferring potential landscapes: A Schrödinger bridge approach to Maximum Caliber\"\n   - Summary: This work extends Schrödinger bridges to account for integral constraints along paths, specifically in the context of Maximum Caliber, a Maximum Entropy principle applied in a dynamic context. While not directly related to PPO reinforcement learning, it can provide insights into stochastic dynamics and inference of time-varying potential landscapes.\n   - Article URL: [Link](http://arxiv.org/abs/2403.01357v1)\n\n3. Title: \"a-DCF: an architecture agnostic metric with application to spoofing-robust speaker verification\"\n   - Summary: This paper proposes an architecture-agnostic detection cost function (a-DCF) for evaluating spoofing-robust automatic speaker verification (ASV) systems. Although it does not focus on PPO reinforcement learning, it provides a metric for evaluating ASV systems in the presence of spoofing attacks.\n   - Article URL: [Link](http://arxiv.org/abs/2403.01355v1)\n\nThese papers should provide insights into different aspects of reinforcement learning and related topics."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add another user message to induce our system to use the second tool\n",
        "paper_conversation.add_message(\n",
        "    \"user\",\n",
        "    \"Can you read the PPO sequence generation paper for me and give me a summary\",\n",
        ")\n",
        "updated_response = chat_completion_with_function_execution(\n",
        "    paper_conversation.conversation_history, functions=arxiv_functions\n",
        ")\n",
        "display(Markdown(updated_response.choices[0].message.content))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "id": "PjtBI2OaII__",
        "outputId": "2a54ea61-579a-4c55-c0fc-afc25112d54e"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function generation requested, calling function\n",
            "Finding and reading paper\n",
            "Chunking text from paper\n",
            "Summarizing each chunk of text\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:04<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarizing into overall summary\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Core Argument:\n- The paper discusses the potential of using a general-purpose large language model (LLM) to learn the structural biophysics of DNA.\n- The authors show that fine-tuning a LLM, specifically chatGPT 3.5-turbo, can enhance its ability to analyze and design DNA sequences and their structures.\n- The study focuses on the formation of secondary structures in DNA, which are governed by base pairing and stacking bonds.\n- The authors propose a method that involves chaining together models fine-tuned for subtasks and using a chain-of-thought approach to improve the model's performance.\n\nEvidence:\n- The authors use the NUPACK software suite to provide data for training and validation.\n- The expert pipeline approach involves using models that have been fine-tuned for subtasks and feeding their outputs into each other.\n- The models perform better when they explicitly consider the nearest neighbor window and the reverse complement of the sequences.\n- The pipeline approach, where a separate model determines the reverse complement and feeds it to another model for secondary structure prediction, enhances the accuracy of the predictions.\n- The performance of the models improves with larger training sets.\n\nConclusions:\n- The study demonstrates the potential of using LLMs to learn DNA structural biophysics.\n- Integrating experimental data and machine learning is important in scientific research.\n- The expert pipeline approach and breaking down the problem into smaller subtasks improve the performance of the models in DNA sequence analysis.\n- The combination of chain-of-thought and model pipeline provides the best results in analysis tasks.\n- The CoT approach, combined with the reverse complement transformation, yields the highest accuracy in design tasks.\n- The addition of an error checking layer further improves accuracy in design tasks.\n- Sequence design is more challenging than analysis, but error correction can compensate for the increased difficulty.\n- Larger training sets benefit design tasks more.\n- Future research directions include exploring chaining smaller models for performance improvement and using an LLM architecture involving both an encoder and decoder for direct sequence comparison."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Lxv7b_GIah5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}