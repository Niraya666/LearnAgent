{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install metagpt==0.5.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "A9xqJgSdOm2C",
        "outputId": "3a1780b6-1d7f-42c8-99f4-7fc26f515648"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting metagpt==0.5.2\n",
            "  Downloading metagpt-0.5.2-py3-none-any.whl (216 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.4/216.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp==3.8.4 (from metagpt==0.5.2)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting channels==4.0.0 (from metagpt==0.5.2)\n",
            "  Downloading channels-4.0.0-py3-none-any.whl (28 kB)\n",
            "Collecting faiss-cpu==1.7.4 (from metagpt==0.5.2)\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (0.9.0)\n",
            "Collecting lancedb==0.1.16 (from metagpt==0.5.2)\n",
            "  Downloading lancedb-0.1.16-py3-none-any.whl (34 kB)\n",
            "Collecting langchain==0.0.231 (from metagpt==0.5.2)\n",
            "  Downloading langchain-0.0.231-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting loguru==0.6.0 (from metagpt==0.5.2)\n",
            "  Downloading loguru-0.6.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting meilisearch==0.21.0 (from metagpt==0.5.2)\n",
            "  Downloading meilisearch-0.21.0-py3-none-any.whl (19 kB)\n",
            "Collecting numpy==1.24.3 (from metagpt==0.5.2)\n",
            "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai>=0.28.1 (from metagpt==0.5.2)\n",
            "  Downloading openai-1.8.0-py3-none-any.whl (222 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.3/222.3 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (3.1.2)\n",
            "Collecting beautifulsoup4==4.12.2 (from metagpt==0.5.2)\n",
            "  Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==2.0.3 (from metagpt==0.5.2)\n",
            "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic==1.10.8 (from metagpt==0.5.2)\n",
            "  Downloading pydantic-1.10.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytest==7.2.2 (from metagpt==0.5.2)\n",
            "  Downloading pytest-7.2.2-py3-none-any.whl (317 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.2/317.2 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-docx==0.8.11 (from metagpt==0.5.2)\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML==6.0.1 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (6.0.1)\n",
            "Collecting setuptools==65.6.3 (from metagpt==0.5.2)\n",
            "  Downloading setuptools-65.6.3-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tenacity==8.2.2 (from metagpt==0.5.2)\n",
            "  Downloading tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
            "Collecting tiktoken==0.4.0 (from metagpt==0.5.2)\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm==4.64.0 (from metagpt==0.5.2)\n",
            "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting anthropic==0.3.6 (from metagpt==0.5.2)\n",
            "  Downloading anthropic-0.3.6-py3-none-any.whl (793 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m793.6/793.6 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect==0.8.0 (from metagpt==0.5.2)\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: typing-extensions==4.5.0 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (4.5.0)\n",
            "Collecting libcst==1.0.1 (from metagpt==0.5.2)\n",
            "  Downloading libcst-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting qdrant-client==1.4.0 (from metagpt==0.5.2)\n",
            "  Downloading qdrant_client-1.4.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.5/132.5 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytest-mock==3.11.1 (from metagpt==0.5.2)\n",
            "  Downloading pytest_mock-3.11.1-py3-none-any.whl (9.6 kB)\n",
            "Collecting ta==0.10.2 (from metagpt==0.5.2)\n",
            "  Downloading ta-0.10.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting semantic-kernel==0.3.13.dev0 (from metagpt==0.5.2)\n",
            "  Downloading semantic_kernel-0.3.13.dev0-py3-none-any.whl (176 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wrapt==1.15.0 (from metagpt==0.5.2)\n",
            "  Downloading wrapt-1.15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websocket-client==0.58.0 (from metagpt==0.5.2)\n",
            "  Downloading websocket_client-0.58.0-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles==23.2.1 (from metagpt==0.5.2)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting gitpython==3.1.40 (from metagpt==0.5.2)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zhipuai==1.0.7 (from metagpt==0.5.2)\n",
            "  Downloading zhipuai-1.0.7-py3-none-any.whl (7.9 kB)\n",
            "Collecting gitignore-parser==0.1.9 (from metagpt==0.5.2)\n",
            "  Downloading gitignore_parser-0.1.9.tar.gz (5.3 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting open-interpreter==0.1.7 (from metagpt==0.5.2)\n",
            "  Downloading open_interpreter-0.1.7-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.4->metagpt==0.5.2) (23.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.4->metagpt==0.5.2) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.4->metagpt==0.5.2) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.4->metagpt==0.5.2) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.4->metagpt==0.5.2) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.4->metagpt==0.5.2) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.4->metagpt==0.5.2) (1.3.1)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic==0.3.6->metagpt==0.5.2) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from anthropic==0.3.6->metagpt==0.5.2) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from anthropic==0.3.6->metagpt==0.5.2)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic==0.3.6->metagpt==0.5.2) (0.15.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4==4.12.2->metagpt==0.5.2) (2.5)\n",
            "Collecting Django>=3.2 (from channels==4.0.0->metagpt==0.5.2)\n",
            "  Downloading Django-5.0.1-py3-none-any.whl (8.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m139.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting asgiref<4,>=3.5.0 (from channels==4.0.0->metagpt==0.5.2)\n",
            "  Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython==3.1.40->metagpt==0.5.2)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pylance==0.5.10 (from lancedb==0.1.16->metagpt==0.5.2)\n",
            "  Downloading pylance-0.5.10-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ratelimiter (from lancedb==0.1.16->metagpt==0.5.2)\n",
            "  Downloading ratelimiter-1.2.0.post0-py3-none-any.whl (6.6 kB)\n",
            "Collecting retry (from lancedb==0.1.16->metagpt==0.5.2)\n",
            "  Downloading retry-0.9.2-py2.py3-none-any.whl (8.0 kB)\n",
            "Collecting attr (from lancedb==0.1.16->metagpt==0.5.2)\n",
            "  Downloading attr-0.3.2-py2.py3-none-any.whl (3.3 kB)\n",
            "Collecting semver (from lancedb==0.1.16->metagpt==0.5.2)\n",
            "  Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231->metagpt==0.5.2) (2.0.24)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.231->metagpt==0.5.2)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langchainplus-sdk<0.0.21,>=0.0.20 (from langchain==0.0.231->metagpt==0.5.2)\n",
            "  Downloading langchainplus_sdk-0.0.20-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231->metagpt==0.5.2) (2.8.8)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.231->metagpt==0.5.2)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231->metagpt==0.5.2) (2.31.0)\n",
            "Collecting camel-converter[pydantic] (from meilisearch==0.21.0->metagpt==0.5.2)\n",
            "  Downloading camel_converter-3.1.1-py3-none-any.whl (5.7 kB)\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from open-interpreter==0.1.7->metagpt==0.5.2) (1.4.4)\n",
            "Collecting astor<0.9.0,>=0.8.1 (from open-interpreter==0.1.7->metagpt==0.5.2)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting git-python<2.0.0,>=1.0.3 (from open-interpreter==0.1.7->metagpt==0.5.2)\n",
            "  Downloading git_python-1.0.3-py2.py3-none-any.whl (1.9 kB)\n",
            "Collecting huggingface-hub<0.17.0,>=0.16.4 (from open-interpreter==0.1.7->metagpt==0.5.2)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting inquirer<4.0.0,>=3.1.3 (from open-interpreter==0.1.7->metagpt==0.5.2)\n",
            "  Downloading inquirer-3.2.1-py3-none-any.whl (18 kB)\n",
            "Collecting litellm<0.2.0,>=0.1.590 (from open-interpreter==0.1.7->metagpt==0.5.2)\n",
            "  Downloading litellm-0.1.824-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai>=0.28.1 (from metagpt==0.5.2)\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv<2.0.0,>=1.0.0 (from open-interpreter==0.1.7->metagpt==0.5.2)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.4.2 in /usr/local/lib/python3.10/dist-packages (from open-interpreter==0.1.7->metagpt==0.5.2) (13.7.0)\n",
            "Collecting semgrep<2.0.0,>=1.41.0 (from open-interpreter==0.1.7->metagpt==0.5.2)\n",
            "  Downloading semgrep-1.56.0-cp38.cp39.cp310.cp311.py37.py38.py39.py310.py311-none-any.whl (34.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six<2.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from open-interpreter==0.1.7->metagpt==0.5.2) (1.16.0)\n",
            "Collecting tokentrim<0.2.0,>=0.1.9 (from open-interpreter==0.1.7->metagpt==0.5.2)\n",
            "  Downloading tokentrim-0.1.13-py3-none-any.whl (7.8 kB)\n",
            "Collecting wget<4.0,>=3.2 (from open-interpreter==0.1.7->metagpt==0.5.2)\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting yaspin<4.0.0,>=3.0.1 (from open-interpreter==0.1.7->metagpt==0.5.2)\n",
            "  Downloading yaspin-3.0.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->metagpt==0.5.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->metagpt==0.5.2) (2023.3.post1)\n",
            "Collecting tzdata>=2022.1 (from pandas==2.0.3->metagpt==0.5.2)\n",
            "  Downloading tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest==7.2.2->metagpt==0.5.2) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest==7.2.2->metagpt==0.5.2) (23.2)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest==7.2.2->metagpt==0.5.2) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest==7.2.2->metagpt==0.5.2) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest==7.2.2->metagpt==0.5.2) (2.0.1)\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from python-docx==0.8.11->metagpt==0.5.2) (4.9.4)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client==1.4.0->metagpt==0.5.2) (1.60.0)\n",
            "Collecting grpcio-tools>=1.41.0 (from qdrant-client==1.4.0->metagpt==0.5.2)\n",
            "  Downloading grpcio_tools-1.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker<3.0.0,>=2.7.0 (from qdrant-client==1.4.0->metagpt==0.5.2)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Collecting urllib3<2.0.0,>=1.26.14 (from qdrant-client==1.4.0->metagpt==0.5.2)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openapi_core<0.19.0,>=0.18.0 (from semantic-kernel==0.3.13.dev0->metagpt==0.5.2)\n",
            "  Downloading openapi_core-0.18.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.4/82.4 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting prance<24.0.0.0,>=23.6.21.0 (from semantic-kernel==0.3.13.dev0->metagpt==0.5.2)\n",
            "  Downloading prance-23.6.21.0-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: regex<2024.0.0,>=2023.6.3 in /usr/local/lib/python3.10/dist-packages (from semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (2023.6.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect==0.8.0->metagpt==0.5.2)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: PyJWT in /usr/lib/python3/dist-packages (from zhipuai==1.0.7->metagpt==0.5.2) (2.3.0)\n",
            "Collecting dataclasses (from zhipuai==1.0.7->metagpt==0.5.2)\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from zhipuai==1.0.7->metagpt==0.5.2) (5.3.2)\n",
            "Requirement already satisfied: pyarrow>=10 in /usr/local/lib/python3.10/dist-packages (from pylance==0.5.10->lancedb==0.1.16->metagpt==0.5.2) (10.0.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->metagpt==0.5.2) (1.1.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer->metagpt==0.5.2) (8.1.7)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->anthropic==0.3.6->metagpt==0.5.2) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->anthropic==0.3.6->metagpt==0.5.2) (1.3.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.231->metagpt==0.5.2)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from Django>=3.2->channels==4.0.0->metagpt==0.5.2) (0.4.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython==3.1.40->metagpt==0.5.2)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Collecting protobuf<5.0dev,>=4.21.6 (from grpcio-tools>=1.41.0->qdrant-client==1.4.0->metagpt==0.5.2)\n",
            "  Downloading protobuf-4.25.2-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic==0.3.6->metagpt==0.5.2) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->anthropic==0.3.6->metagpt==0.5.2)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->anthropic==0.3.6->metagpt==0.5.2)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2<5,>=3 (from httpx<1,>=0.23.0->anthropic==0.3.6->metagpt==0.5.2)\n",
            "  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.17.0,>=0.16.4->open-interpreter==0.1.7->metagpt==0.5.2) (3.13.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.17.0,>=0.16.4->open-interpreter==0.1.7->metagpt==0.5.2) (2023.6.0)\n",
            "Collecting blessed>=1.19.0 (from inquirer<4.0.0,>=3.1.3->open-interpreter==0.1.7->metagpt==0.5.2)\n",
            "  Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting editor>=1.6.0 (from inquirer<4.0.0,>=3.1.3->open-interpreter==0.1.7->metagpt==0.5.2)\n",
            "  Downloading editor-1.6.5-py3-none-any.whl (4.0 kB)\n",
            "Collecting readchar>=3.0.6 (from inquirer<4.0.0,>=3.1.3->open-interpreter==0.1.7->metagpt==0.5.2)\n",
            "  Downloading readchar-4.0.5-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.10/dist-packages (from litellm<0.2.0,>=0.1.590->open-interpreter==0.1.7->metagpt==0.5.2) (7.0.1)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from litellm<0.2.0,>=0.1.590->open-interpreter==0.1.7->metagpt==0.5.2) (3.1.3)\n",
            "Collecting isodate (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema<5.0.0,>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (4.19.2)\n",
            "Collecting jsonschema-spec<0.3.0,>=0.2.3 (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2)\n",
            "  Downloading jsonschema_spec-0.2.4-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (10.1.0)\n",
            "Collecting openapi-schema-validator<0.7.0,>=0.6.0 (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2)\n",
            "  Downloading openapi_schema_validator-0.6.2-py3-none-any.whl (8.8 kB)\n",
            "Collecting openapi-spec-validator<0.8.0,>=0.7.1 (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2)\n",
            "  Downloading openapi_spec_validator-0.7.1-py3-none-any.whl (38 kB)\n",
            "Collecting parse (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2)\n",
            "  Downloading parse-1.20.0-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: werkzeug in /usr/local/lib/python3.10/dist-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (3.0.1)\n",
            "Requirement already satisfied: chardet>=3.0 in /usr/local/lib/python3.10/dist-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (5.2.0)\n",
            "Collecting ruamel.yaml>=0.17.10 (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2)\n",
            "  Downloading ruamel.yaml-0.18.5-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.4.2->open-interpreter==0.1.7->metagpt==0.5.2) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.4.2->open-interpreter==0.1.7->metagpt==0.5.2) (2.16.1)\n",
            "Collecting boltons~=21.0 (from semgrep<2.0.0,>=1.41.0->open-interpreter==0.1.7->metagpt==0.5.2)\n",
            "  Downloading boltons-21.0.0-py2.py3-none-any.whl (193 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.7/193.7 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting click-option-group~=0.5 (from semgrep<2.0.0,>=1.41.0->open-interpreter==0.1.7->metagpt==0.5.2)\n",
            "  Downloading click_option_group-0.5.6-py3-none-any.whl (12 kB)\n",
            "Collecting colorama~=0.4.0 (from semgrep<2.0.0,>=1.41.0->open-interpreter==0.1.7->metagpt==0.5.2)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: defusedxml~=0.7.1 in /usr/local/lib/python3.10/dist-packages (from semgrep<2.0.0,>=1.41.0->open-interpreter==0.1.7->metagpt==0.5.2) (0.7.1)\n",
            "Collecting glom~=22.1 (from semgrep<2.0.0,>=1.41.0->open-interpreter==0.1.7->metagpt==0.5.2)\n",
            "  Downloading glom-22.1.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.7/100.7 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: peewee~=3.14 in /usr/local/lib/python3.10/dist-packages (from semgrep<2.0.0,>=1.41.0->open-interpreter==0.1.7->metagpt==0.5.2) (3.17.0)\n",
            "Collecting ruamel.yaml>=0.17.10 (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2)\n",
            "  Downloading ruamel.yaml-0.17.40-py3-none-any.whl (113 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.7/113.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of semgrep to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting semgrep<2.0.0,>=1.41.0 (from open-interpreter==0.1.7->metagpt==0.5.2)\n",
            "  Downloading semgrep-1.55.2-cp38.cp39.cp310.cp311.py37.py38.py39.py310.py311-none-any.whl (34.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading semgrep-1.55.1-cp38.cp39.cp310.cp311.py37.py38.py39.py310.py311-none-any.whl (34.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading semgrep-1.55.0-cp38.cp39.cp310.cp311.py37.py38.py39.py310.py311-none-any.whl (34.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading semgrep-1.54.3-cp38.cp39.cp310.cp311.py37.py38.py39.py310.py311-none-any.whl (34.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading semgrep-1.54.2-cp38.cp39.cp310.cp311.py37.py38.py39.py310.py311-none-any.whl (34.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading semgrep-1.54.1-cp38.cp39.cp310.cp311.py37.py38.py39.py310.py311-none-any.whl (34.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.2/34.2 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading semgrep-1.54.0-cp38.cp39.cp310.cp311.py37.py38.py39.py310.py311-none-any.whl (34.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.2/34.2 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of semgrep to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading semgrep-1.53.0-cp38.cp39.cp310.cp311.py37.py38.py39.py310.py311-none-any.whl (34.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.2/34.2 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading semgrep-1.52.0-cp38.cp39.cp310.cp311.py37.py38.py39.py310.py311-none-any.whl (34.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.1/34.1 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wcmatch~=8.3 (from semgrep<2.0.0,>=1.41.0->open-interpreter==0.1.7->metagpt==0.5.2)\n",
            "  Downloading wcmatch-8.5-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.231->metagpt==0.5.2) (3.0.3)\n",
            "Requirement already satisfied: termcolor<3.0,>=2.3 in /usr/local/lib/python3.10/dist-packages (from yaspin<4.0.0,>=3.0.1->open-interpreter==0.1.7->metagpt==0.5.2) (2.4.0)\n",
            "Requirement already satisfied: decorator>=3.4.2 in /usr/local/lib/python3.10/dist-packages (from retry->lancedb==0.1.16->metagpt==0.5.2) (4.4.2)\n",
            "Collecting py<2.0.0,>=1.4.26 (from retry->lancedb==0.1.16->metagpt==0.5.2)\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.19.0->inquirer<4.0.0,>=3.1.3->open-interpreter==0.1.7->metagpt==0.5.2) (0.2.13)\n",
            "Collecting runs (from editor>=1.6.0->inquirer<4.0.0,>=3.1.3->open-interpreter==0.1.7->metagpt==0.5.2)\n",
            "  Downloading runs-1.2.0-py3-none-any.whl (6.9 kB)\n",
            "Collecting xmod (from editor>=1.6.0->inquirer<4.0.0,>=3.1.3->open-interpreter==0.1.7->metagpt==0.5.2)\n",
            "  Downloading xmod-1.8.1-py3-none-any.whl (4.6 kB)\n",
            "Collecting face>=20.1.0 (from glom~=22.1->semgrep<2.0.0,>=1.41.0->open-interpreter==0.1.7->metagpt==0.5.2)\n",
            "  Downloading face-22.0.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx<1,>=0.23.0->anthropic==0.3.6->metagpt==0.5.2)\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx<1,>=0.23.0->anthropic==0.3.6->metagpt==0.5.2)\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.8.0->litellm<0.2.0,>=0.1.590->open-interpreter==0.1.7->metagpt==0.5.2) (3.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm<0.2.0,>=0.1.590->open-interpreter==0.1.7->metagpt==0.5.2) (2.1.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (0.32.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (0.16.2)\n",
            "Collecting pathable<0.5.0,>=0.4.1 (from jsonschema-spec<0.3.0,>=0.2.3->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2)\n",
            "  Downloading pathable-0.4.3-py3-none-any.whl (9.6 kB)\n",
            "Collecting referencing>=0.28.4 (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2)\n",
            "  Downloading referencing-0.30.2-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.4.2->open-interpreter==0.1.7->metagpt==0.5.2) (0.1.2)\n",
            "Collecting rfc3339-validator (from openapi-schema-validator<0.7.0,>=0.6.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2)\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Collecting jsonschema-path<0.4.0,>=0.3.1 (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2)\n",
            "  Downloading jsonschema_path-0.3.2-py3-none-any.whl (14 kB)\n",
            "Collecting lazy-object-proxy<2.0.0,>=1.7.1 (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2)\n",
            "  Downloading lazy_object_proxy-1.10.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.3/68.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.10->prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2)\n",
            "  Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bracex>=2.1.1 (from wcmatch~=8.3->semgrep<2.0.0,>=1.41.0->open-interpreter==0.1.7->metagpt==0.5.2)\n",
            "  Downloading bracex-2.4-py3-none-any.whl (11 kB)\n",
            "INFO: pip is looking at multiple versions of jsonschema-specifications to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2)\n",
            "  Downloading jsonschema_specifications-2023.11.2-py3-none-any.whl (17 kB)\n",
            "  Downloading jsonschema_specifications-2023.11.1-py3-none-any.whl (17 kB)\n",
            "  Downloading jsonschema_specifications-2023.7.1-py3-none-any.whl (17 kB)\n",
            "Building wheels for collected packages: gitignore-parser, python-docx, ta, wget\n",
            "  Building wheel for gitignore-parser (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gitignore-parser: filename=gitignore_parser-0.1.9-py3-none-any.whl size=4956 sha256=ddb5f2610aabfc6f172a8d7631c48b89a62183237252160431aecc791c57a7e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/74/1a/fa/30ef804eda1ccc7f64be4784b16a5eb0822be8c6345458b87e\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184488 sha256=d6ebe7e11518fc2a3e41993f751ac5f451453972d56b90106756f263b09b8a50\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/27/06/837436d4c3bd989b957a91679966f207bfd71d358d63a8194d\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.10.2-py3-none-any.whl size=29088 sha256=45bc5be6939d96320be79a3255ce818a077e56d082438ed1217eb993467020eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/51/06/380dc516ea78621870b93ff65527c251afdfdc5fa9d7f4d248\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=5c4eb636ec21749fd2803a5338341e29e64ff73aa5cf38753c223155b1266616\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built gitignore-parser python-docx ta wget\n",
            "Installing collected packages: wget, ratelimiter, parse, gitignore-parser, faiss-cpu, dataclasses, boltons, attr, yaspin, xmod, wrapt, websocket-client, urllib3, tzdata, tqdm, tenacity, smmap, setuptools, semver, ruamel.yaml.clib, rfc3339-validator, referencing, python-dotenv, python-docx, pytest, pydantic, py, protobuf, portalocker, pathable, numpy, mypy-extensions, marshmallow, loguru, lazy-object-proxy, isodate, hyperframe, hpack, h11, face, colorama, click-option-group, camel-converter, bracex, blessed, beautifulsoup4, astor, asgiref, aiofiles, wcmatch, typing-inspect, runs, ruamel.yaml, retry, readchar, pytest-mock, pandas, openapi-schema-pydantic, jsonschema-specifications, httpcore, h2, grpcio-tools, glom, gitdb, Django, aiohttp, zhipuai, tiktoken, ta, pylance, prance, openai, meilisearch, libcst, langchainplus-sdk, jsonschema-spec, jsonschema-path, huggingface-hub, httpx, gitpython, editor, dataclasses-json, channels, tokentrim, semgrep, openapi-schema-validator, langchain, lancedb, inquirer, git-python, qdrant-client, openapi-spec-validator, litellm, anthropic, openapi_core, open-interpreter, semantic-kernel, metagpt\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.14.1\n",
            "    Uninstalling wrapt-1.14.1:\n",
            "      Successfully uninstalled wrapt-1.14.1\n",
            "  Attempting uninstall: websocket-client\n",
            "    Found existing installation: websocket-client 1.7.0\n",
            "    Uninstalling websocket-client-1.7.0:\n",
            "      Successfully uninstalled websocket-client-1.7.0\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.1\n",
            "    Uninstalling tqdm-4.66.1:\n",
            "      Successfully uninstalled tqdm-4.66.1\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 8.2.3\n",
            "    Uninstalling tenacity-8.2.3:\n",
            "      Successfully uninstalled tenacity-8.2.3\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "  Attempting uninstall: referencing\n",
            "    Found existing installation: referencing 0.32.1\n",
            "    Uninstalling referencing-0.32.1:\n",
            "      Successfully uninstalled referencing-0.32.1\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 7.4.4\n",
            "    Uninstalling pytest-7.4.4:\n",
            "      Successfully uninstalled pytest-7.4.4\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.13\n",
            "    Uninstalling pydantic-1.10.13:\n",
            "      Successfully uninstalled pydantic-1.10.13\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.11.2\n",
            "    Uninstalling beautifulsoup4-4.11.2:\n",
            "      Successfully uninstalled beautifulsoup4-4.11.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "  Attempting uninstall: jsonschema-specifications\n",
            "    Found existing installation: jsonschema-specifications 2023.12.1\n",
            "    Uninstalling jsonschema-specifications-2023.12.1:\n",
            "      Successfully uninstalled jsonschema-specifications-2023.12.1\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.9.1\n",
            "    Uninstalling aiohttp-3.9.1:\n",
            "      Successfully uninstalled aiohttp-3.9.1\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.2\n",
            "    Uninstalling huggingface-hub-0.20.2:\n",
            "      Successfully uninstalled huggingface-hub-0.20.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.0.3 which is incompatible.\n",
            "tensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 4.25.2 which is incompatible.\n",
            "tensorflow 2.15.0 requires wrapt<1.15,>=1.11.0, but you have wrapt 1.15.0 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Django-5.0.1 aiofiles-23.2.1 aiohttp-3.8.4 anthropic-0.3.6 asgiref-3.7.2 astor-0.8.1 attr-0.3.2 beautifulsoup4-4.12.2 blessed-1.20.0 boltons-21.0.0 bracex-2.4 camel-converter-3.1.1 channels-4.0.0 click-option-group-0.5.6 colorama-0.4.6 dataclasses-0.6 dataclasses-json-0.5.14 editor-1.6.5 face-22.0.0 faiss-cpu-1.7.4 git-python-1.0.3 gitdb-4.0.11 gitignore-parser-0.1.9 gitpython-3.1.40 glom-22.1.0 grpcio-tools-1.60.0 h11-0.14.0 h2-4.1.0 hpack-4.0.0 httpcore-1.0.2 httpx-0.26.0 huggingface-hub-0.16.4 hyperframe-6.0.1 inquirer-3.2.1 isodate-0.6.1 jsonschema-path-0.3.2 jsonschema-spec-0.2.4 jsonschema-specifications-2023.7.1 lancedb-0.1.16 langchain-0.0.231 langchainplus-sdk-0.0.20 lazy-object-proxy-1.10.0 libcst-1.0.1 litellm-0.1.824 loguru-0.6.0 marshmallow-3.20.2 meilisearch-0.21.0 metagpt-0.5.2 mypy-extensions-1.0.0 numpy-1.24.3 open-interpreter-0.1.7 openai-0.28.1 openapi-schema-pydantic-1.2.4 openapi-schema-validator-0.6.2 openapi-spec-validator-0.7.1 openapi_core-0.18.2 pandas-2.0.3 parse-1.20.0 pathable-0.4.3 portalocker-2.8.2 prance-23.6.21.0 protobuf-4.25.2 py-1.11.0 pydantic-1.10.8 pylance-0.5.10 pytest-7.2.2 pytest-mock-3.11.1 python-docx-0.8.11 python-dotenv-1.0.0 qdrant-client-1.4.0 ratelimiter-1.2.0.post0 readchar-4.0.5 referencing-0.30.2 retry-0.9.2 rfc3339-validator-0.1.4 ruamel.yaml-0.17.40 ruamel.yaml.clib-0.2.8 runs-1.2.0 semantic-kernel-0.3.13.dev0 semgrep-1.52.0 semver-3.0.2 setuptools-65.6.3 smmap-5.0.1 ta-0.10.2 tenacity-8.2.2 tiktoken-0.4.0 tokentrim-0.1.13 tqdm-4.64.0 typing-inspect-0.8.0 tzdata-2023.4 urllib3-1.26.18 wcmatch-8.5 websocket-client-0.58.0 wget-3.2 wrapt-1.15.0 xmod-1.8.1 yaspin-3.0.1 zhipuai-1.0.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "dataclasses",
                  "numpy",
                  "pkg_resources",
                  "setuptools"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install aiocron discord"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRxaG0NV8LX-",
        "outputId": "062052fd-14e7-49a8-8561-c247b6d4c397"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aiocron\n",
            "  Downloading aiocron-1.8-py3-none-any.whl (4.8 kB)\n",
            "Collecting discord\n",
            "  Downloading discord-2.3.2-py3-none-any.whl (1.1 kB)\n",
            "Collecting croniter (from aiocron)\n",
            "  Downloading croniter-2.0.1-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from aiocron) (5.2)\n",
            "Collecting discord.py>=2.3.2 (from discord)\n",
            "  Downloading discord.py-2.3.2-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp<4,>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from discord.py>=2.3.2->discord) (3.8.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from croniter->aiocron) (2.8.2)\n",
            "Requirement already satisfied: pytz>2021.1 in /usr/local/lib/python3.10/dist-packages (from croniter->aiocron) (2023.3.post1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.7.4->discord.py>=2.3.2->discord) (23.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.7.4->discord.py>=2.3.2->discord) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.7.4->discord.py>=2.3.2->discord) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.7.4->discord.py>=2.3.2->discord) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.7.4->discord.py>=2.3.2->discord) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.7.4->discord.py>=2.3.2->discord) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.7.4->discord.py>=2.3.2->discord) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->croniter->aiocron) (1.16.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp<4,>=3.7.4->discord.py>=2.3.2->discord) (3.6)\n",
            "Installing collected packages: croniter, discord.py, aiocron, discord\n",
            "Successfully installed aiocron-1.8 croniter-2.0.1 discord-2.3.2 discord.py-2.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RJdjWa2D8fn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import aiohttp\n",
        "import asyncio\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "async def fetch_html(url):\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        async with session.get(url) as response:\n",
        "            return await response.text()\n",
        "\n",
        "async def parse_main_page(html):\n",
        "    title_list = []\n",
        "    href_list = []\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    # 更新查找标签的逻辑以匹配当前网页结构\n",
        "    title_tags = soup.find_all('h3', class_='mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl')\n",
        "    for title_tag in title_tags:\n",
        "        a_tag = title_tag.find('a')  # 标题内的<a>标签\n",
        "        if a_tag:\n",
        "            title = a_tag.text.strip()  # 清除空白字符得到标题文本\n",
        "            href = a_tag['href']  # 提取href属性\n",
        "            title_list.append(title)  # 添加标题到列表\n",
        "            href_list.append(href)  # 添加链接到列表\n",
        "    return title_list, href_list\n",
        "\n",
        "async def parse_sub_page(html):\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    abstract = soup.find('div', class_=\"pb-8 pr-4 md:pr-16\").p.text\n",
        "    arxiv_url = soup.find('a', class_=\"btn inline-flex h-9 items-center\", href=True)['href']\n",
        "    return abstract, arxiv_url\n",
        "\n",
        "async def main():\n",
        "    url = 'https://huggingface.co/papers'\n",
        "    base_url = 'https://huggingface.co'\n",
        "    repositories = []\n",
        "    try:\n",
        "        html = await fetch_html(url)\n",
        "        title_list, href_list = await parse_main_page(html)\n",
        "\n",
        "        for title, href in zip(title_list, href_list):\n",
        "            repo_info = {}\n",
        "            repo_info['title'] = title\n",
        "            # repo_info['href'] = href\n",
        "            repositories.append(repo_info)\n",
        "            # print(title, href)\n",
        "            sub_html = await fetch_html(base_url + href)\n",
        "            abstract, arxiv_url = await parse_sub_page(sub_html)\n",
        "            # print(abstract, arxiv_url)\n",
        "            repo_info['abstract'] = abstract\n",
        "            repo_info['arxiv_url'] = arxiv_url\n",
        "            repositories.append(repo_info)\n",
        "        return repositories\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5cAtKz7GLh21"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repositories = await main()\n",
        "repositories"
      ],
      "metadata": {
        "id": "skQN364GNqZ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4396570-f64f-4aac-eba0-302ffbb97770"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference',\n",
              "  'abstract': \"The deployment and scaling of large language models (LLMs) have become\\ncritical as they permeate various applications, demanding high-throughput and\\nlow-latency serving systems. Existing frameworks struggle to balance these\\nrequirements, especially for workloads with long prompts. This paper introduces\\nDeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and\\ngeneration composition strategy, to deliver up to 2.3x higher effective\\nthroughput, 2x lower latency on average, and up to 3.7x lower (token-level)\\ntail latency, compared to state-of-the-art systems like vLLM. We leverage a\\nsynergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an\\nefficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced\\nimplementation supports a range of models and offers both non-persistent and\\npersistent deployment options, catering to diverse user scenarios from\\ninteractive sessions to long-running applications. We present a detailed\\nbenchmarking methodology, analyze the performance through latency-throughput\\ncurves, and investigate scalability via load balancing. Our evaluations\\ndemonstrate substantial improvements in throughput and latency across various\\nmodels and hardware configurations. We discuss our roadmap for future\\nenhancements, including broader model support and new hardware backends. The\\nDeepSpeed-FastGen code is readily available for community engagement and\\ncontribution.\",\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.08671'},\n",
              " {'title': 'DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference',\n",
              "  'abstract': \"The deployment and scaling of large language models (LLMs) have become\\ncritical as they permeate various applications, demanding high-throughput and\\nlow-latency serving systems. Existing frameworks struggle to balance these\\nrequirements, especially for workloads with long prompts. This paper introduces\\nDeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and\\ngeneration composition strategy, to deliver up to 2.3x higher effective\\nthroughput, 2x lower latency on average, and up to 3.7x lower (token-level)\\ntail latency, compared to state-of-the-art systems like vLLM. We leverage a\\nsynergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an\\nefficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced\\nimplementation supports a range of models and offers both non-persistent and\\npersistent deployment options, catering to diverse user scenarios from\\ninteractive sessions to long-running applications. We present a detailed\\nbenchmarking methodology, analyze the performance through latency-throughput\\ncurves, and investigate scalability via load balancing. Our evaluations\\ndemonstrate substantial improvements in throughput and latency across various\\nmodels and hardware configurations. We discuss our roadmap for future\\nenhancements, including broader model support and new hardware backends. The\\nDeepSpeed-FastGen code is readily available for community engagement and\\ncontribution.\",\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.08671'},\n",
              " {'title': 'Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model',\n",
              "  'abstract': 'Recently the state space models (SSMs) with efficient hardware-aware designs,\\ni.e., Mamba, have shown great potential for long sequence modeling. Building\\nefficient and generic vision backbones purely upon SSMs is an appealing\\ndirection. However, representing visual data is challenging for SSMs due to the\\nposition-sensitivity of visual data and the requirement of global context for\\nvisual understanding. In this paper, we show that the reliance of visual\\nrepresentation learning on self-attention is not necessary and propose a new\\ngeneric vision backbone with bidirectional Mamba blocks (Vim), which marks the\\nimage sequences with position embeddings and compresses the visual\\nrepresentation with bidirectional state space models. On ImageNet\\nclassification, COCO object detection, and ADE20k semantic segmentation tasks,\\nVim achieves higher performance compared to well-established vision\\ntransformers like DeiT, while also demonstrating significantly improved\\ncomputation & memory efficiency. For example, Vim is 2.8times faster than\\nDeiT and saves 86.8% GPU memory when performing batch inference to extract\\nfeatures on images with a resolution of 1248times1248. The results\\ndemonstrate that Vim is capable of overcoming the computation & memory\\nconstraints on performing Transformer-style understanding for high-resolution\\nimages and it has great potential to become the next-generation backbone for\\nvision foundation models. Code is available at https://github.com/hustvl/Vim.',\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.09417'},\n",
              " {'title': 'Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model',\n",
              "  'abstract': 'Recently the state space models (SSMs) with efficient hardware-aware designs,\\ni.e., Mamba, have shown great potential for long sequence modeling. Building\\nefficient and generic vision backbones purely upon SSMs is an appealing\\ndirection. However, representing visual data is challenging for SSMs due to the\\nposition-sensitivity of visual data and the requirement of global context for\\nvisual understanding. In this paper, we show that the reliance of visual\\nrepresentation learning on self-attention is not necessary and propose a new\\ngeneric vision backbone with bidirectional Mamba blocks (Vim), which marks the\\nimage sequences with position embeddings and compresses the visual\\nrepresentation with bidirectional state space models. On ImageNet\\nclassification, COCO object detection, and ADE20k semantic segmentation tasks,\\nVim achieves higher performance compared to well-established vision\\ntransformers like DeiT, while also demonstrating significantly improved\\ncomputation & memory efficiency. For example, Vim is 2.8times faster than\\nDeiT and saves 86.8% GPU memory when performing batch inference to extract\\nfeatures on images with a resolution of 1248times1248. The results\\ndemonstrate that Vim is capable of overcoming the computation & memory\\nconstraints on performing Transformer-style understanding for high-resolution\\nimages and it has great potential to become the next-generation backbone for\\nvision foundation models. Code is available at https://github.com/hustvl/Vim.',\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.09417'},\n",
              " {'title': 'Asynchronous Local-SGD Training for Language Modeling',\n",
              "  'abstract': \"Local stochastic gradient descent (Local-SGD), also referred to as federated\\naveraging, is an approach to distributed optimization where each device\\nperforms more than one SGD update per communication. This work presents an\\nempirical study of {\\\\it asynchronous} Local-SGD for training language models;\\nthat is, each worker updates the global parameters as soon as it has finished\\nits SGD steps. We conduct a comprehensive investigation by examining how worker\\nhardware heterogeneity, model size, number of workers, and optimizer could\\nimpact the learning performance. We find that with naive implementations,\\nasynchronous Local-SGD takes more iterations to converge than its synchronous\\ncounterpart despite updating the (global) model parameters more frequently. We\\nidentify momentum acceleration on the global parameters when worker gradients\\nare stale as a key challenge. We propose a novel method that utilizes a delayed\\nNesterov momentum update and adjusts the workers' local training steps based on\\ntheir computation speed. This approach, evaluated with models up to 150M\\nparameters on the C4 dataset, matches the performance of synchronous Local-SGD\\nin terms of perplexity per update step, and significantly surpasses it in terms\\nof wall clock time.\",\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.09135'},\n",
              " {'title': 'Asynchronous Local-SGD Training for Language Modeling',\n",
              "  'abstract': \"Local stochastic gradient descent (Local-SGD), also referred to as federated\\naveraging, is an approach to distributed optimization where each device\\nperforms more than one SGD update per communication. This work presents an\\nempirical study of {\\\\it asynchronous} Local-SGD for training language models;\\nthat is, each worker updates the global parameters as soon as it has finished\\nits SGD steps. We conduct a comprehensive investigation by examining how worker\\nhardware heterogeneity, model size, number of workers, and optimizer could\\nimpact the learning performance. We find that with naive implementations,\\nasynchronous Local-SGD takes more iterations to converge than its synchronous\\ncounterpart despite updating the (global) model parameters more frequently. We\\nidentify momentum acceleration on the global parameters when worker gradients\\nare stale as a key challenge. We propose a novel method that utilizes a delayed\\nNesterov momentum update and adjusts the workers' local training steps based on\\ntheir computation speed. This approach, evaluated with models up to 150M\\nparameters on the C4 dataset, matches the performance of synchronous Local-SGD\\nin terms of perplexity per update step, and significantly surpasses it in terms\\nof wall clock time.\",\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.09135'},\n",
              " {'title': 'ReFT: Reasoning with Reinforced Fine-Tuning',\n",
              "  'abstract': 'One way to enhance the reasoning capability of Large Language Models (LLMs)\\nis to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT)\\nannotations. This approach does not show sufficiently strong generalization\\nability, however, because the training only relies on the given CoT data. In\\nmath problem-solving, for example, there is usually only one annotated\\nreasoning path for each question in the training data. Intuitively, it would be\\nbetter for the algorithm to learn from multiple annotated reasoning paths given\\na question. To address this issue, we propose a simple yet effective approach\\ncalled Reinforced Fine-Tuning (ReFT) to enhance the generalizability of\\nlearning LLMs for reasoning, with math problem-solving as an example. ReFT\\nfirst warmups the model with SFT, and then employs on-line reinforcement\\nlearning, specifically the PPO algorithm in this paper, to further fine-tune\\nthe model, where an abundance of reasoning paths are automatically sampled\\ngiven the question and the rewards are naturally derived from the ground-truth\\nanswers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that\\nReFT significantly outperforms SFT, and the performance can be potentially\\nfurther boosted by combining inference-time strategies such as majority voting\\nand re-ranking. Note that ReFT obtains the improvement by learning from the\\nsame training questions as SFT, without relying on extra or augmented training\\nquestions. This indicates a superior generalization ability for ReFT.',\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.08967'},\n",
              " {'title': 'ReFT: Reasoning with Reinforced Fine-Tuning',\n",
              "  'abstract': 'One way to enhance the reasoning capability of Large Language Models (LLMs)\\nis to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT)\\nannotations. This approach does not show sufficiently strong generalization\\nability, however, because the training only relies on the given CoT data. In\\nmath problem-solving, for example, there is usually only one annotated\\nreasoning path for each question in the training data. Intuitively, it would be\\nbetter for the algorithm to learn from multiple annotated reasoning paths given\\na question. To address this issue, we propose a simple yet effective approach\\ncalled Reinforced Fine-Tuning (ReFT) to enhance the generalizability of\\nlearning LLMs for reasoning, with math problem-solving as an example. ReFT\\nfirst warmups the model with SFT, and then employs on-line reinforcement\\nlearning, specifically the PPO algorithm in this paper, to further fine-tune\\nthe model, where an abundance of reasoning paths are automatically sampled\\ngiven the question and the rewards are naturally derived from the ground-truth\\nanswers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that\\nReFT significantly outperforms SFT, and the performance can be potentially\\nfurther boosted by combining inference-time strategies such as majority voting\\nand re-ranking. Note that ReFT obtains the improvement by learning from the\\nsame training questions as SFT, without relying on extra or augmented training\\nquestions. This indicates a superior generalization ability for ReFT.',\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.08967'},\n",
              " {'title': 'UniVG: Towards UNIfied-modal Video Generation',\n",
              "  'abstract': \"Diffusion based video generation has received extensive attention and\\nachieved considerable success within both the academic and industrial\\ncommunities. However, current efforts are mainly concentrated on\\nsingle-objective or single-task video generation, such as generation driven by\\ntext, by image, or by a combination of text and image. This cannot fully meet\\nthe needs of real-world application scenarios, as users are likely to input\\nimages and text conditions in a flexible manner, either individually or in\\ncombination. To address this, we propose a Unified-modal Video Genearation\\nsystem that is capable of handling multiple video generation tasks across text\\nand image modalities. To this end, we revisit the various video generation\\ntasks within our system from the perspective of generative freedom, and\\nclassify them into high-freedom and low-freedom video generation categories.\\nFor high-freedom video generation, we employ Multi-condition Cross Attention to\\ngenerate videos that align with the semantics of the input images or text. For\\nlow-freedom video generation, we introduce Biased Gaussian Noise to replace the\\npure random Gaussian Noise, which helps to better preserve the content of the\\ninput conditions. Our method achieves the lowest Fr\\\\'echet Video Distance (FVD)\\non the public academic benchmark MSR-VTT, surpasses the current open-source\\nmethods in human evaluations, and is on par with the current close-source\\nmethod Gen2. For more samples, visit https://univg-baidu.github.io.\",\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.09084'},\n",
              " {'title': 'UniVG: Towards UNIfied-modal Video Generation',\n",
              "  'abstract': \"Diffusion based video generation has received extensive attention and\\nachieved considerable success within both the academic and industrial\\ncommunities. However, current efforts are mainly concentrated on\\nsingle-objective or single-task video generation, such as generation driven by\\ntext, by image, or by a combination of text and image. This cannot fully meet\\nthe needs of real-world application scenarios, as users are likely to input\\nimages and text conditions in a flexible manner, either individually or in\\ncombination. To address this, we propose a Unified-modal Video Genearation\\nsystem that is capable of handling multiple video generation tasks across text\\nand image modalities. To this end, we revisit the various video generation\\ntasks within our system from the perspective of generative freedom, and\\nclassify them into high-freedom and low-freedom video generation categories.\\nFor high-freedom video generation, we employ Multi-condition Cross Attention to\\ngenerate videos that align with the semantics of the input images or text. For\\nlow-freedom video generation, we introduce Biased Gaussian Noise to replace the\\npure random Gaussian Noise, which helps to better preserve the content of the\\ninput conditions. Our method achieves the lowest Fr\\\\'echet Video Distance (FVD)\\non the public academic benchmark MSR-VTT, surpasses the current open-source\\nmethods in human evaluations, and is on par with the current close-source\\nmethod Gen2. For more samples, visit https://univg-baidu.github.io.\",\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.09084'},\n",
              " {'title': 'VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models',\n",
              "  'abstract': 'Text-to-video generation aims to produce a video based on a given prompt.\\nRecently, several commercial video models have been able to generate plausible\\nvideos with minimal noise, excellent details, and high aesthetic scores.\\nHowever, these models rely on large-scale, well-filtered, high-quality videos\\nthat are not accessible to the community. Many existing research works, which\\ntrain models using the low-quality WebVid-10M dataset, struggle to generate\\nhigh-quality videos because the models are optimized to fit WebVid-10M. In this\\nwork, we explore the training scheme of video models extended from Stable\\nDiffusion and investigate the feasibility of leveraging low-quality videos and\\nsynthesized high-quality images to obtain a high-quality video model. We first\\nanalyze the connection between the spatial and temporal modules of video models\\nand the distribution shift to low-quality videos. We observe that full training\\nof all modules results in a stronger coupling between spatial and temporal\\nmodules than only training temporal modules. Based on this stronger coupling,\\nwe shift the distribution to higher quality without motion degradation by\\nfinetuning spatial modules with high-quality images, resulting in a generic\\nhigh-quality video model. Evaluations are conducted to demonstrate the\\nsuperiority of the proposed method, particularly in picture quality, motion,\\nand concept composition.',\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.09047'},\n",
              " {'title': 'VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models',\n",
              "  'abstract': 'Text-to-video generation aims to produce a video based on a given prompt.\\nRecently, several commercial video models have been able to generate plausible\\nvideos with minimal noise, excellent details, and high aesthetic scores.\\nHowever, these models rely on large-scale, well-filtered, high-quality videos\\nthat are not accessible to the community. Many existing research works, which\\ntrain models using the low-quality WebVid-10M dataset, struggle to generate\\nhigh-quality videos because the models are optimized to fit WebVid-10M. In this\\nwork, we explore the training scheme of video models extended from Stable\\nDiffusion and investigate the feasibility of leveraging low-quality videos and\\nsynthesized high-quality images to obtain a high-quality video model. We first\\nanalyze the connection between the spatial and temporal modules of video models\\nand the distribution shift to low-quality videos. We observe that full training\\nof all modules results in a stronger coupling between spatial and temporal\\nmodules than only training temporal modules. Based on this stronger coupling,\\nwe shift the distribution to higher quality without motion degradation by\\nfinetuning spatial modules with high-quality images, resulting in a generic\\nhigh-quality video model. Evaluations are conducted to demonstrate the\\nsuperiority of the proposed method, particularly in picture quality, motion,\\nand concept composition.',\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.09047'},\n",
              " {'title': 'SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers',\n",
              "  'abstract': 'We present Scalable Interpolant Transformers (SiT), a family of generative\\nmodels built on the backbone of Diffusion Transformers (DiT). The interpolant\\nframework, which allows for connecting two distributions in a more flexible way\\nthan standard diffusion models, makes possible a modular study of various\\ndesign choices impacting generative models built on dynamical transport: using\\ndiscrete vs. continuous time learning, deciding the objective for the model to\\nlearn, choosing the interpolant connecting the distributions, and deploying a\\ndeterministic or stochastic sampler. By carefully introducing the above\\ningredients, SiT surpasses DiT uniformly across model sizes on the conditional\\nImageNet 256x256 benchmark using the exact same backbone, number of parameters,\\nand GFLOPs. By exploring various diffusion coefficients, which can be tuned\\nseparately from learning, SiT achieves an FID-50K score of 2.06.',\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.08740'},\n",
              " {'title': 'SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers',\n",
              "  'abstract': 'We present Scalable Interpolant Transformers (SiT), a family of generative\\nmodels built on the backbone of Diffusion Transformers (DiT). The interpolant\\nframework, which allows for connecting two distributions in a more flexible way\\nthan standard diffusion models, makes possible a modular study of various\\ndesign choices impacting generative models built on dynamical transport: using\\ndiscrete vs. continuous time learning, deciding the objective for the model to\\nlearn, choosing the interpolant connecting the distributions, and deploying a\\ndeterministic or stochastic sampler. By carefully introducing the above\\ningredients, SiT surpasses DiT uniformly across model sizes on the conditional\\nImageNet 256x256 benchmark using the exact same backbone, number of parameters,\\nand GFLOPs. By exploring various diffusion coefficients, which can be tuned\\nseparately from learning, SiT achieves an FID-50K score of 2.06.',\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.08740'},\n",
              " {'title': 'ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization',\n",
              "  'abstract': 'Neural Radiance Fields (NeRF) exhibit remarkable performance for Novel View\\nSynthesis (NVS) given a set of 2D images. However, NeRF training requires\\naccurate camera pose for each input view, typically obtained by\\nStructure-from-Motion (SfM) pipelines. Recent works have attempted to relax\\nthis constraint, but they still often rely on decent initial poses which they\\ncan refine. Here we aim at removing the requirement for pose initialization. We\\npresent Incremental CONfidence (ICON), an optimization procedure for training\\nNeRFs from 2D video frames. ICON only assumes smooth camera motion to estimate\\ninitial guess for poses. Further, ICON introduces ``confidence\": an adaptive\\nmeasure of model quality used to dynamically reweight gradients. ICON relies on\\nhigh-confidence poses to learn NeRF, and high-confidence 3D structure (as\\nencoded by NeRF) to learn poses. We show that ICON, without prior pose\\ninitialization, achieves superior performance in both CO3D and HO3D versus\\nmethods which use SfM pose.',\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.08937'},\n",
              " {'title': 'ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization',\n",
              "  'abstract': 'Neural Radiance Fields (NeRF) exhibit remarkable performance for Novel View\\nSynthesis (NVS) given a set of 2D images. However, NeRF training requires\\naccurate camera pose for each input view, typically obtained by\\nStructure-from-Motion (SfM) pipelines. Recent works have attempted to relax\\nthis constraint, but they still often rely on decent initial poses which they\\ncan refine. Here we aim at removing the requirement for pose initialization. We\\npresent Incremental CONfidence (ICON), an optimization procedure for training\\nNeRFs from 2D video frames. ICON only assumes smooth camera motion to estimate\\ninitial guess for poses. Further, ICON introduces ``confidence\": an adaptive\\nmeasure of model quality used to dynamically reweight gradients. ICON relies on\\nhigh-confidence poses to learn NeRF, and high-confidence 3D structure (as\\nencoded by NeRF) to learn poses. We show that ICON, without prior pose\\ninitialization, achieves superior performance in both CO3D and HO3D versus\\nmethods which use SfM pose.',\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.08937'},\n",
              " {'title': 'TextureDreamer: Image-guided Texture Synthesis through Geometry-aware Diffusion',\n",
              "  'abstract': 'We present TextureDreamer, a novel image-guided texture synthesis method to\\ntransfer relightable textures from a small number of input images (3 to 5) to\\ntarget 3D shapes across arbitrary categories. Texture creation is a pivotal\\nchallenge in vision and graphics. Industrial companies hire experienced artists\\nto manually craft textures for 3D assets. Classical methods require densely\\nsampled views and accurately aligned geometry, while learning-based methods are\\nconfined to category-specific shapes within the dataset. In contrast,\\nTextureDreamer can transfer highly detailed, intricate textures from real-world\\nenvironments to arbitrary objects with only a few casually captured images,\\npotentially significantly democratizing texture creation. Our core idea,\\npersonalized geometry-aware score distillation (PGSD), draws inspiration from\\nrecent advancements in diffuse models, including personalized modeling for\\ntexture information extraction, variational score distillation for detailed\\nappearance synthesis, and explicit geometry guidance with ControlNet. Our\\nintegration and several essential modifications substantially improve the\\ntexture quality. Experiments on real images spanning different categories show\\nthat TextureDreamer can successfully transfer highly realistic, semantic\\nmeaningful texture to arbitrary objects, surpassing the visual quality of\\nprevious state-of-the-art.',\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.09416'},\n",
              " {'title': 'TextureDreamer: Image-guided Texture Synthesis through Geometry-aware Diffusion',\n",
              "  'abstract': 'We present TextureDreamer, a novel image-guided texture synthesis method to\\ntransfer relightable textures from a small number of input images (3 to 5) to\\ntarget 3D shapes across arbitrary categories. Texture creation is a pivotal\\nchallenge in vision and graphics. Industrial companies hire experienced artists\\nto manually craft textures for 3D assets. Classical methods require densely\\nsampled views and accurately aligned geometry, while learning-based methods are\\nconfined to category-specific shapes within the dataset. In contrast,\\nTextureDreamer can transfer highly detailed, intricate textures from real-world\\nenvironments to arbitrary objects with only a few casually captured images,\\npotentially significantly democratizing texture creation. Our core idea,\\npersonalized geometry-aware score distillation (PGSD), draws inspiration from\\nrecent advancements in diffuse models, including personalized modeling for\\ntexture information extraction, variational score distillation for detailed\\nappearance synthesis, and explicit geometry guidance with ControlNet. Our\\nintegration and several essential modifications substantially improve the\\ntexture quality. Experiments on real images spanning different categories show\\nthat TextureDreamer can successfully transfer highly realistic, semantic\\nmeaningful texture to arbitrary objects, surpassing the visual quality of\\nprevious state-of-the-art.',\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.09416'}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Action: CrawlHuggingfaceDailyPaper"
      ],
      "metadata": {
        "id": "fUIpO-X34zm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from metagpt.actions.action import Action\n",
        "from metagpt.config import CONFIG\n",
        "\n",
        "class CrawlHuggingfaceDailyPaper(Action):\n",
        "    \"\"\"\n",
        "    This class specifically targets the daily papers section of the Huggingface website.\n",
        "    Its main functionality includes asynchronously fetching and parsing the latest research papers\n",
        "    published on Huggingface, extracting relevant details such as titles, abstracts, and arXiv URLs.\n",
        "    It can be utilized in applications where up-to-date research information from Huggingface\n",
        "    is required, making it a valuable tool for researchers and developers in AI and machine learning.\n",
        "    \"\"\"\n",
        "\n",
        "    async def run(self, url: str = \"https://huggingface.co/papers\"):\n",
        "        async with aiohttp.ClientSession() as client:\n",
        "            async with client.get(url, proxy=CONFIG.global_proxy) as response:\n",
        "                response.raise_for_status()\n",
        "                html = await response.text()\n",
        "\n",
        "        title_list, href_list = await parse_main_page(html)\n",
        "\n",
        "        repositories = []\n",
        "        base_url = 'https://huggingface.co'\n",
        "\n",
        "        for title, href in zip(title_list, href_list):\n",
        "            repo_info = {'title': title}\n",
        "            sub_html = await fetch_html(base_url + href)\n",
        "            abstract, arxiv_url = await parse_sub_page(sub_html)\n",
        "            repo_info['abstract'] = abstract\n",
        "            repo_info['arxiv_url'] = arxiv_url\n",
        "\n",
        "            repositories.append(repo_info)\n",
        "\n",
        "        return repositories\n"
      ],
      "metadata": {
        "id": "Hp1W0OiAOU_6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "craw_paper_action = CrawlHuggingfaceDailyPaper()\n",
        "resp = await craw_paper_action.run()\n",
        "resp"
      ],
      "metadata": {
        "id": "s2sDPcbEQaDr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d5d79ac-da3d-4bc8-ef65-be17659db47f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference',\n",
              "  'abstract': \"The deployment and scaling of large language models (LLMs) have become\\ncritical as they permeate various applications, demanding high-throughput and\\nlow-latency serving systems. Existing frameworks struggle to balance these\\nrequirements, especially for workloads with long prompts. This paper introduces\\nDeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and\\ngeneration composition strategy, to deliver up to 2.3x higher effective\\nthroughput, 2x lower latency on average, and up to 3.7x lower (token-level)\\ntail latency, compared to state-of-the-art systems like vLLM. We leverage a\\nsynergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an\\nefficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced\\nimplementation supports a range of models and offers both non-persistent and\\npersistent deployment options, catering to diverse user scenarios from\\ninteractive sessions to long-running applications. We present a detailed\\nbenchmarking methodology, analyze the performance through latency-throughput\\ncurves, and investigate scalability via load balancing. Our evaluations\\ndemonstrate substantial improvements in throughput and latency across various\\nmodels and hardware configurations. We discuss our roadmap for future\\nenhancements, including broader model support and new hardware backends. The\\nDeepSpeed-FastGen code is readily available for community engagement and\\ncontribution.\",\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.08671'},\n",
              " {'title': 'Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model',\n",
              "  'abstract': 'Recently the state space models (SSMs) with efficient hardware-aware designs,\\ni.e., Mamba, have shown great potential for long sequence modeling. Building\\nefficient and generic vision backbones purely upon SSMs is an appealing\\ndirection. However, representing visual data is challenging for SSMs due to the\\nposition-sensitivity of visual data and the requirement of global context for\\nvisual understanding. In this paper, we show that the reliance of visual\\nrepresentation learning on self-attention is not necessary and propose a new\\ngeneric vision backbone with bidirectional Mamba blocks (Vim), which marks the\\nimage sequences with position embeddings and compresses the visual\\nrepresentation with bidirectional state space models. On ImageNet\\nclassification, COCO object detection, and ADE20k semantic segmentation tasks,\\nVim achieves higher performance compared to well-established vision\\ntransformers like DeiT, while also demonstrating significantly improved\\ncomputation & memory efficiency. For example, Vim is 2.8times faster than\\nDeiT and saves 86.8% GPU memory when performing batch inference to extract\\nfeatures on images with a resolution of 1248times1248. The results\\ndemonstrate that Vim is capable of overcoming the computation & memory\\nconstraints on performing Transformer-style understanding for high-resolution\\nimages and it has great potential to become the next-generation backbone for\\nvision foundation models. Code is available at https://github.com/hustvl/Vim.',\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.09417'},\n",
              " {'title': 'Asynchronous Local-SGD Training for Language Modeling',\n",
              "  'abstract': \"Local stochastic gradient descent (Local-SGD), also referred to as federated\\naveraging, is an approach to distributed optimization where each device\\nperforms more than one SGD update per communication. This work presents an\\nempirical study of {\\\\it asynchronous} Local-SGD for training language models;\\nthat is, each worker updates the global parameters as soon as it has finished\\nits SGD steps. We conduct a comprehensive investigation by examining how worker\\nhardware heterogeneity, model size, number of workers, and optimizer could\\nimpact the learning performance. We find that with naive implementations,\\nasynchronous Local-SGD takes more iterations to converge than its synchronous\\ncounterpart despite updating the (global) model parameters more frequently. We\\nidentify momentum acceleration on the global parameters when worker gradients\\nare stale as a key challenge. We propose a novel method that utilizes a delayed\\nNesterov momentum update and adjusts the workers' local training steps based on\\ntheir computation speed. This approach, evaluated with models up to 150M\\nparameters on the C4 dataset, matches the performance of synchronous Local-SGD\\nin terms of perplexity per update step, and significantly surpasses it in terms\\nof wall clock time.\",\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.09135'},\n",
              " {'title': 'ReFT: Reasoning with Reinforced Fine-Tuning',\n",
              "  'abstract': 'One way to enhance the reasoning capability of Large Language Models (LLMs)\\nis to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT)\\nannotations. This approach does not show sufficiently strong generalization\\nability, however, because the training only relies on the given CoT data. In\\nmath problem-solving, for example, there is usually only one annotated\\nreasoning path for each question in the training data. Intuitively, it would be\\nbetter for the algorithm to learn from multiple annotated reasoning paths given\\na question. To address this issue, we propose a simple yet effective approach\\ncalled Reinforced Fine-Tuning (ReFT) to enhance the generalizability of\\nlearning LLMs for reasoning, with math problem-solving as an example. ReFT\\nfirst warmups the model with SFT, and then employs on-line reinforcement\\nlearning, specifically the PPO algorithm in this paper, to further fine-tune\\nthe model, where an abundance of reasoning paths are automatically sampled\\ngiven the question and the rewards are naturally derived from the ground-truth\\nanswers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that\\nReFT significantly outperforms SFT, and the performance can be potentially\\nfurther boosted by combining inference-time strategies such as majority voting\\nand re-ranking. Note that ReFT obtains the improvement by learning from the\\nsame training questions as SFT, without relying on extra or augmented training\\nquestions. This indicates a superior generalization ability for ReFT.',\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.08967'},\n",
              " {'title': 'UniVG: Towards UNIfied-modal Video Generation',\n",
              "  'abstract': \"Diffusion based video generation has received extensive attention and\\nachieved considerable success within both the academic and industrial\\ncommunities. However, current efforts are mainly concentrated on\\nsingle-objective or single-task video generation, such as generation driven by\\ntext, by image, or by a combination of text and image. This cannot fully meet\\nthe needs of real-world application scenarios, as users are likely to input\\nimages and text conditions in a flexible manner, either individually or in\\ncombination. To address this, we propose a Unified-modal Video Genearation\\nsystem that is capable of handling multiple video generation tasks across text\\nand image modalities. To this end, we revisit the various video generation\\ntasks within our system from the perspective of generative freedom, and\\nclassify them into high-freedom and low-freedom video generation categories.\\nFor high-freedom video generation, we employ Multi-condition Cross Attention to\\ngenerate videos that align with the semantics of the input images or text. For\\nlow-freedom video generation, we introduce Biased Gaussian Noise to replace the\\npure random Gaussian Noise, which helps to better preserve the content of the\\ninput conditions. Our method achieves the lowest Fr\\\\'echet Video Distance (FVD)\\non the public academic benchmark MSR-VTT, surpasses the current open-source\\nmethods in human evaluations, and is on par with the current close-source\\nmethod Gen2. For more samples, visit https://univg-baidu.github.io.\",\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.09084'},\n",
              " {'title': 'VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models',\n",
              "  'abstract': 'Text-to-video generation aims to produce a video based on a given prompt.\\nRecently, several commercial video models have been able to generate plausible\\nvideos with minimal noise, excellent details, and high aesthetic scores.\\nHowever, these models rely on large-scale, well-filtered, high-quality videos\\nthat are not accessible to the community. Many existing research works, which\\ntrain models using the low-quality WebVid-10M dataset, struggle to generate\\nhigh-quality videos because the models are optimized to fit WebVid-10M. In this\\nwork, we explore the training scheme of video models extended from Stable\\nDiffusion and investigate the feasibility of leveraging low-quality videos and\\nsynthesized high-quality images to obtain a high-quality video model. We first\\nanalyze the connection between the spatial and temporal modules of video models\\nand the distribution shift to low-quality videos. We observe that full training\\nof all modules results in a stronger coupling between spatial and temporal\\nmodules than only training temporal modules. Based on this stronger coupling,\\nwe shift the distribution to higher quality without motion degradation by\\nfinetuning spatial modules with high-quality images, resulting in a generic\\nhigh-quality video model. Evaluations are conducted to demonstrate the\\nsuperiority of the proposed method, particularly in picture quality, motion,\\nand concept composition.',\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.09047'},\n",
              " {'title': 'SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers',\n",
              "  'abstract': 'We present Scalable Interpolant Transformers (SiT), a family of generative\\nmodels built on the backbone of Diffusion Transformers (DiT). The interpolant\\nframework, which allows for connecting two distributions in a more flexible way\\nthan standard diffusion models, makes possible a modular study of various\\ndesign choices impacting generative models built on dynamical transport: using\\ndiscrete vs. continuous time learning, deciding the objective for the model to\\nlearn, choosing the interpolant connecting the distributions, and deploying a\\ndeterministic or stochastic sampler. By carefully introducing the above\\ningredients, SiT surpasses DiT uniformly across model sizes on the conditional\\nImageNet 256x256 benchmark using the exact same backbone, number of parameters,\\nand GFLOPs. By exploring various diffusion coefficients, which can be tuned\\nseparately from learning, SiT achieves an FID-50K score of 2.06.',\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.08740'},\n",
              " {'title': 'ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization',\n",
              "  'abstract': 'Neural Radiance Fields (NeRF) exhibit remarkable performance for Novel View\\nSynthesis (NVS) given a set of 2D images. However, NeRF training requires\\naccurate camera pose for each input view, typically obtained by\\nStructure-from-Motion (SfM) pipelines. Recent works have attempted to relax\\nthis constraint, but they still often rely on decent initial poses which they\\ncan refine. Here we aim at removing the requirement for pose initialization. We\\npresent Incremental CONfidence (ICON), an optimization procedure for training\\nNeRFs from 2D video frames. ICON only assumes smooth camera motion to estimate\\ninitial guess for poses. Further, ICON introduces ``confidence\": an adaptive\\nmeasure of model quality used to dynamically reweight gradients. ICON relies on\\nhigh-confidence poses to learn NeRF, and high-confidence 3D structure (as\\nencoded by NeRF) to learn poses. We show that ICON, without prior pose\\ninitialization, achieves superior performance in both CO3D and HO3D versus\\nmethods which use SfM pose.',\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.08937'},\n",
              " {'title': 'TextureDreamer: Image-guided Texture Synthesis through Geometry-aware Diffusion',\n",
              "  'abstract': 'We present TextureDreamer, a novel image-guided texture synthesis method to\\ntransfer relightable textures from a small number of input images (3 to 5) to\\ntarget 3D shapes across arbitrary categories. Texture creation is a pivotal\\nchallenge in vision and graphics. Industrial companies hire experienced artists\\nto manually craft textures for 3D assets. Classical methods require densely\\nsampled views and accurately aligned geometry, while learning-based methods are\\nconfined to category-specific shapes within the dataset. In contrast,\\nTextureDreamer can transfer highly detailed, intricate textures from real-world\\nenvironments to arbitrary objects with only a few casually captured images,\\npotentially significantly democratizing texture creation. Our core idea,\\npersonalized geometry-aware score distillation (PGSD), draws inspiration from\\nrecent advancements in diffuse models, including personalized modeling for\\ntexture information extraction, variational score distillation for detailed\\nappearance synthesis, and explicit geometry guidance with ControlNet. Our\\nintegration and several essential modifications substantially improve the\\ntexture quality. Experiments on real images spanning different categories show\\nthat TextureDreamer can successfully transfer highly realistic, semantic\\nmeaningful texture to arbitrary objects, surpassing the visual quality of\\nprevious state-of-the-art.',\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.09416'}]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TRdrSQwoR4b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o973Grm6R60f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Action: SummaryDailyPaper\n",
        "\n",
        "summary each daily paper, add five keywords by LLM"
      ],
      "metadata": {
        "id": "vKItU_B76eQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any\n",
        "PAPER_SUMMARY_PROMPT = \"\"\"\n",
        "    Transform the given data about a research paper into a neat Markdown format. Also, identify and include five relevant keywords that best represent the core themes of the paper.\n",
        "    Don't forget to include the title, abstract, and arXiv URL.\n",
        "    The provided data is:\n",
        "    ```\n",
        "    {data}\n",
        "    ```\n",
        "    Please create a markdown summary and suggest five keywords related to this paper, as well as the title, abstract, and arXiv URL.\n",
        "    \"\"\"\n",
        "class SummaryDailyPaper(Action):\n",
        "    def __init__(self, data: Any):\n",
        "        super().__init__(data)\n",
        "        self.data = data\n",
        "\n",
        "    async def run(\n",
        "        self\n",
        "    ):\n",
        "        return await self._aask(PAPER_SUMMARY_PROMPT.format(data=self.data))"
      ],
      "metadata": {
        "id": "Q_3G_iGbUSxN"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "await SummaryDailyPaper(resp[0]).run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "mIBsrwmEUN2o",
        "outputId": "9d9f9440-4d57-46ee-b459-817233ba7338"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference\n",
            "\n",
            "**Abstract:** The deployment and scaling of large language models (LLMs) have become critical as they permeate various applications, demanding high-throughput and low-latency serving systems. Existing frameworks struggle to balance these requirements, especially for workloads with long prompts. This paper introduces DeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and generation composition strategy, to deliver up to 2.3x higher effective throughput, 2x lower latency on average, and up to 3.7x lower (token-level) tail latency, compared to state-of-the-art systems like vLLM. We leverage a synergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an efficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced implementation supports a range of models and offers both non-persistent and persistent deployment options, catering to diverse user scenarios from interactive sessions to long-running applications. We present a detailed benchmarking methodology, analyze the performance through latency-throughput curves, and investigate scalability via load balancing. Our evaluations demonstrate substantial improvements in throughput and latency across various models and hardware configurations. We discuss our roadmap for future enhancements, including broader model support and new hardware backends. The DeepSpeed-FastGen code is readily available for community engagement and contribution.\n",
            "\n",
            "**Keywords:** language models, text generation, DeepSpeed-FastGen, high-throughput, low-latency\n",
            "\n",
            "[arXiv link](https://arxiv.org/abs/2401.08671)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"# DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference\\n\\n**Abstract:** The deployment and scaling of large language models (LLMs) have become critical as they permeate various applications, demanding high-throughput and low-latency serving systems. Existing frameworks struggle to balance these requirements, especially for workloads with long prompts. This paper introduces DeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and generation composition strategy, to deliver up to 2.3x higher effective throughput, 2x lower latency on average, and up to 3.7x lower (token-level) tail latency, compared to state-of-the-art systems like vLLM. We leverage a synergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an efficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced implementation supports a range of models and offers both non-persistent and persistent deployment options, catering to diverse user scenarios from interactive sessions to long-running applications. We present a detailed benchmarking methodology, analyze the performance through latency-throughput curves, and investigate scalability via load balancing. Our evaluations demonstrate substantial improvements in throughput and latency across various models and hardware configurations. We discuss our roadmap for future enhancements, including broader model support and new hardware backends. The DeepSpeed-FastGen code is readily available for community engagement and contribution.\\n\\n**Keywords:** language models, text generation, DeepSpeed-FastGen, high-throughput, low-latency\\n\\n[arXiv link](https://arxiv.org/abs/2401.08671)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4nkNi32EU9eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KVrQLINjZc0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Role: DailyPaperWatcher\n",
        "\n",
        "for analyze huggingfacce daily papers, and summary."
      ],
      "metadata": {
        "id": "6jeGEoi46ueO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d9EZfKlAdFNV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from typing import Dict, List\n",
        "from metagpt.utils.common import OutputParser\n",
        "from metagpt.roles import Role\n",
        "from metagpt.schema import Message\n",
        "from metagpt.logs import logger\n",
        "\n",
        "class DailyPaperWatcher(Role):\n",
        "    def __init__(\n",
        "        self,\n",
        "        name=\"Huggy\",\n",
        "        profile=\"DailyPaperWatcher\",\n",
        "        goal=\"Generate a summary of Huggingface daily papers.\",\n",
        "        constraints=\"Only analyze based on the provided Huggingface daily papers.\",\n",
        "    ):\n",
        "        super().__init__(name, profile, goal, constraints)\n",
        "        self._init_actions([CrawlHuggingfaceDailyPaper])\n",
        "        self._set_react_mode(react_mode=\"by_order\")\n",
        "\n",
        "\n",
        "    async def _act(self) -> Message:\n",
        "        logger.info(f\"{self._setting}: ready to {self._rc.todo}\")\n",
        "\n",
        "        todo = self._rc.todo\n",
        "\n",
        "        try:\n",
        "            msg = self.get_memories(k=1)[0]\n",
        "        except IndexError:\n",
        "            logger.error(\"No messages in memory\")\n",
        "            return Message(content=\"Error: No messages in memory\", role=self.profile)\n",
        "\n",
        "        try:\n",
        "            result = await todo.run(msg.content)\n",
        "            if isinstance(todo, CrawlHuggingfaceDailyPaper):\n",
        "                # 针对每篇论文创建并执行 SummaryDailyPaper 动作\n",
        "                logger.info(f\"Preparing to summarize {len(result)} papers\")\n",
        "                msg_content = ''\n",
        "                for paper in result:\n",
        "                    summary_action = SummaryDailyPaper(paper)\n",
        "                    summary_result = await summary_action.run(paper)\n",
        "                    summary_msg = Message(content=str(summary_result), role=self.profile, cause_by=type(summary_action))\n",
        "                    self._rc.memory.add(summary_msg)\n",
        "                    msg_content += str(summary_result)\n",
        "                    msg_content += '\\n'\n",
        "\n",
        "            else:\n",
        "                msg = Message(content=str(result), role=self.profile, cause_by=type(todo))\n",
        "                self._rc.memory.add(msg)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during action execution: {e}\")\n",
        "            return Message(content=f\"Error: {e}\", role=self.profile)\n",
        "\n",
        "        return Message(content=str(msg_content), role=self.profile, cause_by=type(todo))\n",
        "\n",
        "\n",
        "\n",
        "    # async def _handle_paper(self, paper_info) -> None:\n",
        "    #     actions = []\n",
        "    #     # Enhanced logging for debugging\n",
        "    #     logger.debug(f\"Handling paper with info: {paper_info}\")\n",
        "\n",
        "    #     for paper in paper_info:\n",
        "    #         actions.append(SummaryDailyPaper(paper))\n",
        "    #         logger.info(f\"Preparing to summarize paper: {paper['title']}\")\n",
        "\n",
        "    #     self._init_actions(actions)\n",
        "    #     self._rc.todo = None\n"
      ],
      "metadata": {
        "id": "GvqUrDnJAef_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List\n",
        "from metagpt.utils.common import OutputParser\n",
        "from metagpt.roles import Role\n",
        "from metagpt.schema import Message\n",
        "from metagpt.logs import logger\n",
        "from datetime import datetime\n",
        "\n",
        "class DailyPaperWatcher(Role):\n",
        "    def __init__(\n",
        "        self,\n",
        "        name=\"Huggy\",\n",
        "        profile=\"DailyPaperWatcher\",\n",
        "        goal=\"Generate a summary of Huggingface daily papers.\",\n",
        "        constraints=\"Only analyze based on the provided Huggingface daily papers.\",\n",
        "    ):\n",
        "        super().__init__(name, profile, goal, constraints)\n",
        "        self._init_actions([CrawlHuggingfaceDailyPaper])\n",
        "        self.tot_content = \"\"\n",
        "\n",
        "    async def _act(self) -> Message:\n",
        "        logger.info(f\"{self._setting}: ready to {self._rc.todo}\")\n",
        "\n",
        "        todo = self._rc.todo\n",
        "        if type(todo) is CrawlHuggingfaceDailyPaper:\n",
        "            msg = self._rc.memory.get(k=1)[0]\n",
        "\n",
        "            resp = await todo.run()\n",
        "            logger.info(resp)\n",
        "            return await self._handle_paper(resp)\n",
        "\n",
        "        resp = await todo.run()\n",
        "        logger.info(resp)\n",
        "\n",
        "        if self.tot_content != \"\":\n",
        "            self.tot_content += \"\\n\\n\\n\"\n",
        "        self.tot_content += resp\n",
        "        return Message(content=resp, role=self.profile)\n",
        "\n",
        "\n",
        "    async def _think(self) -> None:\n",
        "        \"\"\"Determine the next action to be taken by the role.\"\"\"\n",
        "        if self._rc.todo is None:\n",
        "            self._set_state(0)\n",
        "            return\n",
        "\n",
        "        if self._rc.state + 1 < len(self._states):\n",
        "            self._set_state(self._rc.state + 1)\n",
        "        else:\n",
        "            self._rc.todo = None\n",
        "\n",
        "    async def _react(self) -> Message:\n",
        "        \"\"\"Execute the assistant's think and actions.\"\"\"\n",
        "        while True:\n",
        "            await self._think()\n",
        "            if self._rc.todo is None:\n",
        "                break\n",
        "            msg = await self._act()\n",
        "\n",
        "        # return msg\n",
        "        return Message(content=self.tot_content, role=self.profile)\n",
        "\n",
        "    async def _handle_paper(self, paper_info) -> None:\n",
        "        actions = []\n",
        "        # Enhanced logging for debuggingself\n",
        "        logger.debug(f\"Handling paper with info: {paper_info}\")\n",
        "        self.tot_content += f\"# Huggingface Daily Paper: {datetime.now().strftime('%Y-%m-%d')}\"\n",
        "\n",
        "        for paper in paper_info:\n",
        "            # print(paper)\n",
        "            actions.append(SummaryDailyPaper(paper))\n",
        "            # logger.info(f\"Preparing to summarize paper: {paper['title']}\")\n",
        "\n",
        "        self._init_actions(actions)\n",
        "        self._rc.todo = None\n",
        "        return Message(content=\"init\", role=self.profile)\n",
        "\n"
      ],
      "metadata": {
        "id": "dhxC_PvCFor4"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def main():\n",
        "\n",
        "    role = DailyPaperWatcher()\n",
        "    result = await role.run(\"https://huggingface.co/papers\")\n",
        "    logger.info(result)\n",
        "    return result\n",
        "\n",
        "result = await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNHFxtLkdBKB",
        "outputId": "5f4d8b66-ab7f-4ca0-cc03-7f5ffc44f09f"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-18 06:27:55.727 | INFO     | __main__:_act:21 - Huggy(DailyPaperWatcher): ready to CrawlHuggingfaceDailyPaper\n",
            "2024-01-18 06:27:59.120 | INFO     | __main__:_act:28 - [{'title': 'Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model', 'abstract': 'Recently the state space models (SSMs) with efficient hardware-aware designs,\\ni.e., Mamba, have shown great potential for long sequence modeling. Building\\nefficient and generic vision backbones purely upon SSMs is an appealing\\ndirection. However, representing visual data is challenging for SSMs due to the\\nposition-sensitivity of visual data and the requirement of global context for\\nvisual understanding. In this paper, we show that the reliance of visual\\nrepresentation learning on self-attention is not necessary and propose a new\\ngeneric vision backbone with bidirectional Mamba blocks (Vim), which marks the\\nimage sequences with position embeddings and compresses the visual\\nrepresentation with bidirectional state space models. On ImageNet\\nclassification, COCO object detection, and ADE20k semantic segmentation tasks,\\nVim achieves higher performance compared to well-established vision\\ntransformers like DeiT, while also demonstrating significantly improved\\ncomputation & memory efficiency. For example, Vim is 2.8times faster than\\nDeiT and saves 86.8% GPU memory when performing batch inference to extract\\nfeatures on images with a resolution of 1248times1248. The results\\ndemonstrate that Vim is capable of overcoming the computation & memory\\nconstraints on performing Transformer-style understanding for high-resolution\\nimages and it has great potential to become the next-generation backbone for\\nvision foundation models. Code is available at https://github.com/hustvl/Vim.', 'arxiv_url': 'https://arxiv.org/abs/2401.09417'}, {'title': 'DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference', 'abstract': \"The deployment and scaling of large language models (LLMs) have become\\ncritical as they permeate various applications, demanding high-throughput and\\nlow-latency serving systems. Existing frameworks struggle to balance these\\nrequirements, especially for workloads with long prompts. This paper introduces\\nDeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and\\ngeneration composition strategy, to deliver up to 2.3x higher effective\\nthroughput, 2x lower latency on average, and up to 3.7x lower (token-level)\\ntail latency, compared to state-of-the-art systems like vLLM. We leverage a\\nsynergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an\\nefficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced\\nimplementation supports a range of models and offers both non-persistent and\\npersistent deployment options, catering to diverse user scenarios from\\ninteractive sessions to long-running applications. We present a detailed\\nbenchmarking methodology, analyze the performance through latency-throughput\\ncurves, and investigate scalability via load balancing. Our evaluations\\ndemonstrate substantial improvements in throughput and latency across various\\nmodels and hardware configurations. We discuss our roadmap for future\\nenhancements, including broader model support and new hardware backends. The\\nDeepSpeed-FastGen code is readily available for community engagement and\\ncontribution.\", 'arxiv_url': 'https://arxiv.org/abs/2401.08671'}, {'title': 'UniVG: Towards UNIfied-modal Video Generation', 'abstract': \"Diffusion based video generation has received extensive attention and\\nachieved considerable success within both the academic and industrial\\ncommunities. However, current efforts are mainly concentrated on\\nsingle-objective or single-task video generation, such as generation driven by\\ntext, by image, or by a combination of text and image. This cannot fully meet\\nthe needs of real-world application scenarios, as users are likely to input\\nimages and text conditions in a flexible manner, either individually or in\\ncombination. To address this, we propose a Unified-modal Video Genearation\\nsystem that is capable of handling multiple video generation tasks across text\\nand image modalities. To this end, we revisit the various video generation\\ntasks within our system from the perspective of generative freedom, and\\nclassify them into high-freedom and low-freedom video generation categories.\\nFor high-freedom video generation, we employ Multi-condition Cross Attention to\\ngenerate videos that align with the semantics of the input images or text. For\\nlow-freedom video generation, we introduce Biased Gaussian Noise to replace the\\npure random Gaussian Noise, which helps to better preserve the content of the\\ninput conditions. Our method achieves the lowest Fr\\\\'echet Video Distance (FVD)\\non the public academic benchmark MSR-VTT, surpasses the current open-source\\nmethods in human evaluations, and is on par with the current close-source\\nmethod Gen2. For more samples, visit https://univg-baidu.github.io.\", 'arxiv_url': 'https://arxiv.org/abs/2401.09084'}, {'title': 'Asynchronous Local-SGD Training for Language Modeling', 'abstract': \"Local stochastic gradient descent (Local-SGD), also referred to as federated\\naveraging, is an approach to distributed optimization where each device\\nperforms more than one SGD update per communication. This work presents an\\nempirical study of {\\\\it asynchronous} Local-SGD for training language models;\\nthat is, each worker updates the global parameters as soon as it has finished\\nits SGD steps. We conduct a comprehensive investigation by examining how worker\\nhardware heterogeneity, model size, number of workers, and optimizer could\\nimpact the learning performance. We find that with naive implementations,\\nasynchronous Local-SGD takes more iterations to converge than its synchronous\\ncounterpart despite updating the (global) model parameters more frequently. We\\nidentify momentum acceleration on the global parameters when worker gradients\\nare stale as a key challenge. We propose a novel method that utilizes a delayed\\nNesterov momentum update and adjusts the workers' local training steps based on\\ntheir computation speed. This approach, evaluated with models up to 150M\\nparameters on the C4 dataset, matches the performance of synchronous Local-SGD\\nin terms of perplexity per update step, and significantly surpasses it in terms\\nof wall clock time.\", 'arxiv_url': 'https://arxiv.org/abs/2401.09135'}, {'title': 'ReFT: Reasoning with Reinforced Fine-Tuning', 'abstract': 'One way to enhance the reasoning capability of Large Language Models (LLMs)\\nis to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT)\\nannotations. This approach does not show sufficiently strong generalization\\nability, however, because the training only relies on the given CoT data. In\\nmath problem-solving, for example, there is usually only one annotated\\nreasoning path for each question in the training data. Intuitively, it would be\\nbetter for the algorithm to learn from multiple annotated reasoning paths given\\na question. To address this issue, we propose a simple yet effective approach\\ncalled Reinforced Fine-Tuning (ReFT) to enhance the generalizability of\\nlearning LLMs for reasoning, with math problem-solving as an example. ReFT\\nfirst warmups the model with SFT, and then employs on-line reinforcement\\nlearning, specifically the PPO algorithm in this paper, to further fine-tune\\nthe model, where an abundance of reasoning paths are automatically sampled\\ngiven the question and the rewards are naturally derived from the ground-truth\\nanswers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that\\nReFT significantly outperforms SFT, and the performance can be potentially\\nfurther boosted by combining inference-time strategies such as majority voting\\nand re-ranking. Note that ReFT obtains the improvement by learning from the\\nsame training questions as SFT, without relying on extra or augmented training\\nquestions. This indicates a superior generalization ability for ReFT.', 'arxiv_url': 'https://arxiv.org/abs/2401.08967'}, {'title': 'VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models', 'abstract': 'Text-to-video generation aims to produce a video based on a given prompt.\\nRecently, several commercial video models have been able to generate plausible\\nvideos with minimal noise, excellent details, and high aesthetic scores.\\nHowever, these models rely on large-scale, well-filtered, high-quality videos\\nthat are not accessible to the community. Many existing research works, which\\ntrain models using the low-quality WebVid-10M dataset, struggle to generate\\nhigh-quality videos because the models are optimized to fit WebVid-10M. In this\\nwork, we explore the training scheme of video models extended from Stable\\nDiffusion and investigate the feasibility of leveraging low-quality videos and\\nsynthesized high-quality images to obtain a high-quality video model. We first\\nanalyze the connection between the spatial and temporal modules of video models\\nand the distribution shift to low-quality videos. We observe that full training\\nof all modules results in a stronger coupling between spatial and temporal\\nmodules than only training temporal modules. Based on this stronger coupling,\\nwe shift the distribution to higher quality without motion degradation by\\nfinetuning spatial modules with high-quality images, resulting in a generic\\nhigh-quality video model. Evaluations are conducted to demonstrate the\\nsuperiority of the proposed method, particularly in picture quality, motion,\\nand concept composition.', 'arxiv_url': 'https://arxiv.org/abs/2401.09047'}, {'title': 'SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers', 'abstract': 'We present Scalable Interpolant Transformers (SiT), a family of generative\\nmodels built on the backbone of Diffusion Transformers (DiT). The interpolant\\nframework, which allows for connecting two distributions in a more flexible way\\nthan standard diffusion models, makes possible a modular study of various\\ndesign choices impacting generative models built on dynamical transport: using\\ndiscrete vs. continuous time learning, deciding the objective for the model to\\nlearn, choosing the interpolant connecting the distributions, and deploying a\\ndeterministic or stochastic sampler. By carefully introducing the above\\ningredients, SiT surpasses DiT uniformly across model sizes on the conditional\\nImageNet 256x256 benchmark using the exact same backbone, number of parameters,\\nand GFLOPs. By exploring various diffusion coefficients, which can be tuned\\nseparately from learning, SiT achieves an FID-50K score of 2.06.', 'arxiv_url': 'https://arxiv.org/abs/2401.08740'}, {'title': 'GARField: Group Anything with Radiance Fields', 'abstract': \"Grouping is inherently ambiguous due to the multiple levels of granularity in\\nwhich one can decompose a scene -- should the wheels of an excavator be\\nconsidered separate or part of the whole? We present Group Anything with\\nRadiance Fields (GARField), an approach for decomposing 3D scenes into a\\nhierarchy of semantically meaningful groups from posed image inputs. To do this\\nwe embrace group ambiguity through physical scale: by optimizing a\\nscale-conditioned 3D affinity feature field, a point in the world can belong to\\ndifferent groups of different sizes. We optimize this field from a set of 2D\\nmasks provided by Segment Anything (SAM) in a way that respects coarse-to-fine\\nhierarchy, using scale to consistently fuse conflicting masks from different\\nviewpoints. From this field we can derive a hierarchy of possible groupings via\\nautomatic tree construction or user interaction. We evaluate GARField on a\\nvariety of in-the-wild scenes and find it effectively extracts groups at many\\nlevels: clusters of objects, objects, and various subparts. GARField inherently\\nrepresents multi-view consistent groupings and produces higher fidelity groups\\nthan the input SAM masks. GARField's hierarchical grouping could have exciting\\ndownstream applications such as 3D asset extraction or dynamic scene\\nunderstanding. See the project website at https://www.garfield.studio/\", 'arxiv_url': 'https://arxiv.org/abs/2401.09419'}, {'title': 'SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding', 'abstract': '3D vision-language grounding, which focuses on aligning language with the 3D\\nphysical environment, stands as a cornerstone in the development of embodied\\nagents. In comparison to recent advancements in the 2D domain, grounding\\nlanguage in 3D scenes faces several significant challenges: (i) the inherent\\ncomplexity of 3D scenes due to the diverse object configurations, their rich\\nattributes, and intricate relationships; (ii) the scarcity of paired 3D\\nvision-language data to support grounded learning; and (iii) the absence of a\\nunified learning framework to distill knowledge from grounded 3D data. In this\\nwork, we aim to address these three major challenges in 3D vision-language by\\nexamining the potential of systematically upscaling 3D vision-language learning\\nin indoor environments. We introduce the first million-scale 3D vision-language\\ndataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising\\n2.5M vision-language pairs derived from both human annotations and our scalable\\nscene-graph-based generation approach. We demonstrate that this scaling allows\\nfor a unified pre-training framework, Grounded Pre-training for Scenes (GPS),\\nfor 3D vision-language learning. Through extensive experiments, we showcase the\\neffectiveness of GPS by achieving state-of-the-art performance on all existing\\n3D visual grounding benchmarks. The vast potential of SceneVerse and GPS is\\nunveiled through zero-shot transfer experiments in the challenging 3D\\nvision-language tasks. Project website: https://scene-verse.github.io .', 'arxiv_url': 'https://arxiv.org/abs/2401.09340'}, {'title': 'Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis', 'abstract': 'Addressing the limitations of text as a source of accurate layout\\nrepresentation in text-conditional diffusion models, many works incorporate\\nadditional signals to condition certain attributes within a generated image.\\nAlthough successful, previous works do not account for the specific\\nlocalization of said attributes extended into the three dimensional plane. In\\nthis context, we present a conditional diffusion model that integrates control\\nover three-dimensional object placement with disentangled representations of\\nglobal stylistic semantics from multiple exemplar images. Specifically, we\\nfirst introduce depth disentanglement training to leverage the\\nrelative depth of objects as an estimator, allowing the model to identify the\\nabsolute positions of unseen objects through the use of synthetic image\\ntriplets. We also introduce soft guidance, a method for imposing\\nglobal semantics onto targeted regions without the use of any additional\\nlocalization cues. Our integrated framework, Compose and Conquer\\n(CnC), unifies these techniques to localize multiple conditions in a\\ndisentangled manner. We demonstrate that our approach allows perception of\\nobjects at varying depths while offering a versatile framework for composing\\nlocalized objects with different global semantics. Code:\\nhttps://github.com/tomtom1103/compose-and-conquer/', 'arxiv_url': 'https://arxiv.org/abs/2401.09048'}, {'title': 'ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization', 'abstract': 'Neural Radiance Fields (NeRF) exhibit remarkable performance for Novel View\\nSynthesis (NVS) given a set of 2D images. However, NeRF training requires\\naccurate camera pose for each input view, typically obtained by\\nStructure-from-Motion (SfM) pipelines. Recent works have attempted to relax\\nthis constraint, but they still often rely on decent initial poses which they\\ncan refine. Here we aim at removing the requirement for pose initialization. We\\npresent Incremental CONfidence (ICON), an optimization procedure for training\\nNeRFs from 2D video frames. ICON only assumes smooth camera motion to estimate\\ninitial guess for poses. Further, ICON introduces ``confidence\": an adaptive\\nmeasure of model quality used to dynamically reweight gradients. ICON relies on\\nhigh-confidence poses to learn NeRF, and high-confidence 3D structure (as\\nencoded by NeRF) to learn poses. We show that ICON, without prior pose\\ninitialization, achieves superior performance in both CO3D and HO3D versus\\nmethods which use SfM pose.', 'arxiv_url': 'https://arxiv.org/abs/2401.08937'}, {'title': 'TextureDreamer: Image-guided Texture Synthesis through Geometry-aware Diffusion', 'abstract': 'We present TextureDreamer, a novel image-guided texture synthesis method to\\ntransfer relightable textures from a small number of input images (3 to 5) to\\ntarget 3D shapes across arbitrary categories. Texture creation is a pivotal\\nchallenge in vision and graphics. Industrial companies hire experienced artists\\nto manually craft textures for 3D assets. Classical methods require densely\\nsampled views and accurately aligned geometry, while learning-based methods are\\nconfined to category-specific shapes within the dataset. In contrast,\\nTextureDreamer can transfer highly detailed, intricate textures from real-world\\nenvironments to arbitrary objects with only a few casually captured images,\\npotentially significantly democratizing texture creation. Our core idea,\\npersonalized geometry-aware score distillation (PGSD), draws inspiration from\\nrecent advancements in diffuse models, including personalized modeling for\\ntexture information extraction, variational score distillation for detailed\\nappearance synthesis, and explicit geometry guidance with ControlNet. Our\\nintegration and several essential modifications substantially improve the\\ntexture quality. Experiments on real images spanning different categories show\\nthat TextureDreamer can successfully transfer highly realistic, semantic\\nmeaningful texture to arbitrary objects, surpassing the visual quality of\\nprevious state-of-the-art.', 'arxiv_url': 'https://arxiv.org/abs/2401.09416'}]\n",
            "2024-01-18 06:27:59.124 | INFO     | __main__:_act:21 - Huggy(DailyPaperWatcher): ready to SummaryDailyPaper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model\n",
            "\n",
            "**Abstract:** Recently, state space models (SSMs) with efficient hardware-aware designs, specifically Mamba, have shown great potential for long sequence modeling. However, representing visual data is challenging for SSMs due to the position-sensitivity and the requirement of global context. In this paper, the authors propose a new generic vision backbone called Vim, which utilizes bidirectional Mamba blocks to compress visual representation. Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating improved computation and memory efficiency. For example, Vim is 2.8 times faster than DeiT and saves 86.8% GPU memory when performing batch inference on high-resolution images. The results show that Vim overcomes computation and memory constraints, making it a promising backbone for vision foundation models.\n",
            "\n",
            "**Keywords:** Vision Mamba, State Space Model, Visual Representation Learning, Computation Efficiency, Memory"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-18 06:28:02.900 | INFO     | __main__:_act:32 - # Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model\n",
            "\n",
            "**Abstract:** Recently, state space models (SSMs) with efficient hardware-aware designs, specifically Mamba, have shown great potential for long sequence modeling. However, representing visual data is challenging for SSMs due to the position-sensitivity and the requirement of global context. In this paper, the authors propose a new generic vision backbone called Vim, which utilizes bidirectional Mamba blocks to compress visual representation. Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating improved computation and memory efficiency. For example, Vim is 2.8 times faster than DeiT and saves 86.8% GPU memory when performing batch inference on high-resolution images. The results show that Vim overcomes computation and memory constraints, making it a promising backbone for vision foundation models.\n",
            "\n",
            "**Keywords:** Vision Mamba, State Space Model, Visual Representation Learning, Computation Efficiency, Memory Efficiency\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.09417)\n",
            "2024-01-18 06:28:02.902 | INFO     | __main__:_act:21 - Huggy(DailyPaperWatcher): ready to SummaryDailyPaper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Efficiency\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.09417)\n",
            "# DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference\n",
            "\n",
            "**Abstract:** The deployment and scaling of large language models (LLMs) have become critical as they permeate various applications, demanding high-throughput and low-latency serving systems. Existing frameworks struggle to balance these requirements, especially for workloads with long prompts. This paper introduces DeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and generation composition strategy, to deliver up to 2.3x higher effective throughput, 2x lower latency on average, and up to 3.7x lower (token-level) tail latency, compared to state-of-the-art systems like vLLM. We leverage a synergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an efficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced implementation supports a range of models and offers both non-persistent and persistent deployment options, catering to diverse user scenarios from interactive sessions to long-running applications. We present a detailed benchmarking methodology, analyze the performance through latency-throughput curves, and investigate scalability via load balancing. Our evaluations demonstrate substantial improvements in throughput and latency across various models and hardware configurations. We discuss our roadmap for future enhancements, including broader model support and new hardware backends. The DeepSpeed-FastGen code is readily available for community engagement and contribution.\n",
            "\n",
            "**Keywords:** Language models, high-throughput text generation, DeepSpeed-FastGen, MII, DeepSpeed-Inference\n",
            "\n",
            "[arXiv URL](https://arxiv.org/abs/2401.08671)"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-18 06:28:06.882 | INFO     | __main__:_act:32 - # DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference\n",
            "\n",
            "**Abstract:** The deployment and scaling of large language models (LLMs) have become critical as they permeate various applications, demanding high-throughput and low-latency serving systems. Existing frameworks struggle to balance these requirements, especially for workloads with long prompts. This paper introduces DeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and generation composition strategy, to deliver up to 2.3x higher effective throughput, 2x lower latency on average, and up to 3.7x lower (token-level) tail latency, compared to state-of-the-art systems like vLLM. We leverage a synergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an efficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced implementation supports a range of models and offers both non-persistent and persistent deployment options, catering to diverse user scenarios from interactive sessions to long-running applications. We present a detailed benchmarking methodology, analyze the performance through latency-throughput curves, and investigate scalability via load balancing. Our evaluations demonstrate substantial improvements in throughput and latency across various models and hardware configurations. We discuss our roadmap for future enhancements, including broader model support and new hardware backends. The DeepSpeed-FastGen code is readily available for community engagement and contribution.\n",
            "\n",
            "**Keywords:** Language models, high-throughput text generation, DeepSpeed-FastGen, MII, DeepSpeed-Inference\n",
            "\n",
            "[arXiv URL](https://arxiv.org/abs/2401.08671)\n",
            "2024-01-18 06:28:06.883 | INFO     | __main__:_act:21 - Huggy(DailyPaperWatcher): ready to SummaryDailyPaper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# UniVG: Towards UNIfied-modal Video Generation\n",
            "\n",
            "**Abstract:** Diffusion based video generation has received extensive attention and achieved considerable success within both the academic and industrial communities. However, current efforts are mainly concentrated on single-objective or single-task video generation, such as generation driven by text, by image, or by a combination of text and image. This cannot fully meet the needs of real-world application scenarios, as users are likely to input images and text conditions in a flexible manner, either individually or in combination. To address this, we propose a Unified-modal Video Generation system that is capable of handling multiple video generation tasks across text and image modalities. To this end, we revisit the various video generation tasks within our system from the perspective of generative freedom, and classify them into high-freedom and low-freedom video generation categories. For high-freedom video generation, we employ Multi-condition Cross Attention to generate videos that align with the semantics of the input images or text. For low-freedom video generation, we introduce Biased Gaussian Noise to replace the pure random Gaussian Noise, which helps to better preserve the content of the input conditions. Our method achieves the lowest Fréchet Video Distance (FVD) on the public academic benchmark MSR-VTT, surpasses the current open-source methods in human evaluations, and is on par with the current close-source method Gen2. For more samples, visit [https://univg-baidu.github.io](https://univg-baidu.github.io).\n",
            "\n",
            "**Keywords:** video generation, unified-modal, generative freedom, multi-condition"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-18 06:28:12.248 | INFO     | __main__:_act:32 - # UniVG: Towards UNIfied-modal Video Generation\n",
            "\n",
            "**Abstract:** Diffusion based video generation has received extensive attention and achieved considerable success within both the academic and industrial communities. However, current efforts are mainly concentrated on single-objective or single-task video generation, such as generation driven by text, by image, or by a combination of text and image. This cannot fully meet the needs of real-world application scenarios, as users are likely to input images and text conditions in a flexible manner, either individually or in combination. To address this, we propose a Unified-modal Video Generation system that is capable of handling multiple video generation tasks across text and image modalities. To this end, we revisit the various video generation tasks within our system from the perspective of generative freedom, and classify them into high-freedom and low-freedom video generation categories. For high-freedom video generation, we employ Multi-condition Cross Attention to generate videos that align with the semantics of the input images or text. For low-freedom video generation, we introduce Biased Gaussian Noise to replace the pure random Gaussian Noise, which helps to better preserve the content of the input conditions. Our method achieves the lowest Fréchet Video Distance (FVD) on the public academic benchmark MSR-VTT, surpasses the current open-source methods in human evaluations, and is on par with the current close-source method Gen2. For more samples, visit [https://univg-baidu.github.io](https://univg-baidu.github.io).\n",
            "\n",
            "**Keywords:** video generation, unified-modal, generative freedom, multi-condition cross attention, biased Gaussian noise\n",
            "\n",
            "**arXiv URL:** [https://arxiv.org/abs/2401.09084](https://arxiv.org/abs/2401.09084)\n",
            "2024-01-18 06:28:12.250 | INFO     | __main__:_act:21 - Huggy(DailyPaperWatcher): ready to SummaryDailyPaper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " cross attention, biased Gaussian noise\n",
            "\n",
            "**arXiv URL:** [https://arxiv.org/abs/2401.09084](https://arxiv.org/abs/2401.09084)\n",
            "# Asynchronous Local-SGD Training for Language Modeling\n",
            "\n",
            "**Abstract:** Local stochastic gradient descent (Local-SGD), also referred to as federated averaging, is an approach to distributed optimization where each device performs more than one SGD update per communication. This work presents an empirical study of *asynchronous* Local-SGD for training language models; that is, each worker updates the global parameters as soon as it has finished its SGD steps. We conduct a comprehensive investigation by examining how worker hardware heterogeneity, model size, number of workers, and optimizer could impact the learning performance. We find that with naive implementations, asynchronous Local-SGD takes more iterations to converge than its synchronous counterpart despite updating the (global) model parameters more frequently. We identify momentum acceleration on the global parameters when worker gradients are stale as a key challenge. We propose a novel method that utilizes a delayed Nesterov momentum update and adjusts the workers' local training steps based on their computation speed. This approach, evaluated with models up to 150M parameters on the C4 dataset, matches the performance of synchronous Local-SGD in terms of perplexity per update step, and significantly surpasses it in terms of wall clock time.\n",
            "\n",
            "**Keywords:** distributed optimization, asynchronous"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-18 06:28:15.643 | INFO     | __main__:_act:32 - # Asynchronous Local-SGD Training for Language Modeling\n",
            "\n",
            "**Abstract:** Local stochastic gradient descent (Local-SGD), also referred to as federated averaging, is an approach to distributed optimization where each device performs more than one SGD update per communication. This work presents an empirical study of *asynchronous* Local-SGD for training language models; that is, each worker updates the global parameters as soon as it has finished its SGD steps. We conduct a comprehensive investigation by examining how worker hardware heterogeneity, model size, number of workers, and optimizer could impact the learning performance. We find that with naive implementations, asynchronous Local-SGD takes more iterations to converge than its synchronous counterpart despite updating the (global) model parameters more frequently. We identify momentum acceleration on the global parameters when worker gradients are stale as a key challenge. We propose a novel method that utilizes a delayed Nesterov momentum update and adjusts the workers' local training steps based on their computation speed. This approach, evaluated with models up to 150M parameters on the C4 dataset, matches the performance of synchronous Local-SGD in terms of perplexity per update step, and significantly surpasses it in terms of wall clock time.\n",
            "\n",
            "**Keywords:** distributed optimization, asynchronous Local-SGD, language modeling, worker hardware heterogeneity, delayed Nesterov momentum update\n",
            "\n",
            "**arXiv URL:** [https://arxiv.org/abs/2401.09135](https://arxiv.org/abs/2401.09135)\n",
            "2024-01-18 06:28:15.645 | INFO     | __main__:_act:21 - Huggy(DailyPaperWatcher): ready to SummaryDailyPaper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Local-SGD, language modeling, worker hardware heterogeneity, delayed Nesterov momentum update\n",
            "\n",
            "**arXiv URL:** [https://arxiv.org/abs/2401.09135](https://arxiv.org/abs/2401.09135)\n",
            "# ReFT: Reasoning with Reinforced Fine-Tuning\n",
            "\n",
            "**Abstract:** One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question and the rewards are naturally derived from the ground-truth answers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that ReFT significantly outperforms SFT, and the performance can be potentially further boosted by combining inference-time strategies such as majority voting and re-ranking. Note that ReFT obtains the improvement by learning from the same training questions as SFT, without relying on extra or augmented training questions. This indicates a superior generalization ability for ReFT.\n",
            "\n",
            "**Keywords:** Large Language Models, Reinforced Fine-Tuning, Reasoning, Chain-of-Thought"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-18 06:28:20.540 | INFO     | __main__:_act:32 - # ReFT: Reasoning with Reinforced Fine-Tuning\n",
            "\n",
            "**Abstract:** One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question and the rewards are naturally derived from the ground-truth answers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that ReFT significantly outperforms SFT, and the performance can be potentially further boosted by combining inference-time strategies such as majority voting and re-ranking. Note that ReFT obtains the improvement by learning from the same training questions as SFT, without relying on extra or augmented training questions. This indicates a superior generalization ability for ReFT.\n",
            "\n",
            "**Keywords:** Large Language Models, Reinforced Fine-Tuning, Reasoning, Chain-of-Thought, Math problem-solving\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08967)\n",
            "2024-01-18 06:28:20.541 | INFO     | __main__:_act:21 - Huggy(DailyPaperWatcher): ready to SummaryDailyPaper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ", Math problem-solving\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08967)\n",
            "# VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models\n",
            "\n",
            "**Abstract:**\n",
            "Text-to-video generation aims to produce a video based on a given prompt. Recently, several commercial video models have been able to generate plausible videos with minimal noise, excellent details, and high aesthetic scores. However, these models rely on large-scale, well-filtered, high-quality videos that are not accessible to the community. Many existing research works, which train models using the low-quality WebVid-10M dataset, struggle to generate high-quality videos because the models are optimized to fit WebVid-10M. In this work, we explore the training scheme of video models extended from Stable Diffusion and investigate the feasibility of leveraging low-quality videos and synthesized high-quality images to obtain a high-quality video model. We first analyze the connection between the spatial and temporal modules of video models and the distribution shift to low-quality videos. We observe that full training of all modules results in a stronger coupling between spatial and temporal modules than only training temporal modules. Based on this stronger coupling, we shift the distribution to higher quality without motion degradation by finetuning spatial modules with high-quality images, resulting in a generic high-quality video model. Evaluations are conducted to demonstrate the superiority of the proposed method, particularly in picture quality, motion, and concept composition.\n",
            "\n",
            "**Keywords:** Text-to-video"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-18 06:28:24.428 | INFO     | __main__:_act:32 - # VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models\n",
            "\n",
            "**Abstract:**\n",
            "Text-to-video generation aims to produce a video based on a given prompt. Recently, several commercial video models have been able to generate plausible videos with minimal noise, excellent details, and high aesthetic scores. However, these models rely on large-scale, well-filtered, high-quality videos that are not accessible to the community. Many existing research works, which train models using the low-quality WebVid-10M dataset, struggle to generate high-quality videos because the models are optimized to fit WebVid-10M. In this work, we explore the training scheme of video models extended from Stable Diffusion and investigate the feasibility of leveraging low-quality videos and synthesized high-quality images to obtain a high-quality video model. We first analyze the connection between the spatial and temporal modules of video models and the distribution shift to low-quality videos. We observe that full training of all modules results in a stronger coupling between spatial and temporal modules than only training temporal modules. Based on this stronger coupling, we shift the distribution to higher quality without motion degradation by finetuning spatial modules with high-quality images, resulting in a generic high-quality video model. Evaluations are conducted to demonstrate the superiority of the proposed method, particularly in picture quality, motion, and concept composition.\n",
            "\n",
            "**Keywords:** Text-to-video generation, video models, high-quality videos, low-quality videos, synthesized high-quality images\n",
            "\n",
            "[arXiv link](https://arxiv.org/abs/2401.09047)\n",
            "2024-01-18 06:28:24.429 | INFO     | __main__:_act:21 - Huggy(DailyPaperWatcher): ready to SummaryDailyPaper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " generation, video models, high-quality videos, low-quality videos, synthesized high-quality images\n",
            "\n",
            "[arXiv link](https://arxiv.org/abs/2401.09047)\n",
            "# SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers\n",
            "\n",
            "**Abstract:** We present Scalable Interpolant Transformers (SiT), a family of generative models built on the backbone of Diffusion Transformers (DiT). The interpolant framework, which allows for connecting two distributions in a more flexible way than standard diffusion models, makes possible a modular study of various design choices impacting generative models built on dynamical transport: using discrete vs. continuous time learning, deciding the objective for the model to learn, choosing the interpolant connecting the distributions, and deploying a deterministic or stochastic sampler. By carefully introducing the above ingredients, SiT surpasses DiT uniformly across model sizes on the conditional ImageNet 256x256 benchmark using the exact same backbone, number of parameters, and GFLOPs. By exploring various diffusion coefficients, which can be tuned separately from learning, SiT achieves an FID-50K score of 2.06.\n",
            "\n",
            "**Keywords:** generative models, Diffusion Transformers"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-18 06:28:28.116 | INFO     | __main__:_act:32 - # SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers\n",
            "\n",
            "**Abstract:** We present Scalable Interpolant Transformers (SiT), a family of generative models built on the backbone of Diffusion Transformers (DiT). The interpolant framework, which allows for connecting two distributions in a more flexible way than standard diffusion models, makes possible a modular study of various design choices impacting generative models built on dynamical transport: using discrete vs. continuous time learning, deciding the objective for the model to learn, choosing the interpolant connecting the distributions, and deploying a deterministic or stochastic sampler. By carefully introducing the above ingredients, SiT surpasses DiT uniformly across model sizes on the conditional ImageNet 256x256 benchmark using the exact same backbone, number of parameters, and GFLOPs. By exploring various diffusion coefficients, which can be tuned separately from learning, SiT achieves an FID-50K score of 2.06.\n",
            "\n",
            "**Keywords:** generative models, Diffusion Transformers, Scalable Interpolant Transformers, modular study, dynamical transport\n",
            "\n",
            "**arXiv URL:** [https://arxiv.org/abs/2401.08740](https://arxiv.org/abs/2401.08740)\n",
            "2024-01-18 06:28:28.118 | INFO     | __main__:_act:21 - Huggy(DailyPaperWatcher): ready to SummaryDailyPaper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ", Scalable Interpolant Transformers, modular study, dynamical transport\n",
            "\n",
            "**arXiv URL:** [https://arxiv.org/abs/2401.08740](https://arxiv.org/abs/2401.08740)\n",
            "# GARField: Group Anything with Radiance Fields\n",
            "\n",
            "**Abstract:** Grouping is inherently ambiguous due to the multiple levels of granularity in which one can decompose a scene -- should the wheels of an excavator be considered separate or part of the whole? We present Group Anything with Radiance Fields (GARField), an approach for decomposing 3D scenes into a hierarchy of semantically meaningful groups from posed image inputs. To do this we embrace group ambiguity through physical scale: by optimizing a scale-conditioned 3D affinity feature field, a point in the world can belong to different groups of different sizes. We optimize this field from a set of 2D masks provided by Segment Anything (SAM) in a way that respects coarse-to-fine hierarchy, using scale to consistently fuse conflicting masks from different viewpoints. From this field we can derive a hierarchy of possible groupings via automatic tree construction or user interaction. We evaluate GARField on a variety of in-the-wild scenes and find it effectively extracts groups at many levels: clusters of objects, objects, and various subparts. GARField inherently represents multi-view consistent groupings and produces higher fidelity groups than the input SAM masks. GARField's hierarchical grouping could have exciting downstream applications such as 3D asset extraction or dynamic scene understanding"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-18 06:28:33.693 | INFO     | __main__:_act:32 - # GARField: Group Anything with Radiance Fields\n",
            "\n",
            "**Abstract:** Grouping is inherently ambiguous due to the multiple levels of granularity in which one can decompose a scene -- should the wheels of an excavator be considered separate or part of the whole? We present Group Anything with Radiance Fields (GARField), an approach for decomposing 3D scenes into a hierarchy of semantically meaningful groups from posed image inputs. To do this we embrace group ambiguity through physical scale: by optimizing a scale-conditioned 3D affinity feature field, a point in the world can belong to different groups of different sizes. We optimize this field from a set of 2D masks provided by Segment Anything (SAM) in a way that respects coarse-to-fine hierarchy, using scale to consistently fuse conflicting masks from different viewpoints. From this field we can derive a hierarchy of possible groupings via automatic tree construction or user interaction. We evaluate GARField on a variety of in-the-wild scenes and find it effectively extracts groups at many levels: clusters of objects, objects, and various subparts. GARField inherently represents multi-view consistent groupings and produces higher fidelity groups than the input SAM masks. GARField's hierarchical grouping could have exciting downstream applications such as 3D asset extraction or dynamic scene understanding. See the project website at [https://www.garfield.studio/](https://www.garfield.studio/)\n",
            "\n",
            "**Keywords:** Grouping, Radiance Fields, 3D scenes, Hierarchy, Semantically meaningful groups\n",
            "2024-01-18 06:28:33.696 | INFO     | __main__:_act:21 - Huggy(DailyPaperWatcher): ready to SummaryDailyPaper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ". See the project website at [https://www.garfield.studio/](https://www.garfield.studio/)\n",
            "\n",
            "**Keywords:** Grouping, Radiance Fields, 3D scenes, Hierarchy, Semantically meaningful groups\n",
            "# SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding\n",
            "\n",
            "**Abstract:** \n",
            "3D vision-language grounding, which focuses on aligning language with the 3D physical environment, stands as a cornerstone in the development of embodied agents. In comparison to recent advancements in the 2D domain, grounding language in 3D scenes faces several significant challenges: (i) the inherent complexity of 3D scenes due to the diverse object configurations, their rich attributes, and intricate relationships; (ii) the scarcity of paired 3D vision-language data to support grounded learning; and (iii) the absence of a unified learning framework to distill knowledge from grounded 3D data. In this work, we aim to address these three major challenges in 3D vision-language by examining the potential of systematically upscaling 3D vision-language learning in indoor environments. We introduce the first million-scale 3D vision-language dataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising 2.5M vision-language pairs derived from both human annotations and our scalable scene-graph-based generation approach. We demonstrate that this scaling allows for a unified pre-training framework, Grounded Pre-training for Scenes (GPS), for 3D vision-language learning. Through extensive experiments, we showcase the effectiveness of GPS by achieving state-of-the-art performance on all existing 3D visual grounding benchmarks. The vast potential of SceneVerse and GPS is unveiled through zero-shot transfer experiments in the challenging 3D vision-language tasks. Project website: [https://scene-verse.github.io](https://scene-verse.github.io).\n",
            "\n",
            "**Keywords:** 3D vision-language grounding"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-18 06:28:40.199 | INFO     | __main__:_act:32 - # SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding\n",
            "\n",
            "**Abstract:** \n",
            "3D vision-language grounding, which focuses on aligning language with the 3D physical environment, stands as a cornerstone in the development of embodied agents. In comparison to recent advancements in the 2D domain, grounding language in 3D scenes faces several significant challenges: (i) the inherent complexity of 3D scenes due to the diverse object configurations, their rich attributes, and intricate relationships; (ii) the scarcity of paired 3D vision-language data to support grounded learning; and (iii) the absence of a unified learning framework to distill knowledge from grounded 3D data. In this work, we aim to address these three major challenges in 3D vision-language by examining the potential of systematically upscaling 3D vision-language learning in indoor environments. We introduce the first million-scale 3D vision-language dataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising 2.5M vision-language pairs derived from both human annotations and our scalable scene-graph-based generation approach. We demonstrate that this scaling allows for a unified pre-training framework, Grounded Pre-training for Scenes (GPS), for 3D vision-language learning. Through extensive experiments, we showcase the effectiveness of GPS by achieving state-of-the-art performance on all existing 3D visual grounding benchmarks. The vast potential of SceneVerse and GPS is unveiled through zero-shot transfer experiments in the challenging 3D vision-language tasks. Project website: [https://scene-verse.github.io](https://scene-verse.github.io).\n",
            "\n",
            "**Keywords:** 3D vision-language grounding, embodied agents, 3D scenes, grounded learning, vision-language dataset. \n",
            "\n",
            "**arXiv URL:** [https://arxiv.org/abs/2401.09340](https://arxiv.org/abs/2401.09340)\n",
            "2024-01-18 06:28:40.201 | INFO     | __main__:_act:21 - Huggy(DailyPaperWatcher): ready to SummaryDailyPaper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ", embodied agents, 3D scenes, grounded learning, vision-language dataset. \n",
            "\n",
            "**arXiv URL:** [https://arxiv.org/abs/2401.09340](https://arxiv.org/abs/2401.09340)\n",
            "# Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis\n",
            "\n",
            "**Abstract:** Addressing the limitations of text as a source of accurate layout representation in text-conditional diffusion models, many works incorporate additional signals to condition certain attributes within a generated image. Although successful, previous works do not account for the specific localization of said attributes extended into the three dimensional plane. In this context, we present a conditional diffusion model that integrates control over three-dimensional object placement with disentangled representations of global stylistic semantics from multiple exemplar images. Specifically, we first introduce depth disentanglement training to leverage the relative depth of objects as an estimator, allowing the model to identify the absolute positions of unseen objects through the use of synthetic image triplets. We also introduce soft guidance, a method for imposing global semantics onto targeted regions without the use of any additional localization cues. Our integrated framework, Compose and Conquer (CnC), unifies these techniques to localize multiple conditions in a disentangled manner. We demonstrate that our approach allows perception of objects at varying depths while offering a versatile framework for composing localized objects with different global semantics.\n",
            "\n",
            "**Keywords:** diffusion-based image synthesis, 3D depth-aware, composable image synthesis, disentangled representations, global stylistic"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-18 06:28:45.178 | INFO     | __main__:_act:32 - # Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis\n",
            "\n",
            "**Abstract:** Addressing the limitations of text as a source of accurate layout representation in text-conditional diffusion models, many works incorporate additional signals to condition certain attributes within a generated image. Although successful, previous works do not account for the specific localization of said attributes extended into the three dimensional plane. In this context, we present a conditional diffusion model that integrates control over three-dimensional object placement with disentangled representations of global stylistic semantics from multiple exemplar images. Specifically, we first introduce depth disentanglement training to leverage the relative depth of objects as an estimator, allowing the model to identify the absolute positions of unseen objects through the use of synthetic image triplets. We also introduce soft guidance, a method for imposing global semantics onto targeted regions without the use of any additional localization cues. Our integrated framework, Compose and Conquer (CnC), unifies these techniques to localize multiple conditions in a disentangled manner. We demonstrate that our approach allows perception of objects at varying depths while offering a versatile framework for composing localized objects with different global semantics.\n",
            "\n",
            "**Keywords:** diffusion-based image synthesis, 3D depth-aware, composable image synthesis, disentangled representations, global stylistic semantics\n",
            "\n",
            "**ArXiv URL:** [https://arxiv.org/abs/2401.09048](https://arxiv.org/abs/2401.09048)\n",
            "2024-01-18 06:28:45.179 | INFO     | __main__:_act:21 - Huggy(DailyPaperWatcher): ready to SummaryDailyPaper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " semantics\n",
            "\n",
            "**ArXiv URL:** [https://arxiv.org/abs/2401.09048](https://arxiv.org/abs/2401.09048)\n",
            "# ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization\n",
            "\n",
            "**Abstract:** Neural Radiance Fields (NeRF) exhibit remarkable performance for Novel View Synthesis (NVS) given a set of 2D images. However, NeRF training requires accurate camera pose for each input view, typically obtained by Structure-from-Motion (SfM) pipelines. Recent works have attempted to relax this constraint, but they still often rely on decent initial poses which they can refine. Here we aim at removing the requirement for pose initialization. We present Incremental CONfidence (ICON), an optimization procedure for training NeRFs from 2D video frames. ICON only assumes smooth camera motion to estimate initial guess for poses. Further, ICON introduces \"confidence\": an adaptive measure of model quality used to dynamically reweight gradients. ICON relies on high-confidence poses to learn NeRF, and high-confidence 3D structure (as encoded by NeRF) to learn poses. We show that ICON, without prior pose initialization, achieves superior performance in both CO3D and HO3D versus methods which use SfM pose.\n",
            "\n",
            "**Keywords:** Neural Radiance Fields, Novel View Synthesis, Pose Initialization, Incremental Confidence, 2D Video"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-18 06:28:49.409 | INFO     | __main__:_act:32 - # ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization\n",
            "\n",
            "**Abstract:** Neural Radiance Fields (NeRF) exhibit remarkable performance for Novel View Synthesis (NVS) given a set of 2D images. However, NeRF training requires accurate camera pose for each input view, typically obtained by Structure-from-Motion (SfM) pipelines. Recent works have attempted to relax this constraint, but they still often rely on decent initial poses which they can refine. Here we aim at removing the requirement for pose initialization. We present Incremental CONfidence (ICON), an optimization procedure for training NeRFs from 2D video frames. ICON only assumes smooth camera motion to estimate initial guess for poses. Further, ICON introduces \"confidence\": an adaptive measure of model quality used to dynamically reweight gradients. ICON relies on high-confidence poses to learn NeRF, and high-confidence 3D structure (as encoded by NeRF) to learn poses. We show that ICON, without prior pose initialization, achieves superior performance in both CO3D and HO3D versus methods which use SfM pose.\n",
            "\n",
            "**Keywords:** Neural Radiance Fields, Novel View Synthesis, Pose Initialization, Incremental Confidence, 2D Video Frames\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08937)\n",
            "2024-01-18 06:28:49.411 | INFO     | __main__:_act:21 - Huggy(DailyPaperWatcher): ready to SummaryDailyPaper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Frames\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08937)\n",
            "# TextureDreamer: Image-guided Texture Synthesis through Geometry-aware Diffusion\n",
            "\n",
            "**Abstract:** We present TextureDreamer, a novel image-guided texture synthesis method to transfer relightable textures from a small number of input images (3 to 5) to target 3D shapes across arbitrary categories. Texture creation is a pivotal challenge in vision and graphics. Industrial companies hire experienced artists to manually craft textures for 3D assets. Classical methods require densely sampled views and accurately aligned geometry, while learning-based methods are confined to category-specific shapes within the dataset. In contrast, TextureDreamer can transfer highly detailed, intricate textures from real-world environments to arbitrary objects with only a few casually captured images, potentially significantly democratizing texture creation. Our core idea, personalized geometry-aware score distillation (PGSD), draws inspiration from recent advancements in diffuse models, including personalized modeling for texture information extraction, variational score distillation for detailed appearance synthesis, and explicit geometry guidance with ControlNet. Our integration and several essential modifications substantially improve the texture quality. Experiments on real images spanning different categories show that TextureDreamer can successfully transfer highly realistic, semantic meaningful texture to arbitrary objects, surpassing the visual quality of previous state-of-the-art.\n",
            "\n",
            "**Keywords:** Texture synthesis, Image"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-18 06:28:53.407 | INFO     | __main__:_act:32 - # TextureDreamer: Image-guided Texture Synthesis through Geometry-aware Diffusion\n",
            "\n",
            "**Abstract:** We present TextureDreamer, a novel image-guided texture synthesis method to transfer relightable textures from a small number of input images (3 to 5) to target 3D shapes across arbitrary categories. Texture creation is a pivotal challenge in vision and graphics. Industrial companies hire experienced artists to manually craft textures for 3D assets. Classical methods require densely sampled views and accurately aligned geometry, while learning-based methods are confined to category-specific shapes within the dataset. In contrast, TextureDreamer can transfer highly detailed, intricate textures from real-world environments to arbitrary objects with only a few casually captured images, potentially significantly democratizing texture creation. Our core idea, personalized geometry-aware score distillation (PGSD), draws inspiration from recent advancements in diffuse models, including personalized modeling for texture information extraction, variational score distillation for detailed appearance synthesis, and explicit geometry guidance with ControlNet. Our integration and several essential modifications substantially improve the texture quality. Experiments on real images spanning different categories show that TextureDreamer can successfully transfer highly realistic, semantic meaningful texture to arbitrary objects, surpassing the visual quality of previous state-of-the-art.\n",
            "\n",
            "**Keywords:** Texture synthesis, Image-guided, Geometry-aware diffusion, Texture creation, Relightable textures.\n",
            "\n",
            "[arXiv link](https://arxiv.org/abs/2401.09416)\n",
            "2024-01-18 06:28:53.409 | INFO     | __main__:main:5 - DailyPaperWatcher: # Huggingface Daily Paper: 2024-01-18\n",
            "\n",
            "\n",
            "# Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model\n",
            "\n",
            "**Abstract:** Recently, state space models (SSMs) with efficient hardware-aware designs, specifically Mamba, have shown great potential for long sequence modeling. However, representing visual data is challenging for SSMs due to the position-sensitivity and the requirement of global context. In this paper, the authors propose a new generic vision backbone called Vim, which utilizes bidirectional Mamba blocks to compress visual representation. Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating improved computation and memory efficiency. For example, Vim is 2.8 times faster than DeiT and saves 86.8% GPU memory when performing batch inference on high-resolution images. The results show that Vim overcomes computation and memory constraints, making it a promising backbone for vision foundation models.\n",
            "\n",
            "**Keywords:** Vision Mamba, State Space Model, Visual Representation Learning, Computation Efficiency, Memory Efficiency\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.09417)\n",
            "\n",
            "\n",
            "# DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference\n",
            "\n",
            "**Abstract:** The deployment and scaling of large language models (LLMs) have become critical as they permeate various applications, demanding high-throughput and low-latency serving systems. Existing frameworks struggle to balance these requirements, especially for workloads with long prompts. This paper introduces DeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and generation composition strategy, to deliver up to 2.3x higher effective throughput, 2x lower latency on average, and up to 3.7x lower (token-level) tail latency, compared to state-of-the-art systems like vLLM. We leverage a synergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an efficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced implementation supports a range of models and offers both non-persistent and persistent deployment options, catering to diverse user scenarios from interactive sessions to long-running applications. We present a detailed benchmarking methodology, analyze the performance through latency-throughput curves, and investigate scalability via load balancing. Our evaluations demonstrate substantial improvements in throughput and latency across various models and hardware configurations. We discuss our roadmap for future enhancements, including broader model support and new hardware backends. The DeepSpeed-FastGen code is readily available for community engagement and contribution.\n",
            "\n",
            "**Keywords:** Language models, high-throughput text generation, DeepSpeed-FastGen, MII, DeepSpeed-Inference\n",
            "\n",
            "[arXiv URL](https://arxiv.org/abs/2401.08671)\n",
            "\n",
            "\n",
            "# UniVG: Towards UNIfied-modal Video Generation\n",
            "\n",
            "**Abstract:** Diffusion based video generation has received extensive attention and achieved considerable success within both the academic and industrial communities. However, current efforts are mainly concentrated on single-objective or single-task video generation, such as generation driven by text, by image, or by a combination of text and image. This cannot fully meet the needs of real-world application scenarios, as users are likely to input images and text conditions in a flexible manner, either individually or in combination. To address this, we propose a Unified-modal Video Generation system that is capable of handling multiple video generation tasks across text and image modalities. To this end, we revisit the various video generation tasks within our system from the perspective of generative freedom, and classify them into high-freedom and low-freedom video generation categories. For high-freedom video generation, we employ Multi-condition Cross Attention to generate videos that align with the semantics of the input images or text. For low-freedom video generation, we introduce Biased Gaussian Noise to replace the pure random Gaussian Noise, which helps to better preserve the content of the input conditions. Our method achieves the lowest Fréchet Video Distance (FVD) on the public academic benchmark MSR-VTT, surpasses the current open-source methods in human evaluations, and is on par with the current close-source method Gen2. For more samples, visit [https://univg-baidu.github.io](https://univg-baidu.github.io).\n",
            "\n",
            "**Keywords:** video generation, unified-modal, generative freedom, multi-condition cross attention, biased Gaussian noise\n",
            "\n",
            "**arXiv URL:** [https://arxiv.org/abs/2401.09084](https://arxiv.org/abs/2401.09084)\n",
            "\n",
            "\n",
            "# Asynchronous Local-SGD Training for Language Modeling\n",
            "\n",
            "**Abstract:** Local stochastic gradient descent (Local-SGD), also referred to as federated averaging, is an approach to distributed optimization where each device performs more than one SGD update per communication. This work presents an empirical study of *asynchronous* Local-SGD for training language models; that is, each worker updates the global parameters as soon as it has finished its SGD steps. We conduct a comprehensive investigation by examining how worker hardware heterogeneity, model size, number of workers, and optimizer could impact the learning performance. We find that with naive implementations, asynchronous Local-SGD takes more iterations to converge than its synchronous counterpart despite updating the (global) model parameters more frequently. We identify momentum acceleration on the global parameters when worker gradients are stale as a key challenge. We propose a novel method that utilizes a delayed Nesterov momentum update and adjusts the workers' local training steps based on their computation speed. This approach, evaluated with models up to 150M parameters on the C4 dataset, matches the performance of synchronous Local-SGD in terms of perplexity per update step, and significantly surpasses it in terms of wall clock time.\n",
            "\n",
            "**Keywords:** distributed optimization, asynchronous Local-SGD, language modeling, worker hardware heterogeneity, delayed Nesterov momentum update\n",
            "\n",
            "**arXiv URL:** [https://arxiv.org/abs/2401.09135](https://arxiv.org/abs/2401.09135)\n",
            "\n",
            "\n",
            "# ReFT: Reasoning with Reinforced Fine-Tuning\n",
            "\n",
            "**Abstract:** One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question and the rewards are naturally derived from the ground-truth answers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that ReFT significantly outperforms SFT, and the performance can be potentially further boosted by combining inference-time strategies such as majority voting and re-ranking. Note that ReFT obtains the improvement by learning from the same training questions as SFT, without relying on extra or augmented training questions. This indicates a superior generalization ability for ReFT.\n",
            "\n",
            "**Keywords:** Large Language Models, Reinforced Fine-Tuning, Reasoning, Chain-of-Thought, Math problem-solving\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08967)\n",
            "\n",
            "\n",
            "# VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models\n",
            "\n",
            "**Abstract:**\n",
            "Text-to-video generation aims to produce a video based on a given prompt. Recently, several commercial video models have been able to generate plausible videos with minimal noise, excellent details, and high aesthetic scores. However, these models rely on large-scale, well-filtered, high-quality videos that are not accessible to the community. Many existing research works, which train models using the low-quality WebVid-10M dataset, struggle to generate high-quality videos because the models are optimized to fit WebVid-10M. In this work, we explore the training scheme of video models extended from Stable Diffusion and investigate the feasibility of leveraging low-quality videos and synthesized high-quality images to obtain a high-quality video model. We first analyze the connection between the spatial and temporal modules of video models and the distribution shift to low-quality videos. We observe that full training of all modules results in a stronger coupling between spatial and temporal modules than only training temporal modules. Based on this stronger coupling, we shift the distribution to higher quality without motion degradation by finetuning spatial modules with high-quality images, resulting in a generic high-quality video model. Evaluations are conducted to demonstrate the superiority of the proposed method, particularly in picture quality, motion, and concept composition.\n",
            "\n",
            "**Keywords:** Text-to-video generation, video models, high-quality videos, low-quality videos, synthesized high-quality images\n",
            "\n",
            "[arXiv link](https://arxiv.org/abs/2401.09047)\n",
            "\n",
            "\n",
            "# SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers\n",
            "\n",
            "**Abstract:** We present Scalable Interpolant Transformers (SiT), a family of generative models built on the backbone of Diffusion Transformers (DiT). The interpolant framework, which allows for connecting two distributions in a more flexible way than standard diffusion models, makes possible a modular study of various design choices impacting generative models built on dynamical transport: using discrete vs. continuous time learning, deciding the objective for the model to learn, choosing the interpolant connecting the distributions, and deploying a deterministic or stochastic sampler. By carefully introducing the above ingredients, SiT surpasses DiT uniformly across model sizes on the conditional ImageNet 256x256 benchmark using the exact same backbone, number of parameters, and GFLOPs. By exploring various diffusion coefficients, which can be tuned separately from learning, SiT achieves an FID-50K score of 2.06.\n",
            "\n",
            "**Keywords:** generative models, Diffusion Transformers, Scalable Interpolant Transformers, modular study, dynamical transport\n",
            "\n",
            "**arXiv URL:** [https://arxiv.org/abs/2401.08740](https://arxiv.org/abs/2401.08740)\n",
            "\n",
            "\n",
            "# GARField: Group Anything with Radiance Fields\n",
            "\n",
            "**Abstract:** Grouping is inherently ambiguous due to the multiple levels of granularity in which one can decompose a scene -- should the wheels of an excavator be considered separate or part of the whole? We present Group Anything with Radiance Fields (GARField), an approach for decomposing 3D scenes into a hierarchy of semantically meaningful groups from posed image inputs. To do this we embrace group ambiguity through physical scale: by optimizing a scale-conditioned 3D affinity feature field, a point in the world can belong to different groups of different sizes. We optimize this field from a set of 2D masks provided by Segment Anything (SAM) in a way that respects coarse-to-fine hierarchy, using scale to consistently fuse conflicting masks from different viewpoints. From this field we can derive a hierarchy of possible groupings via automatic tree construction or user interaction. We evaluate GARField on a variety of in-the-wild scenes and find it effectively extracts groups at many levels: clusters of objects, objects, and various subparts. GARField inherently represents multi-view consistent groupings and produces higher fidelity groups than the input SAM masks. GARField's hierarchical grouping could have exciting downstream applications such as 3D asset extraction or dynamic scene understanding. See the project website at [https://www.garfield.studio/](https://www.garfield.studio/)\n",
            "\n",
            "**Keywords:** Grouping, Radiance Fields, 3D scenes, Hierarchy, Semantically meaningful groups\n",
            "\n",
            "\n",
            "# SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding\n",
            "\n",
            "**Abstract:** \n",
            "3D vision-language grounding, which focuses on aligning language with the 3D physical environment, stands as a cornerstone in the development of embodied agents. In comparison to recent advancements in the 2D domain, grounding language in 3D scenes faces several significant challenges: (i) the inherent complexity of 3D scenes due to the diverse object configurations, their rich attributes, and intricate relationships; (ii) the scarcity of paired 3D vision-language data to support grounded learning; and (iii) the absence of a unified learning framework to distill knowledge from grounded 3D data. In this work, we aim to address these three major challenges in 3D vision-language by examining the potential of systematically upscaling 3D vision-language learning in indoor environments. We introduce the first million-scale 3D vision-language dataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising 2.5M vision-language pairs derived from both human annotations and our scalable scene-graph-based generation approach. We demonstrate that this scaling allows for a unified pre-training framework, Grounded Pre-training for Scenes (GPS), for 3D vision-language learning. Through extensive experiments, we showcase the effectiveness of GPS by achieving state-of-the-art performance on all existing 3D visual grounding benchmarks. The vast potential of SceneVerse and GPS is unveiled through zero-shot transfer experiments in the challenging 3D vision-language tasks. Project website: [https://scene-verse.github.io](https://scene-verse.github.io).\n",
            "\n",
            "**Keywords:** 3D vision-language grounding, embodied agents, 3D scenes, grounded learning, vision-language dataset. \n",
            "\n",
            "**arXiv URL:** [https://arxiv.org/abs/2401.09340](https://arxiv.org/abs/2401.09340)\n",
            "\n",
            "\n",
            "# Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis\n",
            "\n",
            "**Abstract:** Addressing the limitations of text as a source of accurate layout representation in text-conditional diffusion models, many works incorporate additional signals to condition certain attributes within a generated image. Although successful, previous works do not account for the specific localization of said attributes extended into the three dimensional plane. In this context, we present a conditional diffusion model that integrates control over three-dimensional object placement with disentangled representations of global stylistic semantics from multiple exemplar images. Specifically, we first introduce depth disentanglement training to leverage the relative depth of objects as an estimator, allowing the model to identify the absolute positions of unseen objects through the use of synthetic image triplets. We also introduce soft guidance, a method for imposing global semantics onto targeted regions without the use of any additional localization cues. Our integrated framework, Compose and Conquer (CnC), unifies these techniques to localize multiple conditions in a disentangled manner. We demonstrate that our approach allows perception of objects at varying depths while offering a versatile framework for composing localized objects with different global semantics.\n",
            "\n",
            "**Keywords:** diffusion-based image synthesis, 3D depth-aware, composable image synthesis, disentangled representations, global stylistic semantics\n",
            "\n",
            "**ArXiv URL:** [https://arxiv.org/abs/2401.09048](https://arxiv.org/abs/2401.09048)\n",
            "\n",
            "\n",
            "# ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization\n",
            "\n",
            "**Abstract:** Neural Radiance Fields (NeRF) exhibit remarkable performance for Novel View Synthesis (NVS) given a set of 2D images. However, NeRF training requires accurate camera pose for each input view, typically obtained by Structure-from-Motion (SfM) pipelines. Recent works have attempted to relax this constraint, but they still often rely on decent initial poses which they can refine. Here we aim at removing the requirement for pose initialization. We present Incremental CONfidence (ICON), an optimization procedure for training NeRFs from 2D video frames. ICON only assumes smooth camera motion to estimate initial guess for poses. Further, ICON introduces \"confidence\": an adaptive measure of model quality used to dynamically reweight gradients. ICON relies on high-confidence poses to learn NeRF, and high-confidence 3D structure (as encoded by NeRF) to learn poses. We show that ICON, without prior pose initialization, achieves superior performance in both CO3D and HO3D versus methods which use SfM pose.\n",
            "\n",
            "**Keywords:** Neural Radiance Fields, Novel View Synthesis, Pose Initialization, Incremental Confidence, 2D Video Frames\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08937)\n",
            "\n",
            "\n",
            "# TextureDreamer: Image-guided Texture Synthesis through Geometry-aware Diffusion\n",
            "\n",
            "**Abstract:** We present TextureDreamer, a novel image-guided texture synthesis method to transfer relightable textures from a small number of input images (3 to 5) to target 3D shapes across arbitrary categories. Texture creation is a pivotal challenge in vision and graphics. Industrial companies hire experienced artists to manually craft textures for 3D assets. Classical methods require densely sampled views and accurately aligned geometry, while learning-based methods are confined to category-specific shapes within the dataset. In contrast, TextureDreamer can transfer highly detailed, intricate textures from real-world environments to arbitrary objects with only a few casually captured images, potentially significantly democratizing texture creation. Our core idea, personalized geometry-aware score distillation (PGSD), draws inspiration from recent advancements in diffuse models, including personalized modeling for texture information extraction, variational score distillation for detailed appearance synthesis, and explicit geometry guidance with ControlNet. Our integration and several essential modifications substantially improve the texture quality. Experiments on real images spanning different categories show that TextureDreamer can successfully transfer highly realistic, semantic meaningful texture to arbitrary objects, surpassing the visual quality of previous state-of-the-art.\n",
            "\n",
            "**Keywords:** Texture synthesis, Image-guided, Geometry-aware diffusion, Texture creation, Relightable textures.\n",
            "\n",
            "[arXiv link](https://arxiv.org/abs/2401.09416)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-guided, Geometry-aware diffusion, Texture creation, Relightable textures.\n",
            "\n",
            "[arXiv link](https://arxiv.org/abs/2401.09416)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "current_time = datetime.now()\n",
        "current_time"
      ],
      "metadata": {
        "id": "M6vUzJwMdKNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac4dc505-c521-4c48-9c03-46a621783a43"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime.datetime(2024, 1, 18, 6, 10, 37, 736240)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trigger"
      ],
      "metadata": {
        "id": "23biLFK_T7Ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from aiocron import crontab\n",
        "from typing import Optional\n",
        "from pytz import BaseTzInfo\n",
        "from pydantic import BaseModel, Field\n",
        "from metagpt.schema import Message\n",
        "\n",
        "class DailyPaperInfo(BaseModel):\n",
        "    url: str\n",
        "    timestamp: float = Field(default_factory=time.time)\n",
        "\n",
        "\n",
        "\n",
        "class HuggingfaceDailyPaperCronTrigger():\n",
        "\n",
        "    def __init__(self, spec: str, tz: Optional[BaseTzInfo] = None, url: str = \"https://huggingface.co/papers\") -> None:\n",
        "        self.crontab = crontab(spec, tz=tz)\n",
        "        self.url = url\n",
        "\n",
        "    def __aiter__(self):\n",
        "        return self\n",
        "\n",
        "    async def __anext__(self):\n",
        "        await self.crontab.next()\n",
        "        return Message(self.url, DailyPaperInfo(url=self.url))\n"
      ],
      "metadata": {
        "id": "XJJ3A80uUWVP"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dVsVCUCIUl0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Callback"
      ],
      "metadata": {
        "id": "n8k-cg5j8TfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Discord\n",
        "from google.colab import userdata\n",
        "\n",
        "TOKEN = userdata.get('DISCORD_TOKEN')\n",
        "CHANNEL_ID = userdata.get('DISCORD_CHANNEL_ID')"
      ],
      "metadata": {
        "id": "G_rBWBVC8UEk"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# callback\n",
        "import os\n",
        "import discord\n",
        "async def discord_callback(msg: Message):\n",
        "    intents = discord.Intents.default()\n",
        "    intents.message_content = True\n",
        "    intents.members = True\n",
        "\n",
        "    client = discord.Client(intents=intents)\n",
        "    token = TOKEN\n",
        "    channel_id = int(CHANNEL_ID)\n",
        "\n",
        "    async with client:\n",
        "        await client.login(token)\n",
        "        channel = await client.fetch_channel(channel_id)\n",
        "        lines = []\n",
        "        for i in msg.content.splitlines():\n",
        "            if i.startswith((\"# \", \"## \", \"### \")):\n",
        "                if lines:\n",
        "                    await channel.send(\"\\n\".join(lines))\n",
        "                    lines = []\n",
        "            lines.append(i)\n",
        "\n",
        "        if lines:\n",
        "            await channel.send(\"\\n\".join(lines))"
      ],
      "metadata": {
        "id": "LWjA0Zon8qIq"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "FljHE6qg9BXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from metagpt.subscription import SubscriptionRunner\n",
        "# 运行入口，\n",
        "async def main(spec: str = \"54 16 * * *\", discord: bool = True, wxpusher: bool = False):\n",
        "    callbacks = []\n",
        "    if discord:\n",
        "        callbacks.append(discord_callback)\n",
        "\n",
        "    if wxpusher:\n",
        "        callbacks.append(wxpusher_callback)\n",
        "\n",
        "    if not callbacks:\n",
        "        async def _print(msg: Message):\n",
        "            print(msg.content)\n",
        "        callbacks.append(_print)\n",
        "\n",
        "    async def callback(msg):\n",
        "        await asyncio.gather(*(call(msg) for call in callbacks))\n",
        "\n",
        "    runner = SubscriptionRunner()\n",
        "    await runner.subscribe(DailyPaperWatcher(), HuggingfaceDailyPaperCronTrigger(spec), callback)\n",
        "    await runner.run()"
      ],
      "metadata": {
        "id": "MFaDBk0I87KK"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytz import timezone\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "current_time = datetime.now()\n",
        "target_time = current_time + timedelta(minutes=1)\n",
        "cron_expression = target_time.strftime('%M %H %d %m %w')\n",
        "print(cron_expression)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V53N81N79RAe",
        "outputId": "6b80efb2-3e7c-4d17-aa7b-7a026027f313"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18 06 18 01 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "await main(cron_expression)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uIAuOp1L9TXI",
        "outputId": "230afa2e-6bc0-4184-fbf9-66f4c722c23a"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-18 06:18:00.002 | INFO     | __main__:_act:21 - Huggy(DailyPaperWatcher): ready to CrawlHuggingfaceDailyPaper\n",
            "2024-01-18 06:18:02.998 | INFO     | __main__:_act:28 - [{'title': 'Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model', 'abstract': 'Recently the state space models (SSMs) with efficient hardware-aware designs,\\ni.e., Mamba, have shown great potential for long sequence modeling. Building\\nefficient and generic vision backbones purely upon SSMs is an appealing\\ndirection. However, representing visual data is challenging for SSMs due to the\\nposition-sensitivity of visual data and the requirement of global context for\\nvisual understanding. In this paper, we show that the reliance of visual\\nrepresentation learning on self-attention is not necessary and propose a new\\ngeneric vision backbone with bidirectional Mamba blocks (Vim), which marks the\\nimage sequences with position embeddings and compresses the visual\\nrepresentation with bidirectional state space models. On ImageNet\\nclassification, COCO object detection, and ADE20k semantic segmentation tasks,\\nVim achieves higher performance compared to well-established vision\\ntransformers like DeiT, while also demonstrating significantly improved\\ncomputation & memory efficiency. For example, Vim is 2.8times faster than\\nDeiT and saves 86.8% GPU memory when performing batch inference to extract\\nfeatures on images with a resolution of 1248times1248. The results\\ndemonstrate that Vim is capable of overcoming the computation & memory\\nconstraints on performing Transformer-style understanding for high-resolution\\nimages and it has great potential to become the next-generation backbone for\\nvision foundation models. Code is available at https://github.com/hustvl/Vim.', 'arxiv_url': 'https://arxiv.org/abs/2401.09417'}, {'title': 'DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference', 'abstract': \"The deployment and scaling of large language models (LLMs) have become\\ncritical as they permeate various applications, demanding high-throughput and\\nlow-latency serving systems. Existing frameworks struggle to balance these\\nrequirements, especially for workloads with long prompts. This paper introduces\\nDeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and\\ngeneration composition strategy, to deliver up to 2.3x higher effective\\nthroughput, 2x lower latency on average, and up to 3.7x lower (token-level)\\ntail latency, compared to state-of-the-art systems like vLLM. We leverage a\\nsynergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an\\nefficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced\\nimplementation supports a range of models and offers both non-persistent and\\npersistent deployment options, catering to diverse user scenarios from\\ninteractive sessions to long-running applications. We present a detailed\\nbenchmarking methodology, analyze the performance through latency-throughput\\ncurves, and investigate scalability via load balancing. Our evaluations\\ndemonstrate substantial improvements in throughput and latency across various\\nmodels and hardware configurations. We discuss our roadmap for future\\nenhancements, including broader model support and new hardware backends. The\\nDeepSpeed-FastGen code is readily available for community engagement and\\ncontribution.\", 'arxiv_url': 'https://arxiv.org/abs/2401.08671'}, {'title': 'UniVG: Towards UNIfied-modal Video Generation', 'abstract': \"Diffusion based video generation has received extensive attention and\\nachieved considerable success within both the academic and industrial\\ncommunities. However, current efforts are mainly concentrated on\\nsingle-objective or single-task video generation, such as generation driven by\\ntext, by image, or by a combination of text and image. This cannot fully meet\\nthe needs of real-world application scenarios, as users are likely to input\\nimages and text conditions in a flexible manner, either individually or in\\ncombination. To address this, we propose a Unified-modal Video Genearation\\nsystem that is capable of handling multiple video generation tasks across text\\nand image modalities. To this end, we revisit the various video generation\\ntasks within our system from the perspective of generative freedom, and\\nclassify them into high-freedom and low-freedom video generation categories.\\nFor high-freedom video generation, we employ Multi-condition Cross Attention to\\ngenerate videos that align with the semantics of the input images or text. For\\nlow-freedom video generation, we introduce Biased Gaussian Noise to replace the\\npure random Gaussian Noise, which helps to better preserve the content of the\\ninput conditions. Our method achieves the lowest Fr\\\\'echet Video Distance (FVD)\\non the public academic benchmark MSR-VTT, surpasses the current open-source\\nmethods in human evaluations, and is on par with the current close-source\\nmethod Gen2. For more samples, visit https://univg-baidu.github.io.\", 'arxiv_url': 'https://arxiv.org/abs/2401.09084'}, {'title': 'Asynchronous Local-SGD Training for Language Modeling', 'abstract': \"Local stochastic gradient descent (Local-SGD), also referred to as federated\\naveraging, is an approach to distributed optimization where each device\\nperforms more than one SGD update per communication. This work presents an\\nempirical study of {\\\\it asynchronous} Local-SGD for training language models;\\nthat is, each worker updates the global parameters as soon as it has finished\\nits SGD steps. We conduct a comprehensive investigation by examining how worker\\nhardware heterogeneity, model size, number of workers, and optimizer could\\nimpact the learning performance. We find that with naive implementations,\\nasynchronous Local-SGD takes more iterations to converge than its synchronous\\ncounterpart despite updating the (global) model parameters more frequently. We\\nidentify momentum acceleration on the global parameters when worker gradients\\nare stale as a key challenge. We propose a novel method that utilizes a delayed\\nNesterov momentum update and adjusts the workers' local training steps based on\\ntheir computation speed. This approach, evaluated with models up to 150M\\nparameters on the C4 dataset, matches the performance of synchronous Local-SGD\\nin terms of perplexity per update step, and significantly surpasses it in terms\\nof wall clock time.\", 'arxiv_url': 'https://arxiv.org/abs/2401.09135'}, {'title': 'ReFT: Reasoning with Reinforced Fine-Tuning', 'abstract': 'One way to enhance the reasoning capability of Large Language Models (LLMs)\\nis to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT)\\nannotations. This approach does not show sufficiently strong generalization\\nability, however, because the training only relies on the given CoT data. In\\nmath problem-solving, for example, there is usually only one annotated\\nreasoning path for each question in the training data. Intuitively, it would be\\nbetter for the algorithm to learn from multiple annotated reasoning paths given\\na question. To address this issue, we propose a simple yet effective approach\\ncalled Reinforced Fine-Tuning (ReFT) to enhance the generalizability of\\nlearning LLMs for reasoning, with math problem-solving as an example. ReFT\\nfirst warmups the model with SFT, and then employs on-line reinforcement\\nlearning, specifically the PPO algorithm in this paper, to further fine-tune\\nthe model, where an abundance of reasoning paths are automatically sampled\\ngiven the question and the rewards are naturally derived from the ground-truth\\nanswers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that\\nReFT significantly outperforms SFT, and the performance can be potentially\\nfurther boosted by combining inference-time strategies such as majority voting\\nand re-ranking. Note that ReFT obtains the improvement by learning from the\\nsame training questions as SFT, without relying on extra or augmented training\\nquestions. This indicates a superior generalization ability for ReFT.', 'arxiv_url': 'https://arxiv.org/abs/2401.08967'}, {'title': 'VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models', 'abstract': 'Text-to-video generation aims to produce a video based on a given prompt.\\nRecently, several commercial video models have been able to generate plausible\\nvideos with minimal noise, excellent details, and high aesthetic scores.\\nHowever, these models rely on large-scale, well-filtered, high-quality videos\\nthat are not accessible to the community. Many existing research works, which\\ntrain models using the low-quality WebVid-10M dataset, struggle to generate\\nhigh-quality videos because the models are optimized to fit WebVid-10M. In this\\nwork, we explore the training scheme of video models extended from Stable\\nDiffusion and investigate the feasibility of leveraging low-quality videos and\\nsynthesized high-quality images to obtain a high-quality video model. We first\\nanalyze the connection between the spatial and temporal modules of video models\\nand the distribution shift to low-quality videos. We observe that full training\\nof all modules results in a stronger coupling between spatial and temporal\\nmodules than only training temporal modules. Based on this stronger coupling,\\nwe shift the distribution to higher quality without motion degradation by\\nfinetuning spatial modules with high-quality images, resulting in a generic\\nhigh-quality video model. Evaluations are conducted to demonstrate the\\nsuperiority of the proposed method, particularly in picture quality, motion,\\nand concept composition.', 'arxiv_url': 'https://arxiv.org/abs/2401.09047'}, {'title': 'SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers', 'abstract': 'We present Scalable Interpolant Transformers (SiT), a family of generative\\nmodels built on the backbone of Diffusion Transformers (DiT). The interpolant\\nframework, which allows for connecting two distributions in a more flexible way\\nthan standard diffusion models, makes possible a modular study of various\\ndesign choices impacting generative models built on dynamical transport: using\\ndiscrete vs. continuous time learning, deciding the objective for the model to\\nlearn, choosing the interpolant connecting the distributions, and deploying a\\ndeterministic or stochastic sampler. By carefully introducing the above\\ningredients, SiT surpasses DiT uniformly across model sizes on the conditional\\nImageNet 256x256 benchmark using the exact same backbone, number of parameters,\\nand GFLOPs. By exploring various diffusion coefficients, which can be tuned\\nseparately from learning, SiT achieves an FID-50K score of 2.06.', 'arxiv_url': 'https://arxiv.org/abs/2401.08740'}, {'title': 'GARField: Group Anything with Radiance Fields', 'abstract': \"Grouping is inherently ambiguous due to the multiple levels of granularity in\\nwhich one can decompose a scene -- should the wheels of an excavator be\\nconsidered separate or part of the whole? We present Group Anything with\\nRadiance Fields (GARField), an approach for decomposing 3D scenes into a\\nhierarchy of semantically meaningful groups from posed image inputs. To do this\\nwe embrace group ambiguity through physical scale: by optimizing a\\nscale-conditioned 3D affinity feature field, a point in the world can belong to\\ndifferent groups of different sizes. We optimize this field from a set of 2D\\nmasks provided by Segment Anything (SAM) in a way that respects coarse-to-fine\\nhierarchy, using scale to consistently fuse conflicting masks from different\\nviewpoints. From this field we can derive a hierarchy of possible groupings via\\nautomatic tree construction or user interaction. We evaluate GARField on a\\nvariety of in-the-wild scenes and find it effectively extracts groups at many\\nlevels: clusters of objects, objects, and various subparts. GARField inherently\\nrepresents multi-view consistent groupings and produces higher fidelity groups\\nthan the input SAM masks. GARField's hierarchical grouping could have exciting\\ndownstream applications such as 3D asset extraction or dynamic scene\\nunderstanding. See the project website at https://www.garfield.studio/\", 'arxiv_url': 'https://arxiv.org/abs/2401.09419'}, {'title': 'SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding', 'abstract': '3D vision-language grounding, which focuses on aligning language with the 3D\\nphysical environment, stands as a cornerstone in the development of embodied\\nagents. In comparison to recent advancements in the 2D domain, grounding\\nlanguage in 3D scenes faces several significant challenges: (i) the inherent\\ncomplexity of 3D scenes due to the diverse object configurations, their rich\\nattributes, and intricate relationships; (ii) the scarcity of paired 3D\\nvision-language data to support grounded learning; and (iii) the absence of a\\nunified learning framework to distill knowledge from grounded 3D data. In this\\nwork, we aim to address these three major challenges in 3D vision-language by\\nexamining the potential of systematically upscaling 3D vision-language learning\\nin indoor environments. We introduce the first million-scale 3D vision-language\\ndataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising\\n2.5M vision-language pairs derived from both human annotations and our scalable\\nscene-graph-based generation approach. We demonstrate that this scaling allows\\nfor a unified pre-training framework, Grounded Pre-training for Scenes (GPS),\\nfor 3D vision-language learning. Through extensive experiments, we showcase the\\neffectiveness of GPS by achieving state-of-the-art performance on all existing\\n3D visual grounding benchmarks. The vast potential of SceneVerse and GPS is\\nunveiled through zero-shot transfer experiments in the challenging 3D\\nvision-language tasks. Project website: https://scene-verse.github.io .', 'arxiv_url': 'https://arxiv.org/abs/2401.09340'}, {'title': 'Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis', 'abstract': 'Addressing the limitations of text as a source of accurate layout\\nrepresentation in text-conditional diffusion models, many works incorporate\\nadditional signals to condition certain attributes within a generated image.\\nAlthough successful, previous works do not account for the specific\\nlocalization of said attributes extended into the three dimensional plane. In\\nthis context, we present a conditional diffusion model that integrates control\\nover three-dimensional object placement with disentangled representations of\\nglobal stylistic semantics from multiple exemplar images. Specifically, we\\nfirst introduce depth disentanglement training to leverage the\\nrelative depth of objects as an estimator, allowing the model to identify the\\nabsolute positions of unseen objects through the use of synthetic image\\ntriplets. We also introduce soft guidance, a method for imposing\\nglobal semantics onto targeted regions without the use of any additional\\nlocalization cues. Our integrated framework, Compose and Conquer\\n(CnC), unifies these techniques to localize multiple conditions in a\\ndisentangled manner. We demonstrate that our approach allows perception of\\nobjects at varying depths while offering a versatile framework for composing\\nlocalized objects with different global semantics. Code:\\nhttps://github.com/tomtom1103/compose-and-conquer/', 'arxiv_url': 'https://arxiv.org/abs/2401.09048'}, {'title': 'ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization', 'abstract': 'Neural Radiance Fields (NeRF) exhibit remarkable performance for Novel View\\nSynthesis (NVS) given a set of 2D images. However, NeRF training requires\\naccurate camera pose for each input view, typically obtained by\\nStructure-from-Motion (SfM) pipelines. Recent works have attempted to relax\\nthis constraint, but they still often rely on decent initial poses which they\\ncan refine. Here we aim at removing the requirement for pose initialization. We\\npresent Incremental CONfidence (ICON), an optimization procedure for training\\nNeRFs from 2D video frames. ICON only assumes smooth camera motion to estimate\\ninitial guess for poses. Further, ICON introduces ``confidence\": an adaptive\\nmeasure of model quality used to dynamically reweight gradients. ICON relies on\\nhigh-confidence poses to learn NeRF, and high-confidence 3D structure (as\\nencoded by NeRF) to learn poses. We show that ICON, without prior pose\\ninitialization, achieves superior performance in both CO3D and HO3D versus\\nmethods which use SfM pose.', 'arxiv_url': 'https://arxiv.org/abs/2401.08937'}, {'title': 'TextureDreamer: Image-guided Texture Synthesis through Geometry-aware Diffusion', 'abstract': 'We present TextureDreamer, a novel image-guided texture synthesis method to\\ntransfer relightable textures from a small number of input images (3 to 5) to\\ntarget 3D shapes across arbitrary categories. Texture creation is a pivotal\\nchallenge in vision and graphics. Industrial companies hire experienced artists\\nto manually craft textures for 3D assets. Classical methods require densely\\nsampled views and accurately aligned geometry, while learning-based methods are\\nconfined to category-specific shapes within the dataset. In contrast,\\nTextureDreamer can transfer highly detailed, intricate textures from real-world\\nenvironments to arbitrary objects with only a few casually captured images,\\npotentially significantly democratizing texture creation. Our core idea,\\npersonalized geometry-aware score distillation (PGSD), draws inspiration from\\nrecent advancements in diffuse models, including personalized modeling for\\ntexture information extraction, variational score distillation for detailed\\nappearance synthesis, and explicit geometry guidance with ControlNet. Our\\nintegration and several essential modifications substantially improve the\\ntexture quality. Experiments on real images spanning different categories show\\nthat TextureDreamer can successfully transfer highly realistic, semantic\\nmeaningful texture to arbitrary objects, surpassing the visual quality of\\nprevious state-of-the-art.', 'arxiv_url': 'https://arxiv.org/abs/2401.09416'}]\n",
            "2024-01-18 06:18:03.002 | INFO     | __main__:_act:21 - Huggy(DailyPaperWatcher): ready to SummaryDailyPaper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model\n",
            "\n",
            "**Abstract:** Recently, state space models (SSMs) with efficient hardware-aware designs, known as Mamba, have shown great potential for long sequence modeling. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, the authors propose a new generic vision backbone called Vim, which combines bidirectional Mamba blocks with position embeddings to compress visual representations. Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation and memory efficiency. For example, Vim is 2.8 times faster than DeiT and saves 86.8% GPU memory when performing batch inference on high-resolution images. The results show that Vim overcomes computation and memory constraints and has the potential to become the next-generation backbone for vision foundation models.\n",
            "\n",
            "**Keywords:** Vision backbone, state space models, visual representation learning, bidirectional Mamba blocks, computation efficiency\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.09417)"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-18 06:18:06.115 | INFO     | __main__:_act:32 - # Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model\n",
            "\n",
            "**Abstract:** Recently, state space models (SSMs) with efficient hardware-aware designs, known as Mamba, have shown great potential for long sequence modeling. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, the authors propose a new generic vision backbone called Vim, which combines bidirectional Mamba blocks with position embeddings to compress visual representations. Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation and memory efficiency. For example, Vim is 2.8 times faster than DeiT and saves 86.8% GPU memory when performing batch inference on high-resolution images. The results show that Vim overcomes computation and memory constraints and has the potential to become the next-generation backbone for vision foundation models.\n",
            "\n",
            "**Keywords:** Vision backbone, state space models, visual representation learning, bidirectional Mamba blocks, computation efficiency\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.09417)\n",
            "2024-01-18 06:18:06.116 | INFO     | __main__:_act:21 - Huggy(DailyPaperWatcher): ready to SummaryDailyPaper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# Huggingface Daily Paper: 2024-01-18 <class 'str'>\n",
            "# DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference\n",
            "\n",
            "**Abstract:** The deployment and scaling of large language models (LLMs) have become critical as they permeate various applications, demanding high-throughput and low-latency serving systems. Existing frameworks struggle to balance these requirements, especially for workloads with long prompts. This paper introduces DeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and generation composition strategy, to deliver up to 2.3x higher effective throughput, 2x lower latency on average, and up to 3.7x lower (token-level) tail latency, compared to state-of-the-art systems like vLLM. We leverage a synergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an efficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced implementation supports a range of models and offers both non-persistent and persistent deployment options, catering to diverse user scenarios from interactive sessions to long-running applications. We present a detailed benchmarking methodology, analyze the performance through latency-throughput curves, and investigate scalability via load balancing. Our evaluations demonstrate substantial improvements in throughput and latency across various models and hardware configurations. We discuss our roadmap for future enhancements, including broader model support and new hardware backends. The DeepSpeed-FastGen code is readily available for community engagement"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-18 06:18:10.764 | INFO     | __main__:_act:32 - # DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference\n",
            "\n",
            "**Abstract:** The deployment and scaling of large language models (LLMs) have become critical as they permeate various applications, demanding high-throughput and low-latency serving systems. Existing frameworks struggle to balance these requirements, especially for workloads with long prompts. This paper introduces DeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and generation composition strategy, to deliver up to 2.3x higher effective throughput, 2x lower latency on average, and up to 3.7x lower (token-level) tail latency, compared to state-of-the-art systems like vLLM. We leverage a synergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an efficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced implementation supports a range of models and offers both non-persistent and persistent deployment options, catering to diverse user scenarios from interactive sessions to long-running applications. We present a detailed benchmarking methodology, analyze the performance through latency-throughput curves, and investigate scalability via load balancing. Our evaluations demonstrate substantial improvements in throughput and latency across various models and hardware configurations. We discuss our roadmap for future enhancements, including broader model support and new hardware backends. The DeepSpeed-FastGen code is readily available for community engagement and contribution.\n",
            "\n",
            "**Keywords:** Language models, Text generation, High-throughput, Low-latency, DeepSpeed-FastGen\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08671)\n",
            "2024-01-18 06:18:10.765 | INFO     | __main__:_act:21 - Huggy(DailyPaperWatcher): ready to SummaryDailyPaper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " and contribution.\n",
            "\n",
            "**Keywords:** Language models, Text generation, High-throughput, Low-latency, DeepSpeed-FastGen\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08671)\n",
            "# Huggingface Daily Paper: 2024-01-18\n",
            "\n",
            "\n",
            "# Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model\n",
            "\n",
            "**Abstract:** Recently, state space models (SSMs) with efficient hardware-aware designs, known as Mamba, have shown great potential for long sequence modeling. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, the authors propose a new generic vision backbone called Vim, which combines bidirectional Mamba blocks with position embeddings to compress visual representations. Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation and memory efficiency. For example, Vim is 2.8 times faster than DeiT and saves 86.8% GPU memory when performing batch inference on high-resolution images. The results show that Vim overcomes computation and memory constraints and has the potential to become the next-generation backbone for vision foundation models.\n",
            "\n",
            "**Keywords:** Vision backbone, state space models, visual representation learning, bidirectional Mamba blocks, computation efficiency\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.09417) <class 'str'>\n",
            "# UniVG: Towards UNIfied-modal Video Generation\n",
            "\n",
            "**Abstract:** Diffusion based video generation has received extensive attention and achieved considerable success within both the academic and industrial communities. However, current efforts are mainly concentrated on single-objective or single-task video generation, such as generation driven by text, by image, or by a combination of text and image. This cannot fully meet the needs of real-world application scenarios, as users are likely to input images and text conditions in a flexible manner, either individually or in combination. To address this, we propose a Unified-modal Video Generation system that is capable of handling multiple video generation tasks across text and image modalities. To this end, we revisit the various video generation tasks within our system from the perspective of generative freedom, and classify them into high-freedom and low-freedom video generation categories. For high-freedom video generation, we employ Multi-condition Cross Attention to generate videos that align with the semantics of the input images or text. For low-freedom video generation, we introduce Biased Gaussian Noise to replace the pure random Gaussian Noise, which helps to better preserve the content of the input conditions. Our method achieves the lowest Fréchet Video Distance (FVD) on the public academic benchmark MSR-VTT, surpasses the current open-source methods in human evaluations, and is on par with the current close-source method Gen2. For more samples, visit [here](https://univg-baidu.github.io).\n",
            "\n",
            "**Keywords:** video generation, unified-modal, generative freedom, multi-condition cross attention, biased"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-18 06:18:16.115 | INFO     | __main__:_act:32 - # UniVG: Towards UNIfied-modal Video Generation\n",
            "\n",
            "**Abstract:** Diffusion based video generation has received extensive attention and achieved considerable success within both the academic and industrial communities. However, current efforts are mainly concentrated on single-objective or single-task video generation, such as generation driven by text, by image, or by a combination of text and image. This cannot fully meet the needs of real-world application scenarios, as users are likely to input images and text conditions in a flexible manner, either individually or in combination. To address this, we propose a Unified-modal Video Generation system that is capable of handling multiple video generation tasks across text and image modalities. To this end, we revisit the various video generation tasks within our system from the perspective of generative freedom, and classify them into high-freedom and low-freedom video generation categories. For high-freedom video generation, we employ Multi-condition Cross Attention to generate videos that align with the semantics of the input images or text. For low-freedom video generation, we introduce Biased Gaussian Noise to replace the pure random Gaussian Noise, which helps to better preserve the content of the input conditions. Our method achieves the lowest Fréchet Video Distance (FVD) on the public academic benchmark MSR-VTT, surpasses the current open-source methods in human evaluations, and is on par with the current close-source method Gen2. For more samples, visit [here](https://univg-baidu.github.io).\n",
            "\n",
            "**Keywords:** video generation, unified-modal, generative freedom, multi-condition cross attention, biased Gaussian noise\n",
            "\n",
            "**arXiv URL:** [https://arxiv.org/abs/2401.09084](https://arxiv.org/abs/2401.09084)\n",
            "2024-01-18 06:18:16.117 | INFO     | __main__:_act:21 - Huggy(DailyPaperWatcher): ready to SummaryDailyPaper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Gaussian noise\n",
            "\n",
            "**arXiv URL:** [https://arxiv.org/abs/2401.09084](https://arxiv.org/abs/2401.09084)\n",
            "# Huggingface Daily Paper: 2024-01-18\n",
            "\n",
            "\n",
            "# Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model\n",
            "\n",
            "**Abstract:** Recently, state space models (SSMs) with efficient hardware-aware designs, known as Mamba, have shown great potential for long sequence modeling. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, the authors propose a new generic vision backbone called Vim, which combines bidirectional Mamba blocks with position embeddings to compress visual representations. Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation and memory efficiency. For example, Vim is 2.8 times faster than DeiT and saves 86.8% GPU memory when performing batch inference on high-resolution images. The results show that Vim overcomes computation and memory constraints and has the potential to become the next-generation backbone for vision foundation models.\n",
            "\n",
            "**Keywords:** Vision backbone, state space models, visual representation learning, bidirectional Mamba blocks, computation efficiency\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.09417)\n",
            "\n",
            "\n",
            "# DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference\n",
            "\n",
            "**Abstract:** The deployment and scaling of large language models (LLMs) have become critical as they permeate various applications, demanding high-throughput and low-latency serving systems. Existing frameworks struggle to balance these requirements, especially for workloads with long prompts. This paper introduces DeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and generation composition strategy, to deliver up to 2.3x higher effective throughput, 2x lower latency on average, and up to 3.7x lower (token-level) tail latency, compared to state-of-the-art systems like vLLM. We leverage a synergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an efficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced implementation supports a range of models and offers both non-persistent and persistent deployment options, catering to diverse user scenarios from interactive sessions to long-running applications. We present a detailed benchmarking methodology, analyze the performance through latency-throughput curves, and investigate scalability via load balancing. Our evaluations demonstrate substantial improvements in throughput and latency across various models and hardware configurations. We discuss our roadmap for future enhancements, including broader model support and new hardware backends. The DeepSpeed-FastGen code is readily available for community engagement and contribution.\n",
            "\n",
            "**Keywords:** Language models, Text generation, High-throughput, Low-latency, DeepSpeed-FastGen\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08671) <class 'str'>\n",
            "# Asynchronous Local-SGD Training for Language Modeling\n",
            "\n",
            "**Abstract:** Local stochastic gradient descent (Local-SGD), also referred to as federated averaging, is an approach to distributed optimization where each device performs more than one SGD update per communication. This work presents an empirical study of *asynchronous* Local-SGD for training language models; that is, each worker updates the global parameters as soon as it has finished its SGD steps. We conduct a comprehensive investigation by examining how worker hardware heterogeneity, model size, number of workers, and optimizer could impact the learning performance. We find that with naive implementations, asynchronous Local-SGD takes more iterations to converge than its synchronous counterpart despite updating the (global) model parameters more frequently. We identify momentum acceleration on the global parameters when worker gradients are stale as a key challenge. We propose a novel method that utilizes a delayed Nesterov momentum update and adjusts the workers' local training steps based on their computation speed. This approach, evaluated with models up to 150M parameters on the C4 dataset, matches the performance of synchronous Local-SGD in terms of perplexity per update step, and significantly surpasses it in terms of wall clock time.\n",
            "\n",
            "**Keywords:** Local-SGD, asynchronous training"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-18 06:18:19.757 | INFO     | __main__:_act:32 - # Asynchronous Local-SGD Training for Language Modeling\n",
            "\n",
            "**Abstract:** Local stochastic gradient descent (Local-SGD), also referred to as federated averaging, is an approach to distributed optimization where each device performs more than one SGD update per communication. This work presents an empirical study of *asynchronous* Local-SGD for training language models; that is, each worker updates the global parameters as soon as it has finished its SGD steps. We conduct a comprehensive investigation by examining how worker hardware heterogeneity, model size, number of workers, and optimizer could impact the learning performance. We find that with naive implementations, asynchronous Local-SGD takes more iterations to converge than its synchronous counterpart despite updating the (global) model parameters more frequently. We identify momentum acceleration on the global parameters when worker gradients are stale as a key challenge. We propose a novel method that utilizes a delayed Nesterov momentum update and adjusts the workers' local training steps based on their computation speed. This approach, evaluated with models up to 150M parameters on the C4 dataset, matches the performance of synchronous Local-SGD in terms of perplexity per update step, and significantly surpasses it in terms of wall clock time.\n",
            "\n",
            "**Keywords:** Local-SGD, asynchronous training, language modeling, distributed optimization, Nesterov momentum\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.09135)\n",
            "2024-01-18 06:18:19.758 | INFO     | __main__:_act:21 - Huggy(DailyPaperWatcher): ready to SummaryDailyPaper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ", language modeling, distributed optimization, Nesterov momentum\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.09135)\n",
            "# Huggingface Daily Paper: 2024-01-18\n",
            "\n",
            "\n",
            "# Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model\n",
            "\n",
            "**Abstract:** Recently, state space models (SSMs) with efficient hardware-aware designs, known as Mamba, have shown great potential for long sequence modeling. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, the authors propose a new generic vision backbone called Vim, which combines bidirectional Mamba blocks with position embeddings to compress visual representations. Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation and memory efficiency. For example, Vim is 2.8 times faster than DeiT and saves 86.8% GPU memory when performing batch inference on high-resolution images. The results show that Vim overcomes computation and memory constraints and has the potential to become the next-generation backbone for vision foundation models.\n",
            "\n",
            "**Keywords:** Vision backbone, state space models, visual representation learning, bidirectional Mamba blocks, computation efficiency\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.09417)\n",
            "\n",
            "\n",
            "# DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference\n",
            "\n",
            "**Abstract:** The deployment and scaling of large language models (LLMs) have become critical as they permeate various applications, demanding high-throughput and low-latency serving systems. Existing frameworks struggle to balance these requirements, especially for workloads with long prompts. This paper introduces DeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and generation composition strategy, to deliver up to 2.3x higher effective throughput, 2x lower latency on average, and up to 3.7x lower (token-level) tail latency, compared to state-of-the-art systems like vLLM. We leverage a synergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an efficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced implementation supports a range of models and offers both non-persistent and persistent deployment options, catering to diverse user scenarios from interactive sessions to long-running applications. We present a detailed benchmarking methodology, analyze the performance through latency-throughput curves, and investigate scalability via load balancing. Our evaluations demonstrate substantial improvements in throughput and latency across various models and hardware configurations. We discuss our roadmap for future enhancements, including broader model support and new hardware backends. The DeepSpeed-FastGen code is readily available for community engagement and contribution.\n",
            "\n",
            "**Keywords:** Language models, Text generation, High-throughput, Low-latency, DeepSpeed-FastGen\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08671)\n",
            "\n",
            "\n",
            "# UniVG: Towards UNIfied-modal Video Generation\n",
            "\n",
            "**Abstract:** Diffusion based video generation has received extensive attention and achieved considerable success within both the academic and industrial communities. However, current efforts are mainly concentrated on single-objective or single-task video generation, such as generation driven by text, by image, or by a combination of text and image. This cannot fully meet the needs of real-world application scenarios, as users are likely to input images and text conditions in a flexible manner, either individually or in combination. To address this, we propose a Unified-modal Video Generation system that is capable of handling multiple video generation tasks across text and image modalities. To this end, we revisit the various video generation tasks within our system from the perspective of generative freedom, and classify them into high-freedom and low-freedom video generation categories. For high-freedom video generation, we employ Multi-condition Cross Attention to generate videos that align with the semantics of the input images or text. For low-freedom video generation, we introduce Biased Gaussian Noise to replace the pure random Gaussian Noise, which helps to better preserve the content of the input conditions. Our method achieves the lowest Fréchet Video Distance (FVD) on the public academic benchmark MSR-VTT, surpasses the current open-source methods in human evaluations, and is on par with the current close-source method Gen2. For more samples, visit [here](https://univg-baidu.github.io).\n",
            "\n",
            "**Keywords:** video generation, unified-modal, generative freedom, multi-condition cross attention, biased Gaussian noise\n",
            "\n",
            "**arXiv URL:** [https://arxiv.org/abs/2401.09084](https://arxiv.org/abs/2401.09084) <class 'str'>\n",
            "# ReFT: Reasoning with Reinforced Fine-Tuning\n",
            "\n",
            "**Abstract:** One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question and the rewards are naturally derived from the ground-truth answers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that ReFT significantly outperforms SFT, and the performance can be potentially further boosted by combining inference-time strategies such as majority voting and re-ranking. Note that ReFT obtains the improvement by learning from the same training questions as SFT, without relying on extra or augmented training questions. This indicates a superior generalization ability for ReFT.\n",
            "\n",
            "**Keywords:** Large Language Models, Reinforced Fine-Tuning, Reasoning, Chain-of-Thought"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-18 06:18:23.749 | INFO     | __main__:_act:32 - # ReFT: Reasoning with Reinforced Fine-Tuning\n",
            "\n",
            "**Abstract:** One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question and the rewards are naturally derived from the ground-truth answers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that ReFT significantly outperforms SFT, and the performance can be potentially further boosted by combining inference-time strategies such as majority voting and re-ranking. Note that ReFT obtains the improvement by learning from the same training questions as SFT, without relying on extra or augmented training questions. This indicates a superior generalization ability for ReFT.\n",
            "\n",
            "**Keywords:** Large Language Models, Reinforced Fine-Tuning, Reasoning, Chain-of-Thought, Math problem-solving\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08967)\n",
            "2024-01-18 06:18:23.750 | INFO     | __main__:_act:21 - Huggy(DailyPaperWatcher): ready to SummaryDailyPaper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ", Math problem-solving\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08967)\n",
            "# Huggingface Daily Paper: 2024-01-18\n",
            "\n",
            "\n",
            "# Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model\n",
            "\n",
            "**Abstract:** Recently, state space models (SSMs) with efficient hardware-aware designs, known as Mamba, have shown great potential for long sequence modeling. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, the authors propose a new generic vision backbone called Vim, which combines bidirectional Mamba blocks with position embeddings to compress visual representations. Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation and memory efficiency. For example, Vim is 2.8 times faster than DeiT and saves 86.8% GPU memory when performing batch inference on high-resolution images. The results show that Vim overcomes computation and memory constraints and has the potential to become the next-generation backbone for vision foundation models.\n",
            "\n",
            "**Keywords:** Vision backbone, state space models, visual representation learning, bidirectional Mamba blocks, computation efficiency\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.09417)\n",
            "\n",
            "\n",
            "# DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference\n",
            "\n",
            "**Abstract:** The deployment and scaling of large language models (LLMs) have become critical as they permeate various applications, demanding high-throughput and low-latency serving systems. Existing frameworks struggle to balance these requirements, especially for workloads with long prompts. This paper introduces DeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and generation composition strategy, to deliver up to 2.3x higher effective throughput, 2x lower latency on average, and up to 3.7x lower (token-level) tail latency, compared to state-of-the-art systems like vLLM. We leverage a synergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an efficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced implementation supports a range of models and offers both non-persistent and persistent deployment options, catering to diverse user scenarios from interactive sessions to long-running applications. We present a detailed benchmarking methodology, analyze the performance through latency-throughput curves, and investigate scalability via load balancing. Our evaluations demonstrate substantial improvements in throughput and latency across various models and hardware configurations. We discuss our roadmap for future enhancements, including broader model support and new hardware backends. The DeepSpeed-FastGen code is readily available for community engagement and contribution.\n",
            "\n",
            "**Keywords:** Language models, Text generation, High-throughput, Low-latency, DeepSpeed-FastGen\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08671)\n",
            "\n",
            "\n",
            "# UniVG: Towards UNIfied-modal Video Generation\n",
            "\n",
            "**Abstract:** Diffusion based video generation has received extensive attention and achieved considerable success within both the academic and industrial communities. However, current efforts are mainly concentrated on single-objective or single-task video generation, such as generation driven by text, by image, or by a combination of text and image. This cannot fully meet the needs of real-world application scenarios, as users are likely to input images and text conditions in a flexible manner, either individually or in combination. To address this, we propose a Unified-modal Video Generation system that is capable of handling multiple video generation tasks across text and image modalities. To this end, we revisit the various video generation tasks within our system from the perspective of generative freedom, and classify them into high-freedom and low-freedom video generation categories. For high-freedom video generation, we employ Multi-condition Cross Attention to generate videos that align with the semantics of the input images or text. For low-freedom video generation, we introduce Biased Gaussian Noise to replace the pure random Gaussian Noise, which helps to better preserve the content of the input conditions. Our method achieves the lowest Fréchet Video Distance (FVD) on the public academic benchmark MSR-VTT, surpasses the current open-source methods in human evaluations, and is on par with the current close-source method Gen2. For more samples, visit [here](https://univg-baidu.github.io).\n",
            "\n",
            "**Keywords:** video generation, unified-modal, generative freedom, multi-condition cross attention, biased Gaussian noise\n",
            "\n",
            "**arXiv URL:** [https://arxiv.org/abs/2401.09084](https://arxiv.org/abs/2401.09084)\n",
            "\n",
            "\n",
            "# Asynchronous Local-SGD Training for Language Modeling\n",
            "\n",
            "**Abstract:** Local stochastic gradient descent (Local-SGD), also referred to as federated averaging, is an approach to distributed optimization where each device performs more than one SGD update per communication. This work presents an empirical study of *asynchronous* Local-SGD for training language models; that is, each worker updates the global parameters as soon as it has finished its SGD steps. We conduct a comprehensive investigation by examining how worker hardware heterogeneity, model size, number of workers, and optimizer could impact the learning performance. We find that with naive implementations, asynchronous Local-SGD takes more iterations to converge than its synchronous counterpart despite updating the (global) model parameters more frequently. We identify momentum acceleration on the global parameters when worker gradients are stale as a key challenge. We propose a novel method that utilizes a delayed Nesterov momentum update and adjusts the workers' local training steps based on their computation speed. This approach, evaluated with models up to 150M parameters on the C4 dataset, matches the performance of synchronous Local-SGD in terms of perplexity per update step, and significantly surpasses it in terms of wall clock time.\n",
            "\n",
            "**Keywords:** Local-SGD, asynchronous training, language modeling, distributed optimization, Nesterov momentum\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.09135) <class 'str'>\n",
            "# VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models\n",
            "\n",
            "**Abstract:** Text-to-video generation aims to produce a video based on a given prompt. Recently, several commercial video models have been able to generate plausible videos with minimal noise, excellent details, and high aesthetic scores. However, these models rely on large-scale, well-filtered, high-quality videos that are not accessible to the community. Many existing research works, which train models using the low-quality WebVid-10M dataset, struggle to generate high-quality videos because the models are optimized to fit WebVid-10M. In this work, we explore the training scheme of video models extended from Stable Diffusion and investigate the feasibility of leveraging low-quality videos and synthesized high-quality images to obtain a high-quality video model. We first analyze the connection between the spatial and temporal modules of video models and the distribution shift to low-quality videos. We observe that full training of all modules results in a stronger coupling between spatial and temporal modules than only training temporal modules. Based on this stronger coupling, we shift the distribution to higher quality without motion degradation by finetuning spatial modules with high-quality images, resulting in a generic high-quality video model. Evaluations are conducted to demonstrate the superiority of the proposed method, particularly in picture quality, motion, and concept composition.\n",
            "\n",
            "**Keywords:** Text-to-video"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-18 06:18:27.166 | INFO     | __main__:_act:32 - # VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models\n",
            "\n",
            "**Abstract:** Text-to-video generation aims to produce a video based on a given prompt. Recently, several commercial video models have been able to generate plausible videos with minimal noise, excellent details, and high aesthetic scores. However, these models rely on large-scale, well-filtered, high-quality videos that are not accessible to the community. Many existing research works, which train models using the low-quality WebVid-10M dataset, struggle to generate high-quality videos because the models are optimized to fit WebVid-10M. In this work, we explore the training scheme of video models extended from Stable Diffusion and investigate the feasibility of leveraging low-quality videos and synthesized high-quality images to obtain a high-quality video model. We first analyze the connection between the spatial and temporal modules of video models and the distribution shift to low-quality videos. We observe that full training of all modules results in a stronger coupling between spatial and temporal modules than only training temporal modules. Based on this stronger coupling, we shift the distribution to higher quality without motion degradation by finetuning spatial modules with high-quality images, resulting in a generic high-quality video model. Evaluations are conducted to demonstrate the superiority of the proposed method, particularly in picture quality, motion, and concept composition.\n",
            "\n",
            "**Keywords:** Text-to-video generation, video diffusion models, high-quality videos, low-quality videos, synthesized high-quality images\n",
            "\n",
            "[arXiv link](https://arxiv.org/abs/2401.09047)\n",
            "2024-01-18 06:18:27.168 | INFO     | __main__:_act:21 - Huggy(DailyPaperWatcher): ready to SummaryDailyPaper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " generation, video diffusion models, high-quality videos, low-quality videos, synthesized high-quality images\n",
            "\n",
            "[arXiv link](https://arxiv.org/abs/2401.09047)\n",
            "# Huggingface Daily Paper: 2024-01-18\n",
            "\n",
            "\n",
            "# Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model\n",
            "\n",
            "**Abstract:** Recently, state space models (SSMs) with efficient hardware-aware designs, known as Mamba, have shown great potential for long sequence modeling. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, the authors propose a new generic vision backbone called Vim, which combines bidirectional Mamba blocks with position embeddings to compress visual representations. Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation and memory efficiency. For example, Vim is 2.8 times faster than DeiT and saves 86.8% GPU memory when performing batch inference on high-resolution images. The results show that Vim overcomes computation and memory constraints and has the potential to become the next-generation backbone for vision foundation models.\n",
            "\n",
            "**Keywords:** Vision backbone, state space models, visual representation learning, bidirectional Mamba blocks, computation efficiency\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.09417)\n",
            "\n",
            "\n",
            "# DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference\n",
            "\n",
            "**Abstract:** The deployment and scaling of large language models (LLMs) have become critical as they permeate various applications, demanding high-throughput and low-latency serving systems. Existing frameworks struggle to balance these requirements, especially for workloads with long prompts. This paper introduces DeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and generation composition strategy, to deliver up to 2.3x higher effective throughput, 2x lower latency on average, and up to 3.7x lower (token-level) tail latency, compared to state-of-the-art systems like vLLM. We leverage a synergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an efficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced implementation supports a range of models and offers both non-persistent and persistent deployment options, catering to diverse user scenarios from interactive sessions to long-running applications. We present a detailed benchmarking methodology, analyze the performance through latency-throughput curves, and investigate scalability via load balancing. Our evaluations demonstrate substantial improvements in throughput and latency across various models and hardware configurations. We discuss our roadmap for future enhancements, including broader model support and new hardware backends. The DeepSpeed-FastGen code is readily available for community engagement and contribution.\n",
            "\n",
            "**Keywords:** Language models, Text generation, High-throughput, Low-latency, DeepSpeed-FastGen\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08671)\n",
            "\n",
            "\n",
            "# UniVG: Towards UNIfied-modal Video Generation\n",
            "\n",
            "**Abstract:** Diffusion based video generation has received extensive attention and achieved considerable success within both the academic and industrial communities. However, current efforts are mainly concentrated on single-objective or single-task video generation, such as generation driven by text, by image, or by a combination of text and image. This cannot fully meet the needs of real-world application scenarios, as users are likely to input images and text conditions in a flexible manner, either individually or in combination. To address this, we propose a Unified-modal Video Generation system that is capable of handling multiple video generation tasks across text and image modalities. To this end, we revisit the various video generation tasks within our system from the perspective of generative freedom, and classify them into high-freedom and low-freedom video generation categories. For high-freedom video generation, we employ Multi-condition Cross Attention to generate videos that align with the semantics of the input images or text. For low-freedom video generation, we introduce Biased Gaussian Noise to replace the pure random Gaussian Noise, which helps to better preserve the content of the input conditions. Our method achieves the lowest Fréchet Video Distance (FVD) on the public academic benchmark MSR-VTT, surpasses the current open-source methods in human evaluations, and is on par with the current close-source method Gen2. For more samples, visit [here](https://univg-baidu.github.io).\n",
            "\n",
            "**Keywords:** video generation, unified-modal, generative freedom, multi-condition cross attention, biased Gaussian noise\n",
            "\n",
            "**arXiv URL:** [https://arxiv.org/abs/2401.09084](https://arxiv.org/abs/2401.09084)\n",
            "\n",
            "\n",
            "# Asynchronous Local-SGD Training for Language Modeling\n",
            "\n",
            "**Abstract:** Local stochastic gradient descent (Local-SGD), also referred to as federated averaging, is an approach to distributed optimization where each device performs more than one SGD update per communication. This work presents an empirical study of *asynchronous* Local-SGD for training language models; that is, each worker updates the global parameters as soon as it has finished its SGD steps. We conduct a comprehensive investigation by examining how worker hardware heterogeneity, model size, number of workers, and optimizer could impact the learning performance. We find that with naive implementations, asynchronous Local-SGD takes more iterations to converge than its synchronous counterpart despite updating the (global) model parameters more frequently. We identify momentum acceleration on the global parameters when worker gradients are stale as a key challenge. We propose a novel method that utilizes a delayed Nesterov momentum update and adjusts the workers' local training steps based on their computation speed. This approach, evaluated with models up to 150M parameters on the C4 dataset, matches the performance of synchronous Local-SGD in terms of perplexity per update step, and significantly surpasses it in terms of wall clock time.\n",
            "\n",
            "**Keywords:** Local-SGD, asynchronous training, language modeling, distributed optimization, Nesterov momentum\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.09135)\n",
            "\n",
            "\n",
            "# ReFT: Reasoning with Reinforced Fine-Tuning\n",
            "\n",
            "**Abstract:** One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question and the rewards are naturally derived from the ground-truth answers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that ReFT significantly outperforms SFT, and the performance can be potentially further boosted by combining inference-time strategies such as majority voting and re-ranking. Note that ReFT obtains the improvement by learning from the same training questions as SFT, without relying on extra or augmented training questions. This indicates a superior generalization ability for ReFT.\n",
            "\n",
            "**Keywords:** Large Language Models, Reinforced Fine-Tuning, Reasoning, Chain-of-Thought, Math problem-solving\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08967) <class 'str'>\n",
            "# SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers\n",
            "\n",
            "**Abstract:** \n",
            "In this research paper, the authors introduce Scalable Interpolant Transformers (SiT), a family of generative models that are built on the foundation of Diffusion Transformers (DiT). SiT utilizes the interpolant framework, which provides a more flexible way of connecting two distributions compared to standard diffusion models. This framework allows for a modular study of different design choices that impact generative models built on dynamical transport. These design choices include discrete vs. continuous time learning, the objective for the model to learn, the choice of interpolant connecting the distributions, and the deployment of a deterministic or stochastic sampler. By carefully incorporating these elements, SiT outperforms DiT consistently across various model sizes on the conditional ImageNet 256x256 benchmark, while maintaining the same backbone, number of parameters, and GFLOPs. Additionally, SiT achieves an impressive FID-50K score of 2.06 by exploring different diffusion coefficients, which can be independently tuned from the learning process.\n",
            "\n",
            "**Keywords:** Generative models, Diff"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-18 06:18:31.083 | INFO     | __main__:_act:32 - # SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers\n",
            "\n",
            "**Abstract:** \n",
            "In this research paper, the authors introduce Scalable Interpolant Transformers (SiT), a family of generative models that are built on the foundation of Diffusion Transformers (DiT). SiT utilizes the interpolant framework, which provides a more flexible way of connecting two distributions compared to standard diffusion models. This framework allows for a modular study of different design choices that impact generative models built on dynamical transport. These design choices include discrete vs. continuous time learning, the objective for the model to learn, the choice of interpolant connecting the distributions, and the deployment of a deterministic or stochastic sampler. By carefully incorporating these elements, SiT outperforms DiT consistently across various model sizes on the conditional ImageNet 256x256 benchmark, while maintaining the same backbone, number of parameters, and GFLOPs. Additionally, SiT achieves an impressive FID-50K score of 2.06 by exploring different diffusion coefficients, which can be independently tuned from the learning process.\n",
            "\n",
            "**Keywords:** Generative models, Diffusion Transformers, Interpolant framework, Dynamical transport, Conditional ImageNet\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08740)\n",
            "2024-01-18 06:18:31.085 | INFO     | __main__:_act:21 - Huggy(DailyPaperWatcher): ready to SummaryDailyPaper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usion Transformers, Interpolant framework, Dynamical transport, Conditional ImageNet\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08740)\n",
            "# Huggingface Daily Paper: 2024-01-18\n",
            "\n",
            "\n",
            "# Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model\n",
            "\n",
            "**Abstract:** Recently, state space models (SSMs) with efficient hardware-aware designs, known as Mamba, have shown great potential for long sequence modeling. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, the authors propose a new generic vision backbone called Vim, which combines bidirectional Mamba blocks with position embeddings to compress visual representations. Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation and memory efficiency. For example, Vim is 2.8 times faster than DeiT and saves 86.8% GPU memory when performing batch inference on high-resolution images. The results show that Vim overcomes computation and memory constraints and has the potential to become the next-generation backbone for vision foundation models.\n",
            "\n",
            "**Keywords:** Vision backbone, state space models, visual representation learning, bidirectional Mamba blocks, computation efficiency\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.09417)\n",
            "\n",
            "\n",
            "# DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference\n",
            "\n",
            "**Abstract:** The deployment and scaling of large language models (LLMs) have become critical as they permeate various applications, demanding high-throughput and low-latency serving systems. Existing frameworks struggle to balance these requirements, especially for workloads with long prompts. This paper introduces DeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and generation composition strategy, to deliver up to 2.3x higher effective throughput, 2x lower latency on average, and up to 3.7x lower (token-level) tail latency, compared to state-of-the-art systems like vLLM. We leverage a synergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an efficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced implementation supports a range of models and offers both non-persistent and persistent deployment options, catering to diverse user scenarios from interactive sessions to long-running applications. We present a detailed benchmarking methodology, analyze the performance through latency-throughput curves, and investigate scalability via load balancing. Our evaluations demonstrate substantial improvements in throughput and latency across various models and hardware configurations. We discuss our roadmap for future enhancements, including broader model support and new hardware backends. The DeepSpeed-FastGen code is readily available for community engagement and contribution.\n",
            "\n",
            "**Keywords:** Language models, Text generation, High-throughput, Low-latency, DeepSpeed-FastGen\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08671)\n",
            "\n",
            "\n",
            "# UniVG: Towards UNIfied-modal Video Generation\n",
            "\n",
            "**Abstract:** Diffusion based video generation has received extensive attention and achieved considerable success within both the academic and industrial communities. However, current efforts are mainly concentrated on single-objective or single-task video generation, such as generation driven by text, by image, or by a combination of text and image. This cannot fully meet the needs of real-world application scenarios, as users are likely to input images and text conditions in a flexible manner, either individually or in combination. To address this, we propose a Unified-modal Video Generation system that is capable of handling multiple video generation tasks across text and image modalities. To this end, we revisit the various video generation tasks within our system from the perspective of generative freedom, and classify them into high-freedom and low-freedom video generation categories. For high-freedom video generation, we employ Multi-condition Cross Attention to generate videos that align with the semantics of the input images or text. For low-freedom video generation, we introduce Biased Gaussian Noise to replace the pure random Gaussian Noise, which helps to better preserve the content of the input conditions. Our method achieves the lowest Fréchet Video Distance (FVD) on the public academic benchmark MSR-VTT, surpasses the current open-source methods in human evaluations, and is on par with the current close-source method Gen2. For more samples, visit [here](https://univg-baidu.github.io).\n",
            "\n",
            "**Keywords:** video generation, unified-modal, generative freedom, multi-condition cross attention, biased Gaussian noise\n",
            "\n",
            "**arXiv URL:** [https://arxiv.org/abs/2401.09084](https://arxiv.org/abs/2401.09084)\n",
            "\n",
            "\n",
            "# Asynchronous Local-SGD Training for Language Modeling\n",
            "\n",
            "**Abstract:** Local stochastic gradient descent (Local-SGD), also referred to as federated averaging, is an approach to distributed optimization where each device performs more than one SGD update per communication. This work presents an empirical study of *asynchronous* Local-SGD for training language models; that is, each worker updates the global parameters as soon as it has finished its SGD steps. We conduct a comprehensive investigation by examining how worker hardware heterogeneity, model size, number of workers, and optimizer could impact the learning performance. We find that with naive implementations, asynchronous Local-SGD takes more iterations to converge than its synchronous counterpart despite updating the (global) model parameters more frequently. We identify momentum acceleration on the global parameters when worker gradients are stale as a key challenge. We propose a novel method that utilizes a delayed Nesterov momentum update and adjusts the workers' local training steps based on their computation speed. This approach, evaluated with models up to 150M parameters on the C4 dataset, matches the performance of synchronous Local-SGD in terms of perplexity per update step, and significantly surpasses it in terms of wall clock time.\n",
            "\n",
            "**Keywords:** Local-SGD, asynchronous training, language modeling, distributed optimization, Nesterov momentum\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.09135)\n",
            "\n",
            "\n",
            "# ReFT: Reasoning with Reinforced Fine-Tuning\n",
            "\n",
            "**Abstract:** One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question and the rewards are naturally derived from the ground-truth answers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that ReFT significantly outperforms SFT, and the performance can be potentially further boosted by combining inference-time strategies such as majority voting and re-ranking. Note that ReFT obtains the improvement by learning from the same training questions as SFT, without relying on extra or augmented training questions. This indicates a superior generalization ability for ReFT.\n",
            "\n",
            "**Keywords:** Large Language Models, Reinforced Fine-Tuning, Reasoning, Chain-of-Thought, Math problem-solving\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08967)\n",
            "\n",
            "\n",
            "# VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models\n",
            "\n",
            "**Abstract:** Text-to-video generation aims to produce a video based on a given prompt. Recently, several commercial video models have been able to generate plausible videos with minimal noise, excellent details, and high aesthetic scores. However, these models rely on large-scale, well-filtered, high-quality videos that are not accessible to the community. Many existing research works, which train models using the low-quality WebVid-10M dataset, struggle to generate high-quality videos because the models are optimized to fit WebVid-10M. In this work, we explore the training scheme of video models extended from Stable Diffusion and investigate the feasibility of leveraging low-quality videos and synthesized high-quality images to obtain a high-quality video model. We first analyze the connection between the spatial and temporal modules of video models and the distribution shift to low-quality videos. We observe that full training of all modules results in a stronger coupling between spatial and temporal modules than only training temporal modules. Based on this stronger coupling, we shift the distribution to higher quality without motion degradation by finetuning spatial modules with high-quality images, resulting in a generic high-quality video model. Evaluations are conducted to demonstrate the superiority of the proposed method, particularly in picture quality, motion, and concept composition.\n",
            "\n",
            "**Keywords:** Text-to-video generation, video diffusion models, high-quality videos, low-quality videos, synthesized high-quality images\n",
            "\n",
            "[arXiv link](https://arxiv.org/abs/2401.09047) <class 'str'>\n",
            "# GARField: Group Anything with Radiance Fields\n",
            "\n",
            "**Abstract:** Grouping is inherently ambiguous due to the multiple levels of granularity in which one can decompose a scene -- should the wheels of an excavator be considered separate or part of the whole? We present Group Anything with Radiance Fields (GARField), an approach for decomposing 3D scenes into a hierarchy of semantically meaningful groups from posed image inputs. To do this we embrace group ambiguity through physical scale: by optimizing a scale-conditioned 3D affinity feature field, a point in the world can belong to different groups of different sizes. We optimize this field from a set of 2D masks provided by Segment Anything (SAM) in a way that respects coarse-to-fine hierarchy, using scale to consistently fuse conflicting masks from different viewpoints. From this field we can derive a hierarchy of possible groupings via automatic tree construction or user interaction. We evaluate GARField on a variety of in-the-wild scenes and find it effectively extracts groups at many levels: clusters of objects, objects, and various subparts. GARField inherently represents multi-view consistent groupings and produces higher fidelity groups than the input SAM masks. GARField's hierarchical grouping could have exciting downstream applications such as 3D asset extraction or dynamic scene understanding. See the project website at [https://www.garfield.studio/](https://www.garfield.studio/)\n",
            "\n",
            "**Keywords:** Grouping, 3D scenes, Radiance Fields, Semantically"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-18 06:18:36.790 | INFO     | __main__:_act:32 - # GARField: Group Anything with Radiance Fields\n",
            "\n",
            "**Abstract:** Grouping is inherently ambiguous due to the multiple levels of granularity in which one can decompose a scene -- should the wheels of an excavator be considered separate or part of the whole? We present Group Anything with Radiance Fields (GARField), an approach for decomposing 3D scenes into a hierarchy of semantically meaningful groups from posed image inputs. To do this we embrace group ambiguity through physical scale: by optimizing a scale-conditioned 3D affinity feature field, a point in the world can belong to different groups of different sizes. We optimize this field from a set of 2D masks provided by Segment Anything (SAM) in a way that respects coarse-to-fine hierarchy, using scale to consistently fuse conflicting masks from different viewpoints. From this field we can derive a hierarchy of possible groupings via automatic tree construction or user interaction. We evaluate GARField on a variety of in-the-wild scenes and find it effectively extracts groups at many levels: clusters of objects, objects, and various subparts. GARField inherently represents multi-view consistent groupings and produces higher fidelity groups than the input SAM masks. GARField's hierarchical grouping could have exciting downstream applications such as 3D asset extraction or dynamic scene understanding. See the project website at [https://www.garfield.studio/](https://www.garfield.studio/)\n",
            "\n",
            "**Keywords:** Grouping, 3D scenes, Radiance Fields, Semantically meaningful groups, Scale-conditioned 3D affinity feature field.\n",
            "2024-01-18 06:18:36.791 | INFO     | __main__:_act:21 - Huggy(DailyPaperWatcher): ready to SummaryDailyPaper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " meaningful groups, Scale-conditioned 3D affinity feature field.\n",
            "# Huggingface Daily Paper: 2024-01-18\n",
            "\n",
            "\n",
            "# Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model\n",
            "\n",
            "**Abstract:** Recently, state space models (SSMs) with efficient hardware-aware designs, known as Mamba, have shown great potential for long sequence modeling. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, the authors propose a new generic vision backbone called Vim, which combines bidirectional Mamba blocks with position embeddings to compress visual representations. Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation and memory efficiency. For example, Vim is 2.8 times faster than DeiT and saves 86.8% GPU memory when performing batch inference on high-resolution images. The results show that Vim overcomes computation and memory constraints and has the potential to become the next-generation backbone for vision foundation models.\n",
            "\n",
            "**Keywords:** Vision backbone, state space models, visual representation learning, bidirectional Mamba blocks, computation efficiency\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.09417)\n",
            "\n",
            "\n",
            "# DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference\n",
            "\n",
            "**Abstract:** The deployment and scaling of large language models (LLMs) have become critical as they permeate various applications, demanding high-throughput and low-latency serving systems. Existing frameworks struggle to balance these requirements, especially for workloads with long prompts. This paper introduces DeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and generation composition strategy, to deliver up to 2.3x higher effective throughput, 2x lower latency on average, and up to 3.7x lower (token-level) tail latency, compared to state-of-the-art systems like vLLM. We leverage a synergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an efficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced implementation supports a range of models and offers both non-persistent and persistent deployment options, catering to diverse user scenarios from interactive sessions to long-running applications. We present a detailed benchmarking methodology, analyze the performance through latency-throughput curves, and investigate scalability via load balancing. Our evaluations demonstrate substantial improvements in throughput and latency across various models and hardware configurations. We discuss our roadmap for future enhancements, including broader model support and new hardware backends. The DeepSpeed-FastGen code is readily available for community engagement and contribution.\n",
            "\n",
            "**Keywords:** Language models, Text generation, High-throughput, Low-latency, DeepSpeed-FastGen\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08671)\n",
            "\n",
            "\n",
            "# UniVG: Towards UNIfied-modal Video Generation\n",
            "\n",
            "**Abstract:** Diffusion based video generation has received extensive attention and achieved considerable success within both the academic and industrial communities. However, current efforts are mainly concentrated on single-objective or single-task video generation, such as generation driven by text, by image, or by a combination of text and image. This cannot fully meet the needs of real-world application scenarios, as users are likely to input images and text conditions in a flexible manner, either individually or in combination. To address this, we propose a Unified-modal Video Generation system that is capable of handling multiple video generation tasks across text and image modalities. To this end, we revisit the various video generation tasks within our system from the perspective of generative freedom, and classify them into high-freedom and low-freedom video generation categories. For high-freedom video generation, we employ Multi-condition Cross Attention to generate videos that align with the semantics of the input images or text. For low-freedom video generation, we introduce Biased Gaussian Noise to replace the pure random Gaussian Noise, which helps to better preserve the content of the input conditions. Our method achieves the lowest Fréchet Video Distance (FVD) on the public academic benchmark MSR-VTT, surpasses the current open-source methods in human evaluations, and is on par with the current close-source method Gen2. For more samples, visit [here](https://univg-baidu.github.io).\n",
            "\n",
            "**Keywords:** video generation, unified-modal, generative freedom, multi-condition cross attention, biased Gaussian noise\n",
            "\n",
            "**arXiv URL:** [https://arxiv.org/abs/2401.09084](https://arxiv.org/abs/2401.09084)\n",
            "\n",
            "\n",
            "# Asynchronous Local-SGD Training for Language Modeling\n",
            "\n",
            "**Abstract:** Local stochastic gradient descent (Local-SGD), also referred to as federated averaging, is an approach to distributed optimization where each device performs more than one SGD update per communication. This work presents an empirical study of *asynchronous* Local-SGD for training language models; that is, each worker updates the global parameters as soon as it has finished its SGD steps. We conduct a comprehensive investigation by examining how worker hardware heterogeneity, model size, number of workers, and optimizer could impact the learning performance. We find that with naive implementations, asynchronous Local-SGD takes more iterations to converge than its synchronous counterpart despite updating the (global) model parameters more frequently. We identify momentum acceleration on the global parameters when worker gradients are stale as a key challenge. We propose a novel method that utilizes a delayed Nesterov momentum update and adjusts the workers' local training steps based on their computation speed. This approach, evaluated with models up to 150M parameters on the C4 dataset, matches the performance of synchronous Local-SGD in terms of perplexity per update step, and significantly surpasses it in terms of wall clock time.\n",
            "\n",
            "**Keywords:** Local-SGD, asynchronous training, language modeling, distributed optimization, Nesterov momentum\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.09135)\n",
            "\n",
            "\n",
            "# ReFT: Reasoning with Reinforced Fine-Tuning\n",
            "\n",
            "**Abstract:** One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question and the rewards are naturally derived from the ground-truth answers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that ReFT significantly outperforms SFT, and the performance can be potentially further boosted by combining inference-time strategies such as majority voting and re-ranking. Note that ReFT obtains the improvement by learning from the same training questions as SFT, without relying on extra or augmented training questions. This indicates a superior generalization ability for ReFT.\n",
            "\n",
            "**Keywords:** Large Language Models, Reinforced Fine-Tuning, Reasoning, Chain-of-Thought, Math problem-solving\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08967)\n",
            "\n",
            "\n",
            "# VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models\n",
            "\n",
            "**Abstract:** Text-to-video generation aims to produce a video based on a given prompt. Recently, several commercial video models have been able to generate plausible videos with minimal noise, excellent details, and high aesthetic scores. However, these models rely on large-scale, well-filtered, high-quality videos that are not accessible to the community. Many existing research works, which train models using the low-quality WebVid-10M dataset, struggle to generate high-quality videos because the models are optimized to fit WebVid-10M. In this work, we explore the training scheme of video models extended from Stable Diffusion and investigate the feasibility of leveraging low-quality videos and synthesized high-quality images to obtain a high-quality video model. We first analyze the connection between the spatial and temporal modules of video models and the distribution shift to low-quality videos. We observe that full training of all modules results in a stronger coupling between spatial and temporal modules than only training temporal modules. Based on this stronger coupling, we shift the distribution to higher quality without motion degradation by finetuning spatial modules with high-quality images, resulting in a generic high-quality video model. Evaluations are conducted to demonstrate the superiority of the proposed method, particularly in picture quality, motion, and concept composition.\n",
            "\n",
            "**Keywords:** Text-to-video generation, video diffusion models, high-quality videos, low-quality videos, synthesized high-quality images\n",
            "\n",
            "[arXiv link](https://arxiv.org/abs/2401.09047)\n",
            "\n",
            "\n",
            "# SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers\n",
            "\n",
            "**Abstract:** \n",
            "In this research paper, the authors introduce Scalable Interpolant Transformers (SiT), a family of generative models that are built on the foundation of Diffusion Transformers (DiT). SiT utilizes the interpolant framework, which provides a more flexible way of connecting two distributions compared to standard diffusion models. This framework allows for a modular study of different design choices that impact generative models built on dynamical transport. These design choices include discrete vs. continuous time learning, the objective for the model to learn, the choice of interpolant connecting the distributions, and the deployment of a deterministic or stochastic sampler. By carefully incorporating these elements, SiT outperforms DiT consistently across various model sizes on the conditional ImageNet 256x256 benchmark, while maintaining the same backbone, number of parameters, and GFLOPs. Additionally, SiT achieves an impressive FID-50K score of 2.06 by exploring different diffusion coefficients, which can be independently tuned from the learning process.\n",
            "\n",
            "**Keywords:** Generative models, Diffusion Transformers, Interpolant framework, Dynamical transport, Conditional ImageNet\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08740) <class 'str'>\n",
            "# SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding\n",
            "\n",
            "**Abstract:** \n",
            "The paper focuses on the challenges of aligning language with the 3D physical environment, known as 3D vision-language grounding, in the development of embodied agents. It addresses the complexity of 3D scenes, the scarcity of paired 3D vision-language data, and the absence of a unified learning framework. The authors introduce SceneVerse, a million-scale 3D vision-language dataset, and propose a unified pre-training framework called Grounded Pre-training for Scenes (GPS). Through extensive experiments, they demonstrate the effectiveness of GPS in achieving state-of-the-art performance on existing 3D visual grounding benchmarks. The paper also presents zero-shot transfer experiments to showcase the potential of SceneVerse and GPS in challenging 3D vision-language tasks.\n",
            "\n",
            "**Keywords:** 3D vision-language grounding"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-18 06:18:39.443 | INFO     | __main__:_act:32 - # SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding\n",
            "\n",
            "**Abstract:** \n",
            "The paper focuses on the challenges of aligning language with the 3D physical environment, known as 3D vision-language grounding, in the development of embodied agents. It addresses the complexity of 3D scenes, the scarcity of paired 3D vision-language data, and the absence of a unified learning framework. The authors introduce SceneVerse, a million-scale 3D vision-language dataset, and propose a unified pre-training framework called Grounded Pre-training for Scenes (GPS). Through extensive experiments, they demonstrate the effectiveness of GPS in achieving state-of-the-art performance on existing 3D visual grounding benchmarks. The paper also presents zero-shot transfer experiments to showcase the potential of SceneVerse and GPS in challenging 3D vision-language tasks.\n",
            "\n",
            "**Keywords:** 3D vision-language grounding, embodied agents, 3D scenes, pre-training framework, visual grounding\n",
            "\n",
            "**arXiv URL:** [https://arxiv.org/abs/2401.09340](https://arxiv.org/abs/2401.09340)\n",
            "2024-01-18 06:18:39.444 | INFO     | __main__:_act:21 - Huggy(DailyPaperWatcher): ready to SummaryDailyPaper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ", embodied agents, 3D scenes, pre-training framework, visual grounding\n",
            "\n",
            "**arXiv URL:** [https://arxiv.org/abs/2401.09340](https://arxiv.org/abs/2401.09340)\n",
            "# Huggingface Daily Paper: 2024-01-18\n",
            "\n",
            "\n",
            "# Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model\n",
            "\n",
            "**Abstract:** Recently, state space models (SSMs) with efficient hardware-aware designs, known as Mamba, have shown great potential for long sequence modeling. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, the authors propose a new generic vision backbone called Vim, which combines bidirectional Mamba blocks with position embeddings to compress visual representations. Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation and memory efficiency. For example, Vim is 2.8 times faster than DeiT and saves 86.8% GPU memory when performing batch inference on high-resolution images. The results show that Vim overcomes computation and memory constraints and has the potential to become the next-generation backbone for vision foundation models.\n",
            "\n",
            "**Keywords:** Vision backbone, state space models, visual representation learning, bidirectional Mamba blocks, computation efficiency\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.09417)\n",
            "\n",
            "\n",
            "# DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference\n",
            "\n",
            "**Abstract:** The deployment and scaling of large language models (LLMs) have become critical as they permeate various applications, demanding high-throughput and low-latency serving systems. Existing frameworks struggle to balance these requirements, especially for workloads with long prompts. This paper introduces DeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and generation composition strategy, to deliver up to 2.3x higher effective throughput, 2x lower latency on average, and up to 3.7x lower (token-level) tail latency, compared to state-of-the-art systems like vLLM. We leverage a synergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an efficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced implementation supports a range of models and offers both non-persistent and persistent deployment options, catering to diverse user scenarios from interactive sessions to long-running applications. We present a detailed benchmarking methodology, analyze the performance through latency-throughput curves, and investigate scalability via load balancing. Our evaluations demonstrate substantial improvements in throughput and latency across various models and hardware configurations. We discuss our roadmap for future enhancements, including broader model support and new hardware backends. The DeepSpeed-FastGen code is readily available for community engagement and contribution.\n",
            "\n",
            "**Keywords:** Language models, Text generation, High-throughput, Low-latency, DeepSpeed-FastGen\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08671)\n",
            "\n",
            "\n",
            "# UniVG: Towards UNIfied-modal Video Generation\n",
            "\n",
            "**Abstract:** Diffusion based video generation has received extensive attention and achieved considerable success within both the academic and industrial communities. However, current efforts are mainly concentrated on single-objective or single-task video generation, such as generation driven by text, by image, or by a combination of text and image. This cannot fully meet the needs of real-world application scenarios, as users are likely to input images and text conditions in a flexible manner, either individually or in combination. To address this, we propose a Unified-modal Video Generation system that is capable of handling multiple video generation tasks across text and image modalities. To this end, we revisit the various video generation tasks within our system from the perspective of generative freedom, and classify them into high-freedom and low-freedom video generation categories. For high-freedom video generation, we employ Multi-condition Cross Attention to generate videos that align with the semantics of the input images or text. For low-freedom video generation, we introduce Biased Gaussian Noise to replace the pure random Gaussian Noise, which helps to better preserve the content of the input conditions. Our method achieves the lowest Fréchet Video Distance (FVD) on the public academic benchmark MSR-VTT, surpasses the current open-source methods in human evaluations, and is on par with the current close-source method Gen2. For more samples, visit [here](https://univg-baidu.github.io).\n",
            "\n",
            "**Keywords:** video generation, unified-modal, generative freedom, multi-condition cross attention, biased Gaussian noise\n",
            "\n",
            "**arXiv URL:** [https://arxiv.org/abs/2401.09084](https://arxiv.org/abs/2401.09084)\n",
            "\n",
            "\n",
            "# Asynchronous Local-SGD Training for Language Modeling\n",
            "\n",
            "**Abstract:** Local stochastic gradient descent (Local-SGD), also referred to as federated averaging, is an approach to distributed optimization where each device performs more than one SGD update per communication. This work presents an empirical study of *asynchronous* Local-SGD for training language models; that is, each worker updates the global parameters as soon as it has finished its SGD steps. We conduct a comprehensive investigation by examining how worker hardware heterogeneity, model size, number of workers, and optimizer could impact the learning performance. We find that with naive implementations, asynchronous Local-SGD takes more iterations to converge than its synchronous counterpart despite updating the (global) model parameters more frequently. We identify momentum acceleration on the global parameters when worker gradients are stale as a key challenge. We propose a novel method that utilizes a delayed Nesterov momentum update and adjusts the workers' local training steps based on their computation speed. This approach, evaluated with models up to 150M parameters on the C4 dataset, matches the performance of synchronous Local-SGD in terms of perplexity per update step, and significantly surpasses it in terms of wall clock time.\n",
            "\n",
            "**Keywords:** Local-SGD, asynchronous training, language modeling, distributed optimization, Nesterov momentum\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.09135)\n",
            "\n",
            "\n",
            "# ReFT: Reasoning with Reinforced Fine-Tuning\n",
            "\n",
            "**Abstract:** One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question and the rewards are naturally derived from the ground-truth answers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that ReFT significantly outperforms SFT, and the performance can be potentially further boosted by combining inference-time strategies such as majority voting and re-ranking. Note that ReFT obtains the improvement by learning from the same training questions as SFT, without relying on extra or augmented training questions. This indicates a superior generalization ability for ReFT.\n",
            "\n",
            "**Keywords:** Large Language Models, Reinforced Fine-Tuning, Reasoning, Chain-of-Thought, Math problem-solving\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08967)\n",
            "\n",
            "\n",
            "# VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models\n",
            "\n",
            "**Abstract:** Text-to-video generation aims to produce a video based on a given prompt. Recently, several commercial video models have been able to generate plausible videos with minimal noise, excellent details, and high aesthetic scores. However, these models rely on large-scale, well-filtered, high-quality videos that are not accessible to the community. Many existing research works, which train models using the low-quality WebVid-10M dataset, struggle to generate high-quality videos because the models are optimized to fit WebVid-10M. In this work, we explore the training scheme of video models extended from Stable Diffusion and investigate the feasibility of leveraging low-quality videos and synthesized high-quality images to obtain a high-quality video model. We first analyze the connection between the spatial and temporal modules of video models and the distribution shift to low-quality videos. We observe that full training of all modules results in a stronger coupling between spatial and temporal modules than only training temporal modules. Based on this stronger coupling, we shift the distribution to higher quality without motion degradation by finetuning spatial modules with high-quality images, resulting in a generic high-quality video model. Evaluations are conducted to demonstrate the superiority of the proposed method, particularly in picture quality, motion, and concept composition.\n",
            "\n",
            "**Keywords:** Text-to-video generation, video diffusion models, high-quality videos, low-quality videos, synthesized high-quality images\n",
            "\n",
            "[arXiv link](https://arxiv.org/abs/2401.09047)\n",
            "\n",
            "\n",
            "# SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers\n",
            "\n",
            "**Abstract:** \n",
            "In this research paper, the authors introduce Scalable Interpolant Transformers (SiT), a family of generative models that are built on the foundation of Diffusion Transformers (DiT). SiT utilizes the interpolant framework, which provides a more flexible way of connecting two distributions compared to standard diffusion models. This framework allows for a modular study of different design choices that impact generative models built on dynamical transport. These design choices include discrete vs. continuous time learning, the objective for the model to learn, the choice of interpolant connecting the distributions, and the deployment of a deterministic or stochastic sampler. By carefully incorporating these elements, SiT outperforms DiT consistently across various model sizes on the conditional ImageNet 256x256 benchmark, while maintaining the same backbone, number of parameters, and GFLOPs. Additionally, SiT achieves an impressive FID-50K score of 2.06 by exploring different diffusion coefficients, which can be independently tuned from the learning process.\n",
            "\n",
            "**Keywords:** Generative models, Diffusion Transformers, Interpolant framework, Dynamical transport, Conditional ImageNet\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08740)\n",
            "\n",
            "\n",
            "# GARField: Group Anything with Radiance Fields\n",
            "\n",
            "**Abstract:** Grouping is inherently ambiguous due to the multiple levels of granularity in which one can decompose a scene -- should the wheels of an excavator be considered separate or part of the whole? We present Group Anything with Radiance Fields (GARField), an approach for decomposing 3D scenes into a hierarchy of semantically meaningful groups from posed image inputs. To do this we embrace group ambiguity through physical scale: by optimizing a scale-conditioned 3D affinity feature field, a point in the world can belong to different groups of different sizes. We optimize this field from a set of 2D masks provided by Segment Anything (SAM) in a way that respects coarse-to-fine hierarchy, using scale to consistently fuse conflicting masks from different viewpoints. From this field we can derive a hierarchy of possible groupings via automatic tree construction or user interaction. We evaluate GARField on a variety of in-the-wild scenes and find it effectively extracts groups at many levels: clusters of objects, objects, and various subparts. GARField inherently represents multi-view consistent groupings and produces higher fidelity groups than the input SAM masks. GARField's hierarchical grouping could have exciting downstream applications such as 3D asset extraction or dynamic scene understanding. See the project website at [https://www.garfield.studio/](https://www.garfield.studio/)\n",
            "\n",
            "**Keywords:** Grouping, 3D scenes, Radiance Fields, Semantically meaningful groups, Scale-conditioned 3D affinity feature field. <class 'str'>\n",
            "# Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis\n",
            "\n",
            "**Abstract:** Addressing the limitations of text as a source of accurate layout representation in text-conditional diffusion models, many works incorporate additional signals to condition certain attributes within a generated image. Although successful, previous works do not account for the specific localization of said attributes extended into the three-dimensional plane. In this context, we present a conditional diffusion model that integrates control over three-dimensional object placement with disentangled representations of global stylistic semantics from multiple exemplar images. Specifically, we first introduce depth disentanglement training to leverage the relative depth of objects as an estimator, allowing the model to identify the absolute positions of unseen objects through the use of synthetic image triplets. We also introduce soft guidance, a method for imposing global semantics onto targeted regions without the use of any additional localization cues. Our integrated framework, Compose and Conquer (CnC), unifies these techniques to localize multiple conditions in a disentangled manner. We demonstrate that our approach allows perception of objects at varying depths while offering a versatile framework for composing localized objects with different global semantics. \n",
            "\n",
            "**Keywords:** Composable Image Synthesis, Diffusion-Based Model, 3D Depth Awareness, Disentangled Representations, Global Stylistic Semantics\n",
            "\n",
            "[arXiv link](https://arxiv.org/abs/2401.09048)\n",
            "\n",
            "---\n",
            "\n",
            "In this research paper titled \"Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis,\" the authors address the limitations of using text as a source of accurate layout representation in text-conditional diffusion models. They propose a conditional diffusion model that integrates control over three-dimensional object placement and disentangled representations of global stylistic semantics from multiple exemplar images.\n",
            "\n",
            "The authors introduce depth disentanglement training, which leverages the relative depth of objects as an estimator. This allows the model to identify the absolute positions of unseen objects using synthetic image triplets. They also introduce soft guidance, a method for imposing global semantics onto targeted regions without the need for additional localization cues.\n",
            "\n",
            "The integrated framework, called Compose and Conquer (CnC), unifies these techniques to localize multiple conditions in a disentangled manner. The authors demonstrate that their approach enables the perception of objects at varying depths and offers a versatile framework for composing localized objects with different global semantics.\n",
            "\n",
            "Keywords that represent the core themes of this paper include Composable Image Synthesis, Diffusion-Based Model, 3D Depth Awareness, Disentangled Represent"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-18 06:18:47.712 | INFO     | __main__:_act:32 - # Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis\n",
            "\n",
            "**Abstract:** Addressing the limitations of text as a source of accurate layout representation in text-conditional diffusion models, many works incorporate additional signals to condition certain attributes within a generated image. Although successful, previous works do not account for the specific localization of said attributes extended into the three-dimensional plane. In this context, we present a conditional diffusion model that integrates control over three-dimensional object placement with disentangled representations of global stylistic semantics from multiple exemplar images. Specifically, we first introduce depth disentanglement training to leverage the relative depth of objects as an estimator, allowing the model to identify the absolute positions of unseen objects through the use of synthetic image triplets. We also introduce soft guidance, a method for imposing global semantics onto targeted regions without the use of any additional localization cues. Our integrated framework, Compose and Conquer (CnC), unifies these techniques to localize multiple conditions in a disentangled manner. We demonstrate that our approach allows perception of objects at varying depths while offering a versatile framework for composing localized objects with different global semantics. \n",
            "\n",
            "**Keywords:** Composable Image Synthesis, Diffusion-Based Model, 3D Depth Awareness, Disentangled Representations, Global Stylistic Semantics\n",
            "\n",
            "[arXiv link](https://arxiv.org/abs/2401.09048)\n",
            "\n",
            "---\n",
            "\n",
            "In this research paper titled \"Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis,\" the authors address the limitations of using text as a source of accurate layout representation in text-conditional diffusion models. They propose a conditional diffusion model that integrates control over three-dimensional object placement and disentangled representations of global stylistic semantics from multiple exemplar images.\n",
            "\n",
            "The authors introduce depth disentanglement training, which leverages the relative depth of objects as an estimator. This allows the model to identify the absolute positions of unseen objects using synthetic image triplets. They also introduce soft guidance, a method for imposing global semantics onto targeted regions without the need for additional localization cues.\n",
            "\n",
            "The integrated framework, called Compose and Conquer (CnC), unifies these techniques to localize multiple conditions in a disentangled manner. The authors demonstrate that their approach enables the perception of objects at varying depths and offers a versatile framework for composing localized objects with different global semantics.\n",
            "\n",
            "Keywords that represent the core themes of this paper include Composable Image Synthesis, Diffusion-Based Model, 3D Depth Awareness, Disentangled Representations, and Global Stylistic Semantics.\n",
            "\n",
            "For more details, you can refer to the [arXiv link](https://arxiv.org/abs/2401.09048).\n",
            "2024-01-18 06:18:47.720 | INFO     | __main__:_act:21 - Huggy(DailyPaperWatcher): ready to SummaryDailyPaper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ations, and Global Stylistic Semantics.\n",
            "\n",
            "For more details, you can refer to the [arXiv link](https://arxiv.org/abs/2401.09048).\n",
            "# Huggingface Daily Paper: 2024-01-18\n",
            "\n",
            "\n",
            "# Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model\n",
            "\n",
            "**Abstract:** Recently, state space models (SSMs) with efficient hardware-aware designs, known as Mamba, have shown great potential for long sequence modeling. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, the authors propose a new generic vision backbone called Vim, which combines bidirectional Mamba blocks with position embeddings to compress visual representations. Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation and memory efficiency. For example, Vim is 2.8 times faster than DeiT and saves 86.8% GPU memory when performing batch inference on high-resolution images. The results show that Vim overcomes computation and memory constraints and has the potential to become the next-generation backbone for vision foundation models.\n",
            "\n",
            "**Keywords:** Vision backbone, state space models, visual representation learning, bidirectional Mamba blocks, computation efficiency\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.09417)\n",
            "\n",
            "\n",
            "# DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference\n",
            "\n",
            "**Abstract:** The deployment and scaling of large language models (LLMs) have become critical as they permeate various applications, demanding high-throughput and low-latency serving systems. Existing frameworks struggle to balance these requirements, especially for workloads with long prompts. This paper introduces DeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and generation composition strategy, to deliver up to 2.3x higher effective throughput, 2x lower latency on average, and up to 3.7x lower (token-level) tail latency, compared to state-of-the-art systems like vLLM. We leverage a synergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an efficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced implementation supports a range of models and offers both non-persistent and persistent deployment options, catering to diverse user scenarios from interactive sessions to long-running applications. We present a detailed benchmarking methodology, analyze the performance through latency-throughput curves, and investigate scalability via load balancing. Our evaluations demonstrate substantial improvements in throughput and latency across various models and hardware configurations. We discuss our roadmap for future enhancements, including broader model support and new hardware backends. The DeepSpeed-FastGen code is readily available for community engagement and contribution.\n",
            "\n",
            "**Keywords:** Language models, Text generation, High-throughput, Low-latency, DeepSpeed-FastGen\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08671)\n",
            "\n",
            "\n",
            "# UniVG: Towards UNIfied-modal Video Generation\n",
            "\n",
            "**Abstract:** Diffusion based video generation has received extensive attention and achieved considerable success within both the academic and industrial communities. However, current efforts are mainly concentrated on single-objective or single-task video generation, such as generation driven by text, by image, or by a combination of text and image. This cannot fully meet the needs of real-world application scenarios, as users are likely to input images and text conditions in a flexible manner, either individually or in combination. To address this, we propose a Unified-modal Video Generation system that is capable of handling multiple video generation tasks across text and image modalities. To this end, we revisit the various video generation tasks within our system from the perspective of generative freedom, and classify them into high-freedom and low-freedom video generation categories. For high-freedom video generation, we employ Multi-condition Cross Attention to generate videos that align with the semantics of the input images or text. For low-freedom video generation, we introduce Biased Gaussian Noise to replace the pure random Gaussian Noise, which helps to better preserve the content of the input conditions. Our method achieves the lowest Fréchet Video Distance (FVD) on the public academic benchmark MSR-VTT, surpasses the current open-source methods in human evaluations, and is on par with the current close-source method Gen2. For more samples, visit [here](https://univg-baidu.github.io).\n",
            "\n",
            "**Keywords:** video generation, unified-modal, generative freedom, multi-condition cross attention, biased Gaussian noise\n",
            "\n",
            "**arXiv URL:** [https://arxiv.org/abs/2401.09084](https://arxiv.org/abs/2401.09084)\n",
            "\n",
            "\n",
            "# Asynchronous Local-SGD Training for Language Modeling\n",
            "\n",
            "**Abstract:** Local stochastic gradient descent (Local-SGD), also referred to as federated averaging, is an approach to distributed optimization where each device performs more than one SGD update per communication. This work presents an empirical study of *asynchronous* Local-SGD for training language models; that is, each worker updates the global parameters as soon as it has finished its SGD steps. We conduct a comprehensive investigation by examining how worker hardware heterogeneity, model size, number of workers, and optimizer could impact the learning performance. We find that with naive implementations, asynchronous Local-SGD takes more iterations to converge than its synchronous counterpart despite updating the (global) model parameters more frequently. We identify momentum acceleration on the global parameters when worker gradients are stale as a key challenge. We propose a novel method that utilizes a delayed Nesterov momentum update and adjusts the workers' local training steps based on their computation speed. This approach, evaluated with models up to 150M parameters on the C4 dataset, matches the performance of synchronous Local-SGD in terms of perplexity per update step, and significantly surpasses it in terms of wall clock time.\n",
            "\n",
            "**Keywords:** Local-SGD, asynchronous training, language modeling, distributed optimization, Nesterov momentum\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.09135)\n",
            "\n",
            "\n",
            "# ReFT: Reasoning with Reinforced Fine-Tuning\n",
            "\n",
            "**Abstract:** One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question and the rewards are naturally derived from the ground-truth answers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that ReFT significantly outperforms SFT, and the performance can be potentially further boosted by combining inference-time strategies such as majority voting and re-ranking. Note that ReFT obtains the improvement by learning from the same training questions as SFT, without relying on extra or augmented training questions. This indicates a superior generalization ability for ReFT.\n",
            "\n",
            "**Keywords:** Large Language Models, Reinforced Fine-Tuning, Reasoning, Chain-of-Thought, Math problem-solving\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08967)\n",
            "\n",
            "\n",
            "# VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models\n",
            "\n",
            "**Abstract:** Text-to-video generation aims to produce a video based on a given prompt. Recently, several commercial video models have been able to generate plausible videos with minimal noise, excellent details, and high aesthetic scores. However, these models rely on large-scale, well-filtered, high-quality videos that are not accessible to the community. Many existing research works, which train models using the low-quality WebVid-10M dataset, struggle to generate high-quality videos because the models are optimized to fit WebVid-10M. In this work, we explore the training scheme of video models extended from Stable Diffusion and investigate the feasibility of leveraging low-quality videos and synthesized high-quality images to obtain a high-quality video model. We first analyze the connection between the spatial and temporal modules of video models and the distribution shift to low-quality videos. We observe that full training of all modules results in a stronger coupling between spatial and temporal modules than only training temporal modules. Based on this stronger coupling, we shift the distribution to higher quality without motion degradation by finetuning spatial modules with high-quality images, resulting in a generic high-quality video model. Evaluations are conducted to demonstrate the superiority of the proposed method, particularly in picture quality, motion, and concept composition.\n",
            "\n",
            "**Keywords:** Text-to-video generation, video diffusion models, high-quality videos, low-quality videos, synthesized high-quality images\n",
            "\n",
            "[arXiv link](https://arxiv.org/abs/2401.09047)\n",
            "\n",
            "\n",
            "# SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers\n",
            "\n",
            "**Abstract:** \n",
            "In this research paper, the authors introduce Scalable Interpolant Transformers (SiT), a family of generative models that are built on the foundation of Diffusion Transformers (DiT). SiT utilizes the interpolant framework, which provides a more flexible way of connecting two distributions compared to standard diffusion models. This framework allows for a modular study of different design choices that impact generative models built on dynamical transport. These design choices include discrete vs. continuous time learning, the objective for the model to learn, the choice of interpolant connecting the distributions, and the deployment of a deterministic or stochastic sampler. By carefully incorporating these elements, SiT outperforms DiT consistently across various model sizes on the conditional ImageNet 256x256 benchmark, while maintaining the same backbone, number of parameters, and GFLOPs. Additionally, SiT achieves an impressive FID-50K score of 2.06 by exploring different diffusion coefficients, which can be independently tuned from the learning process.\n",
            "\n",
            "**Keywords:** Generative models, Diffusion Transformers, Interpolant framework, Dynamical transport, Conditional ImageNet\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08740)\n",
            "\n",
            "\n",
            "# GARField: Group Anything with Radiance Fields\n",
            "\n",
            "**Abstract:** Grouping is inherently ambiguous due to the multiple levels of granularity in which one can decompose a scene -- should the wheels of an excavator be considered separate or part of the whole? We present Group Anything with Radiance Fields (GARField), an approach for decomposing 3D scenes into a hierarchy of semantically meaningful groups from posed image inputs. To do this we embrace group ambiguity through physical scale: by optimizing a scale-conditioned 3D affinity feature field, a point in the world can belong to different groups of different sizes. We optimize this field from a set of 2D masks provided by Segment Anything (SAM) in a way that respects coarse-to-fine hierarchy, using scale to consistently fuse conflicting masks from different viewpoints. From this field we can derive a hierarchy of possible groupings via automatic tree construction or user interaction. We evaluate GARField on a variety of in-the-wild scenes and find it effectively extracts groups at many levels: clusters of objects, objects, and various subparts. GARField inherently represents multi-view consistent groupings and produces higher fidelity groups than the input SAM masks. GARField's hierarchical grouping could have exciting downstream applications such as 3D asset extraction or dynamic scene understanding. See the project website at [https://www.garfield.studio/](https://www.garfield.studio/)\n",
            "\n",
            "**Keywords:** Grouping, 3D scenes, Radiance Fields, Semantically meaningful groups, Scale-conditioned 3D affinity feature field.\n",
            "\n",
            "\n",
            "# SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding\n",
            "\n",
            "**Abstract:** \n",
            "The paper focuses on the challenges of aligning language with the 3D physical environment, known as 3D vision-language grounding, in the development of embodied agents. It addresses the complexity of 3D scenes, the scarcity of paired 3D vision-language data, and the absence of a unified learning framework. The authors introduce SceneVerse, a million-scale 3D vision-language dataset, and propose a unified pre-training framework called Grounded Pre-training for Scenes (GPS). Through extensive experiments, they demonstrate the effectiveness of GPS in achieving state-of-the-art performance on existing 3D visual grounding benchmarks. The paper also presents zero-shot transfer experiments to showcase the potential of SceneVerse and GPS in challenging 3D vision-language tasks.\n",
            "\n",
            "**Keywords:** 3D vision-language grounding, embodied agents, 3D scenes, pre-training framework, visual grounding\n",
            "\n",
            "**arXiv URL:** [https://arxiv.org/abs/2401.09340](https://arxiv.org/abs/2401.09340) <class 'str'>\n",
            "# ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization\n",
            "\n",
            "**Abstract:** Neural Radiance Fields (NeRF) exhibit remarkable performance for Novel View Synthesis (NVS) given a set of 2D images. However, NeRF training requires accurate camera pose for each input view, typically obtained by Structure-from-Motion (SfM) pipelines. Recent works have attempted to relax this constraint, but they still often rely on decent initial poses which they can refine. Here we aim at removing the requirement for pose initialization. We present Incremental CONfidence (ICON), an optimization procedure for training NeRFs from 2D video frames. ICON only assumes smooth camera motion to estimate initial guess for poses. Further, ICON introduces \"confidence\": an adaptive measure of model quality used to dynamically reweight gradients. ICON relies on high-confidence poses to learn NeRF, and high-confidence 3D structure (as encoded by NeRF) to learn poses. We show that ICON, without prior pose initialization, achieves superior performance in both CO3D and HO3D versus methods which use SfM pose.\n",
            "\n",
            "**Keywords:** Neural Radiance Fields, Novel View Synthesis, Pose Initialization, Incremental Confidence, Structure"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-18 06:18:51.319 | INFO     | __main__:_act:32 - # ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization\n",
            "\n",
            "**Abstract:** Neural Radiance Fields (NeRF) exhibit remarkable performance for Novel View Synthesis (NVS) given a set of 2D images. However, NeRF training requires accurate camera pose for each input view, typically obtained by Structure-from-Motion (SfM) pipelines. Recent works have attempted to relax this constraint, but they still often rely on decent initial poses which they can refine. Here we aim at removing the requirement for pose initialization. We present Incremental CONfidence (ICON), an optimization procedure for training NeRFs from 2D video frames. ICON only assumes smooth camera motion to estimate initial guess for poses. Further, ICON introduces \"confidence\": an adaptive measure of model quality used to dynamically reweight gradients. ICON relies on high-confidence poses to learn NeRF, and high-confidence 3D structure (as encoded by NeRF) to learn poses. We show that ICON, without prior pose initialization, achieves superior performance in both CO3D and HO3D versus methods which use SfM pose.\n",
            "\n",
            "**Keywords:** Neural Radiance Fields, Novel View Synthesis, Pose Initialization, Incremental Confidence, Structure-from-Motion\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08937)\n",
            "2024-01-18 06:18:51.320 | INFO     | __main__:_act:21 - Huggy(DailyPaperWatcher): ready to SummaryDailyPaper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-from-Motion\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08937)\n",
            "# Huggingface Daily Paper: 2024-01-18\n",
            "\n",
            "\n",
            "# Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model\n",
            "\n",
            "**Abstract:** Recently, state space models (SSMs) with efficient hardware-aware designs, known as Mamba, have shown great potential for long sequence modeling. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, the authors propose a new generic vision backbone called Vim, which combines bidirectional Mamba blocks with position embeddings to compress visual representations. Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation and memory efficiency. For example, Vim is 2.8 times faster than DeiT and saves 86.8% GPU memory when performing batch inference on high-resolution images. The results show that Vim overcomes computation and memory constraints and has the potential to become the next-generation backbone for vision foundation models.\n",
            "\n",
            "**Keywords:** Vision backbone, state space models, visual representation learning, bidirectional Mamba blocks, computation efficiency\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.09417)\n",
            "\n",
            "\n",
            "# DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference\n",
            "\n",
            "**Abstract:** The deployment and scaling of large language models (LLMs) have become critical as they permeate various applications, demanding high-throughput and low-latency serving systems. Existing frameworks struggle to balance these requirements, especially for workloads with long prompts. This paper introduces DeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and generation composition strategy, to deliver up to 2.3x higher effective throughput, 2x lower latency on average, and up to 3.7x lower (token-level) tail latency, compared to state-of-the-art systems like vLLM. We leverage a synergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an efficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced implementation supports a range of models and offers both non-persistent and persistent deployment options, catering to diverse user scenarios from interactive sessions to long-running applications. We present a detailed benchmarking methodology, analyze the performance through latency-throughput curves, and investigate scalability via load balancing. Our evaluations demonstrate substantial improvements in throughput and latency across various models and hardware configurations. We discuss our roadmap for future enhancements, including broader model support and new hardware backends. The DeepSpeed-FastGen code is readily available for community engagement and contribution.\n",
            "\n",
            "**Keywords:** Language models, Text generation, High-throughput, Low-latency, DeepSpeed-FastGen\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08671)\n",
            "\n",
            "\n",
            "# UniVG: Towards UNIfied-modal Video Generation\n",
            "\n",
            "**Abstract:** Diffusion based video generation has received extensive attention and achieved considerable success within both the academic and industrial communities. However, current efforts are mainly concentrated on single-objective or single-task video generation, such as generation driven by text, by image, or by a combination of text and image. This cannot fully meet the needs of real-world application scenarios, as users are likely to input images and text conditions in a flexible manner, either individually or in combination. To address this, we propose a Unified-modal Video Generation system that is capable of handling multiple video generation tasks across text and image modalities. To this end, we revisit the various video generation tasks within our system from the perspective of generative freedom, and classify them into high-freedom and low-freedom video generation categories. For high-freedom video generation, we employ Multi-condition Cross Attention to generate videos that align with the semantics of the input images or text. For low-freedom video generation, we introduce Biased Gaussian Noise to replace the pure random Gaussian Noise, which helps to better preserve the content of the input conditions. Our method achieves the lowest Fréchet Video Distance (FVD) on the public academic benchmark MSR-VTT, surpasses the current open-source methods in human evaluations, and is on par with the current close-source method Gen2. For more samples, visit [here](https://univg-baidu.github.io).\n",
            "\n",
            "**Keywords:** video generation, unified-modal, generative freedom, multi-condition cross attention, biased Gaussian noise\n",
            "\n",
            "**arXiv URL:** [https://arxiv.org/abs/2401.09084](https://arxiv.org/abs/2401.09084)\n",
            "\n",
            "\n",
            "# Asynchronous Local-SGD Training for Language Modeling\n",
            "\n",
            "**Abstract:** Local stochastic gradient descent (Local-SGD), also referred to as federated averaging, is an approach to distributed optimization where each device performs more than one SGD update per communication. This work presents an empirical study of *asynchronous* Local-SGD for training language models; that is, each worker updates the global parameters as soon as it has finished its SGD steps. We conduct a comprehensive investigation by examining how worker hardware heterogeneity, model size, number of workers, and optimizer could impact the learning performance. We find that with naive implementations, asynchronous Local-SGD takes more iterations to converge than its synchronous counterpart despite updating the (global) model parameters more frequently. We identify momentum acceleration on the global parameters when worker gradients are stale as a key challenge. We propose a novel method that utilizes a delayed Nesterov momentum update and adjusts the workers' local training steps based on their computation speed. This approach, evaluated with models up to 150M parameters on the C4 dataset, matches the performance of synchronous Local-SGD in terms of perplexity per update step, and significantly surpasses it in terms of wall clock time.\n",
            "\n",
            "**Keywords:** Local-SGD, asynchronous training, language modeling, distributed optimization, Nesterov momentum\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.09135)\n",
            "\n",
            "\n",
            "# ReFT: Reasoning with Reinforced Fine-Tuning\n",
            "\n",
            "**Abstract:** One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question and the rewards are naturally derived from the ground-truth answers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that ReFT significantly outperforms SFT, and the performance can be potentially further boosted by combining inference-time strategies such as majority voting and re-ranking. Note that ReFT obtains the improvement by learning from the same training questions as SFT, without relying on extra or augmented training questions. This indicates a superior generalization ability for ReFT.\n",
            "\n",
            "**Keywords:** Large Language Models, Reinforced Fine-Tuning, Reasoning, Chain-of-Thought, Math problem-solving\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08967)\n",
            "\n",
            "\n",
            "# VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models\n",
            "\n",
            "**Abstract:** Text-to-video generation aims to produce a video based on a given prompt. Recently, several commercial video models have been able to generate plausible videos with minimal noise, excellent details, and high aesthetic scores. However, these models rely on large-scale, well-filtered, high-quality videos that are not accessible to the community. Many existing research works, which train models using the low-quality WebVid-10M dataset, struggle to generate high-quality videos because the models are optimized to fit WebVid-10M. In this work, we explore the training scheme of video models extended from Stable Diffusion and investigate the feasibility of leveraging low-quality videos and synthesized high-quality images to obtain a high-quality video model. We first analyze the connection between the spatial and temporal modules of video models and the distribution shift to low-quality videos. We observe that full training of all modules results in a stronger coupling between spatial and temporal modules than only training temporal modules. Based on this stronger coupling, we shift the distribution to higher quality without motion degradation by finetuning spatial modules with high-quality images, resulting in a generic high-quality video model. Evaluations are conducted to demonstrate the superiority of the proposed method, particularly in picture quality, motion, and concept composition.\n",
            "\n",
            "**Keywords:** Text-to-video generation, video diffusion models, high-quality videos, low-quality videos, synthesized high-quality images\n",
            "\n",
            "[arXiv link](https://arxiv.org/abs/2401.09047)\n",
            "\n",
            "\n",
            "# SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers\n",
            "\n",
            "**Abstract:** \n",
            "In this research paper, the authors introduce Scalable Interpolant Transformers (SiT), a family of generative models that are built on the foundation of Diffusion Transformers (DiT). SiT utilizes the interpolant framework, which provides a more flexible way of connecting two distributions compared to standard diffusion models. This framework allows for a modular study of different design choices that impact generative models built on dynamical transport. These design choices include discrete vs. continuous time learning, the objective for the model to learn, the choice of interpolant connecting the distributions, and the deployment of a deterministic or stochastic sampler. By carefully incorporating these elements, SiT outperforms DiT consistently across various model sizes on the conditional ImageNet 256x256 benchmark, while maintaining the same backbone, number of parameters, and GFLOPs. Additionally, SiT achieves an impressive FID-50K score of 2.06 by exploring different diffusion coefficients, which can be independently tuned from the learning process.\n",
            "\n",
            "**Keywords:** Generative models, Diffusion Transformers, Interpolant framework, Dynamical transport, Conditional ImageNet\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08740)\n",
            "\n",
            "\n",
            "# GARField: Group Anything with Radiance Fields\n",
            "\n",
            "**Abstract:** Grouping is inherently ambiguous due to the multiple levels of granularity in which one can decompose a scene -- should the wheels of an excavator be considered separate or part of the whole? We present Group Anything with Radiance Fields (GARField), an approach for decomposing 3D scenes into a hierarchy of semantically meaningful groups from posed image inputs. To do this we embrace group ambiguity through physical scale: by optimizing a scale-conditioned 3D affinity feature field, a point in the world can belong to different groups of different sizes. We optimize this field from a set of 2D masks provided by Segment Anything (SAM) in a way that respects coarse-to-fine hierarchy, using scale to consistently fuse conflicting masks from different viewpoints. From this field we can derive a hierarchy of possible groupings via automatic tree construction or user interaction. We evaluate GARField on a variety of in-the-wild scenes and find it effectively extracts groups at many levels: clusters of objects, objects, and various subparts. GARField inherently represents multi-view consistent groupings and produces higher fidelity groups than the input SAM masks. GARField's hierarchical grouping could have exciting downstream applications such as 3D asset extraction or dynamic scene understanding. See the project website at [https://www.garfield.studio/](https://www.garfield.studio/)\n",
            "\n",
            "**Keywords:** Grouping, 3D scenes, Radiance Fields, Semantically meaningful groups, Scale-conditioned 3D affinity feature field.\n",
            "\n",
            "\n",
            "# SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding\n",
            "\n",
            "**Abstract:** \n",
            "The paper focuses on the challenges of aligning language with the 3D physical environment, known as 3D vision-language grounding, in the development of embodied agents. It addresses the complexity of 3D scenes, the scarcity of paired 3D vision-language data, and the absence of a unified learning framework. The authors introduce SceneVerse, a million-scale 3D vision-language dataset, and propose a unified pre-training framework called Grounded Pre-training for Scenes (GPS). Through extensive experiments, they demonstrate the effectiveness of GPS in achieving state-of-the-art performance on existing 3D visual grounding benchmarks. The paper also presents zero-shot transfer experiments to showcase the potential of SceneVerse and GPS in challenging 3D vision-language tasks.\n",
            "\n",
            "**Keywords:** 3D vision-language grounding, embodied agents, 3D scenes, pre-training framework, visual grounding\n",
            "\n",
            "**arXiv URL:** [https://arxiv.org/abs/2401.09340](https://arxiv.org/abs/2401.09340)\n",
            "\n",
            "\n",
            "# Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis\n",
            "\n",
            "**Abstract:** Addressing the limitations of text as a source of accurate layout representation in text-conditional diffusion models, many works incorporate additional signals to condition certain attributes within a generated image. Although successful, previous works do not account for the specific localization of said attributes extended into the three-dimensional plane. In this context, we present a conditional diffusion model that integrates control over three-dimensional object placement with disentangled representations of global stylistic semantics from multiple exemplar images. Specifically, we first introduce depth disentanglement training to leverage the relative depth of objects as an estimator, allowing the model to identify the absolute positions of unseen objects through the use of synthetic image triplets. We also introduce soft guidance, a method for imposing global semantics onto targeted regions without the use of any additional localization cues. Our integrated framework, Compose and Conquer (CnC), unifies these techniques to localize multiple conditions in a disentangled manner. We demonstrate that our approach allows perception of objects at varying depths while offering a versatile framework for composing localized objects with different global semantics. \n",
            "\n",
            "**Keywords:** Composable Image Synthesis, Diffusion-Based Model, 3D Depth Awareness, Disentangled Representations, Global Stylistic Semantics\n",
            "\n",
            "[arXiv link](https://arxiv.org/abs/2401.09048)\n",
            "\n",
            "---\n",
            "\n",
            "In this research paper titled \"Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis,\" the authors address the limitations of using text as a source of accurate layout representation in text-conditional diffusion models. They propose a conditional diffusion model that integrates control over three-dimensional object placement and disentangled representations of global stylistic semantics from multiple exemplar images.\n",
            "\n",
            "The authors introduce depth disentanglement training, which leverages the relative depth of objects as an estimator. This allows the model to identify the absolute positions of unseen objects using synthetic image triplets. They also introduce soft guidance, a method for imposing global semantics onto targeted regions without the need for additional localization cues.\n",
            "\n",
            "The integrated framework, called Compose and Conquer (CnC), unifies these techniques to localize multiple conditions in a disentangled manner. The authors demonstrate that their approach enables the perception of objects at varying depths and offers a versatile framework for composing localized objects with different global semantics.\n",
            "\n",
            "Keywords that represent the core themes of this paper include Composable Image Synthesis, Diffusion-Based Model, 3D Depth Awareness, Disentangled Representations, and Global Stylistic Semantics.\n",
            "\n",
            "For more details, you can refer to the [arXiv link](https://arxiv.org/abs/2401.09048). <class 'str'>\n",
            "# TextureDreamer: Image-guided Texture Synthesis through Geometry-aware Diffusion\n",
            "\n",
            "**Abstract:** \n",
            "The paper presents TextureDreamer, a novel image-guided texture synthesis method that can transfer relightable textures from a small number of input images (3 to 5) to target 3D shapes across arbitrary categories. Texture creation is a challenging task in vision and graphics, often requiring experienced artists to manually craft textures for 3D assets. Classical methods rely on densely sampled views and accurately aligned geometry, while learning-based methods are limited to category-specific shapes within the dataset. In contrast, TextureDreamer can transfer highly detailed, intricate textures from real-world environments to arbitrary objects with just a few casually captured images, potentially democratizing texture creation. The core idea behind TextureDreamer is personalized geometry-aware score distillation (PGSD), which combines personalized modeling for texture information extraction, variational score distillation for detailed appearance synthesis, and explicit geometry guidance with ControlNet. The integration of these techniques, along with several essential modifications, significantly improves the texture quality. Experimental results on real images from different categories demonstrate that TextureDreamer successfully transfers highly realistic and semantically meaningful textures to arbitrary objects, surpassing the visual quality of previous state-of-the-art methods.\n",
            "\n",
            "**Keywords:** Texture"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-18 06:18:57.269 | INFO     | __main__:_act:32 - # TextureDreamer: Image-guided Texture Synthesis through Geometry-aware Diffusion\n",
            "\n",
            "**Abstract:** \n",
            "The paper presents TextureDreamer, a novel image-guided texture synthesis method that can transfer relightable textures from a small number of input images (3 to 5) to target 3D shapes across arbitrary categories. Texture creation is a challenging task in vision and graphics, often requiring experienced artists to manually craft textures for 3D assets. Classical methods rely on densely sampled views and accurately aligned geometry, while learning-based methods are limited to category-specific shapes within the dataset. In contrast, TextureDreamer can transfer highly detailed, intricate textures from real-world environments to arbitrary objects with just a few casually captured images, potentially democratizing texture creation. The core idea behind TextureDreamer is personalized geometry-aware score distillation (PGSD), which combines personalized modeling for texture information extraction, variational score distillation for detailed appearance synthesis, and explicit geometry guidance with ControlNet. The integration of these techniques, along with several essential modifications, significantly improves the texture quality. Experimental results on real images from different categories demonstrate that TextureDreamer successfully transfers highly realistic and semantically meaningful textures to arbitrary objects, surpassing the visual quality of previous state-of-the-art methods.\n",
            "\n",
            "**Keywords:** Texture synthesis, Image-guided, Geometry-aware, Relightable textures, Personalized modeling.\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.09416)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " synthesis, Image-guided, Geometry-aware, Relightable textures, Personalized modeling.\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.09416)\n",
            "# Huggingface Daily Paper: 2024-01-18\n",
            "\n",
            "\n",
            "# Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model\n",
            "\n",
            "**Abstract:** Recently, state space models (SSMs) with efficient hardware-aware designs, known as Mamba, have shown great potential for long sequence modeling. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, the authors propose a new generic vision backbone called Vim, which combines bidirectional Mamba blocks with position embeddings to compress visual representations. Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation and memory efficiency. For example, Vim is 2.8 times faster than DeiT and saves 86.8% GPU memory when performing batch inference on high-resolution images. The results show that Vim overcomes computation and memory constraints and has the potential to become the next-generation backbone for vision foundation models.\n",
            "\n",
            "**Keywords:** Vision backbone, state space models, visual representation learning, bidirectional Mamba blocks, computation efficiency\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.09417)\n",
            "\n",
            "\n",
            "# DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference\n",
            "\n",
            "**Abstract:** The deployment and scaling of large language models (LLMs) have become critical as they permeate various applications, demanding high-throughput and low-latency serving systems. Existing frameworks struggle to balance these requirements, especially for workloads with long prompts. This paper introduces DeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and generation composition strategy, to deliver up to 2.3x higher effective throughput, 2x lower latency on average, and up to 3.7x lower (token-level) tail latency, compared to state-of-the-art systems like vLLM. We leverage a synergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an efficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced implementation supports a range of models and offers both non-persistent and persistent deployment options, catering to diverse user scenarios from interactive sessions to long-running applications. We present a detailed benchmarking methodology, analyze the performance through latency-throughput curves, and investigate scalability via load balancing. Our evaluations demonstrate substantial improvements in throughput and latency across various models and hardware configurations. We discuss our roadmap for future enhancements, including broader model support and new hardware backends. The DeepSpeed-FastGen code is readily available for community engagement and contribution.\n",
            "\n",
            "**Keywords:** Language models, Text generation, High-throughput, Low-latency, DeepSpeed-FastGen\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08671)\n",
            "\n",
            "\n",
            "# UniVG: Towards UNIfied-modal Video Generation\n",
            "\n",
            "**Abstract:** Diffusion based video generation has received extensive attention and achieved considerable success within both the academic and industrial communities. However, current efforts are mainly concentrated on single-objective or single-task video generation, such as generation driven by text, by image, or by a combination of text and image. This cannot fully meet the needs of real-world application scenarios, as users are likely to input images and text conditions in a flexible manner, either individually or in combination. To address this, we propose a Unified-modal Video Generation system that is capable of handling multiple video generation tasks across text and image modalities. To this end, we revisit the various video generation tasks within our system from the perspective of generative freedom, and classify them into high-freedom and low-freedom video generation categories. For high-freedom video generation, we employ Multi-condition Cross Attention to generate videos that align with the semantics of the input images or text. For low-freedom video generation, we introduce Biased Gaussian Noise to replace the pure random Gaussian Noise, which helps to better preserve the content of the input conditions. Our method achieves the lowest Fréchet Video Distance (FVD) on the public academic benchmark MSR-VTT, surpasses the current open-source methods in human evaluations, and is on par with the current close-source method Gen2. For more samples, visit [here](https://univg-baidu.github.io).\n",
            "\n",
            "**Keywords:** video generation, unified-modal, generative freedom, multi-condition cross attention, biased Gaussian noise\n",
            "\n",
            "**arXiv URL:** [https://arxiv.org/abs/2401.09084](https://arxiv.org/abs/2401.09084)\n",
            "\n",
            "\n",
            "# Asynchronous Local-SGD Training for Language Modeling\n",
            "\n",
            "**Abstract:** Local stochastic gradient descent (Local-SGD), also referred to as federated averaging, is an approach to distributed optimization where each device performs more than one SGD update per communication. This work presents an empirical study of *asynchronous* Local-SGD for training language models; that is, each worker updates the global parameters as soon as it has finished its SGD steps. We conduct a comprehensive investigation by examining how worker hardware heterogeneity, model size, number of workers, and optimizer could impact the learning performance. We find that with naive implementations, asynchronous Local-SGD takes more iterations to converge than its synchronous counterpart despite updating the (global) model parameters more frequently. We identify momentum acceleration on the global parameters when worker gradients are stale as a key challenge. We propose a novel method that utilizes a delayed Nesterov momentum update and adjusts the workers' local training steps based on their computation speed. This approach, evaluated with models up to 150M parameters on the C4 dataset, matches the performance of synchronous Local-SGD in terms of perplexity per update step, and significantly surpasses it in terms of wall clock time.\n",
            "\n",
            "**Keywords:** Local-SGD, asynchronous training, language modeling, distributed optimization, Nesterov momentum\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.09135)\n",
            "\n",
            "\n",
            "# ReFT: Reasoning with Reinforced Fine-Tuning\n",
            "\n",
            "**Abstract:** One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question and the rewards are naturally derived from the ground-truth answers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that ReFT significantly outperforms SFT, and the performance can be potentially further boosted by combining inference-time strategies such as majority voting and re-ranking. Note that ReFT obtains the improvement by learning from the same training questions as SFT, without relying on extra or augmented training questions. This indicates a superior generalization ability for ReFT.\n",
            "\n",
            "**Keywords:** Large Language Models, Reinforced Fine-Tuning, Reasoning, Chain-of-Thought, Math problem-solving\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08967)\n",
            "\n",
            "\n",
            "# VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models\n",
            "\n",
            "**Abstract:** Text-to-video generation aims to produce a video based on a given prompt. Recently, several commercial video models have been able to generate plausible videos with minimal noise, excellent details, and high aesthetic scores. However, these models rely on large-scale, well-filtered, high-quality videos that are not accessible to the community. Many existing research works, which train models using the low-quality WebVid-10M dataset, struggle to generate high-quality videos because the models are optimized to fit WebVid-10M. In this work, we explore the training scheme of video models extended from Stable Diffusion and investigate the feasibility of leveraging low-quality videos and synthesized high-quality images to obtain a high-quality video model. We first analyze the connection between the spatial and temporal modules of video models and the distribution shift to low-quality videos. We observe that full training of all modules results in a stronger coupling between spatial and temporal modules than only training temporal modules. Based on this stronger coupling, we shift the distribution to higher quality without motion degradation by finetuning spatial modules with high-quality images, resulting in a generic high-quality video model. Evaluations are conducted to demonstrate the superiority of the proposed method, particularly in picture quality, motion, and concept composition.\n",
            "\n",
            "**Keywords:** Text-to-video generation, video diffusion models, high-quality videos, low-quality videos, synthesized high-quality images\n",
            "\n",
            "[arXiv link](https://arxiv.org/abs/2401.09047)\n",
            "\n",
            "\n",
            "# SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers\n",
            "\n",
            "**Abstract:** \n",
            "In this research paper, the authors introduce Scalable Interpolant Transformers (SiT), a family of generative models that are built on the foundation of Diffusion Transformers (DiT). SiT utilizes the interpolant framework, which provides a more flexible way of connecting two distributions compared to standard diffusion models. This framework allows for a modular study of different design choices that impact generative models built on dynamical transport. These design choices include discrete vs. continuous time learning, the objective for the model to learn, the choice of interpolant connecting the distributions, and the deployment of a deterministic or stochastic sampler. By carefully incorporating these elements, SiT outperforms DiT consistently across various model sizes on the conditional ImageNet 256x256 benchmark, while maintaining the same backbone, number of parameters, and GFLOPs. Additionally, SiT achieves an impressive FID-50K score of 2.06 by exploring different diffusion coefficients, which can be independently tuned from the learning process.\n",
            "\n",
            "**Keywords:** Generative models, Diffusion Transformers, Interpolant framework, Dynamical transport, Conditional ImageNet\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08740)\n",
            "\n",
            "\n",
            "# GARField: Group Anything with Radiance Fields\n",
            "\n",
            "**Abstract:** Grouping is inherently ambiguous due to the multiple levels of granularity in which one can decompose a scene -- should the wheels of an excavator be considered separate or part of the whole? We present Group Anything with Radiance Fields (GARField), an approach for decomposing 3D scenes into a hierarchy of semantically meaningful groups from posed image inputs. To do this we embrace group ambiguity through physical scale: by optimizing a scale-conditioned 3D affinity feature field, a point in the world can belong to different groups of different sizes. We optimize this field from a set of 2D masks provided by Segment Anything (SAM) in a way that respects coarse-to-fine hierarchy, using scale to consistently fuse conflicting masks from different viewpoints. From this field we can derive a hierarchy of possible groupings via automatic tree construction or user interaction. We evaluate GARField on a variety of in-the-wild scenes and find it effectively extracts groups at many levels: clusters of objects, objects, and various subparts. GARField inherently represents multi-view consistent groupings and produces higher fidelity groups than the input SAM masks. GARField's hierarchical grouping could have exciting downstream applications such as 3D asset extraction or dynamic scene understanding. See the project website at [https://www.garfield.studio/](https://www.garfield.studio/)\n",
            "\n",
            "**Keywords:** Grouping, 3D scenes, Radiance Fields, Semantically meaningful groups, Scale-conditioned 3D affinity feature field.\n",
            "\n",
            "\n",
            "# SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding\n",
            "\n",
            "**Abstract:** \n",
            "The paper focuses on the challenges of aligning language with the 3D physical environment, known as 3D vision-language grounding, in the development of embodied agents. It addresses the complexity of 3D scenes, the scarcity of paired 3D vision-language data, and the absence of a unified learning framework. The authors introduce SceneVerse, a million-scale 3D vision-language dataset, and propose a unified pre-training framework called Grounded Pre-training for Scenes (GPS). Through extensive experiments, they demonstrate the effectiveness of GPS in achieving state-of-the-art performance on existing 3D visual grounding benchmarks. The paper also presents zero-shot transfer experiments to showcase the potential of SceneVerse and GPS in challenging 3D vision-language tasks.\n",
            "\n",
            "**Keywords:** 3D vision-language grounding, embodied agents, 3D scenes, pre-training framework, visual grounding\n",
            "\n",
            "**arXiv URL:** [https://arxiv.org/abs/2401.09340](https://arxiv.org/abs/2401.09340)\n",
            "\n",
            "\n",
            "# Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis\n",
            "\n",
            "**Abstract:** Addressing the limitations of text as a source of accurate layout representation in text-conditional diffusion models, many works incorporate additional signals to condition certain attributes within a generated image. Although successful, previous works do not account for the specific localization of said attributes extended into the three-dimensional plane. In this context, we present a conditional diffusion model that integrates control over three-dimensional object placement with disentangled representations of global stylistic semantics from multiple exemplar images. Specifically, we first introduce depth disentanglement training to leverage the relative depth of objects as an estimator, allowing the model to identify the absolute positions of unseen objects through the use of synthetic image triplets. We also introduce soft guidance, a method for imposing global semantics onto targeted regions without the use of any additional localization cues. Our integrated framework, Compose and Conquer (CnC), unifies these techniques to localize multiple conditions in a disentangled manner. We demonstrate that our approach allows perception of objects at varying depths while offering a versatile framework for composing localized objects with different global semantics. \n",
            "\n",
            "**Keywords:** Composable Image Synthesis, Diffusion-Based Model, 3D Depth Awareness, Disentangled Representations, Global Stylistic Semantics\n",
            "\n",
            "[arXiv link](https://arxiv.org/abs/2401.09048)\n",
            "\n",
            "---\n",
            "\n",
            "In this research paper titled \"Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis,\" the authors address the limitations of using text as a source of accurate layout representation in text-conditional diffusion models. They propose a conditional diffusion model that integrates control over three-dimensional object placement and disentangled representations of global stylistic semantics from multiple exemplar images.\n",
            "\n",
            "The authors introduce depth disentanglement training, which leverages the relative depth of objects as an estimator. This allows the model to identify the absolute positions of unseen objects using synthetic image triplets. They also introduce soft guidance, a method for imposing global semantics onto targeted regions without the need for additional localization cues.\n",
            "\n",
            "The integrated framework, called Compose and Conquer (CnC), unifies these techniques to localize multiple conditions in a disentangled manner. The authors demonstrate that their approach enables the perception of objects at varying depths and offers a versatile framework for composing localized objects with different global semantics.\n",
            "\n",
            "Keywords that represent the core themes of this paper include Composable Image Synthesis, Diffusion-Based Model, 3D Depth Awareness, Disentangled Representations, and Global Stylistic Semantics.\n",
            "\n",
            "For more details, you can refer to the [arXiv link](https://arxiv.org/abs/2401.09048).\n",
            "\n",
            "\n",
            "# ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization\n",
            "\n",
            "**Abstract:** Neural Radiance Fields (NeRF) exhibit remarkable performance for Novel View Synthesis (NVS) given a set of 2D images. However, NeRF training requires accurate camera pose for each input view, typically obtained by Structure-from-Motion (SfM) pipelines. Recent works have attempted to relax this constraint, but they still often rely on decent initial poses which they can refine. Here we aim at removing the requirement for pose initialization. We present Incremental CONfidence (ICON), an optimization procedure for training NeRFs from 2D video frames. ICON only assumes smooth camera motion to estimate initial guess for poses. Further, ICON introduces \"confidence\": an adaptive measure of model quality used to dynamically reweight gradients. ICON relies on high-confidence poses to learn NeRF, and high-confidence 3D structure (as encoded by NeRF) to learn poses. We show that ICON, without prior pose initialization, achieves superior performance in both CO3D and HO3D versus methods which use SfM pose.\n",
            "\n",
            "**Keywords:** Neural Radiance Fields, Novel View Synthesis, Pose Initialization, Incremental Confidence, Structure-from-Motion\n",
            "\n",
            "[arXiv Link](https://arxiv.org/abs/2401.08937) <class 'str'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CancelledError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-5b28dced21a8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mawait\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcron_expression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-86-52185846b04d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(spec, discord, wxpusher)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSubscriptionRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mawait\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubscribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDailyPaperWatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHuggingfaceDailyPaperCronTrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mawait\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/metagpt/subscription.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, raise_exception)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36msleep\u001b[0;34m(delay, result)\u001b[0m\n\u001b[1;32m    603\u001b[0m                         future, result)\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCancelledError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lBBGDSL4_tCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vWREx52KEwZ5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}