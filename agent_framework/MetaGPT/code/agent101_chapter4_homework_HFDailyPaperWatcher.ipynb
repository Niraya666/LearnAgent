{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install metagpt==0.5.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9xqJgSdOm2C",
        "outputId": "f74efaca-fb75-47a1-e67d-6775c71597a9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: metagpt==0.5.2 in /usr/local/lib/python3.10/dist-packages (0.5.2)\n",
            "Requirement already satisfied: aiohttp==3.8.4 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (3.8.4)\n",
            "Requirement already satisfied: channels==4.0.0 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (4.0.0)\n",
            "Requirement already satisfied: faiss-cpu==1.7.4 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (1.7.4)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (0.9.0)\n",
            "Requirement already satisfied: lancedb==0.1.16 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (0.1.16)\n",
            "Requirement already satisfied: langchain==0.0.231 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (0.0.231)\n",
            "Requirement already satisfied: loguru==0.6.0 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (0.6.0)\n",
            "Requirement already satisfied: meilisearch==0.21.0 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (0.21.0)\n",
            "Requirement already satisfied: numpy==1.24.3 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (1.24.3)\n",
            "Requirement already satisfied: openai>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (0.28.1)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (3.1.2)\n",
            "Requirement already satisfied: beautifulsoup4==4.12.2 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (4.12.2)\n",
            "Requirement already satisfied: pandas==2.0.3 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (2.0.3)\n",
            "Requirement already satisfied: pydantic==1.10.8 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (1.10.8)\n",
            "Requirement already satisfied: pytest==7.2.2 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (7.2.2)\n",
            "Requirement already satisfied: python-docx==0.8.11 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (0.8.11)\n",
            "Requirement already satisfied: PyYAML==6.0.1 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (6.0.1)\n",
            "Requirement already satisfied: setuptools==65.6.3 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (65.6.3)\n",
            "Requirement already satisfied: tenacity==8.2.2 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (8.2.2)\n",
            "Requirement already satisfied: tiktoken==0.4.0 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (0.4.0)\n",
            "Requirement already satisfied: tqdm==4.64.0 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (4.64.0)\n",
            "Requirement already satisfied: anthropic==0.3.6 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (0.3.6)\n",
            "Requirement already satisfied: typing-inspect==0.8.0 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (0.8.0)\n",
            "Requirement already satisfied: typing-extensions==4.5.0 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (4.5.0)\n",
            "Requirement already satisfied: libcst==1.0.1 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (1.0.1)\n",
            "Requirement already satisfied: qdrant-client==1.4.0 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (1.4.0)\n",
            "Requirement already satisfied: pytest-mock==3.11.1 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (3.11.1)\n",
            "Requirement already satisfied: ta==0.10.2 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (0.10.2)\n",
            "Requirement already satisfied: semantic-kernel==0.3.13.dev0 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (0.3.13.dev0)\n",
            "Requirement already satisfied: wrapt==1.15.0 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (1.15.0)\n",
            "Requirement already satisfied: websocket-client==0.58.0 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (0.58.0)\n",
            "Requirement already satisfied: aiofiles==23.2.1 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (23.2.1)\n",
            "Requirement already satisfied: gitpython==3.1.40 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (3.1.40)\n",
            "Requirement already satisfied: zhipuai==1.0.7 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (1.0.7)\n",
            "Requirement already satisfied: gitignore-parser==0.1.9 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (0.1.9)\n",
            "Requirement already satisfied: open-interpreter==0.1.7 in /usr/local/lib/python3.10/dist-packages (from metagpt==0.5.2) (0.1.7)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.4->metagpt==0.5.2) (23.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.4->metagpt==0.5.2) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.4->metagpt==0.5.2) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.4->metagpt==0.5.2) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.4->metagpt==0.5.2) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.4->metagpt==0.5.2) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.4->metagpt==0.5.2) (1.3.1)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic==0.3.6->metagpt==0.5.2) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from anthropic==0.3.6->metagpt==0.5.2) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from anthropic==0.3.6->metagpt==0.5.2) (0.26.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic==0.3.6->metagpt==0.5.2) (0.15.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4==4.12.2->metagpt==0.5.2) (2.5)\n",
            "Requirement already satisfied: Django>=3.2 in /usr/local/lib/python3.10/dist-packages (from channels==4.0.0->metagpt==0.5.2) (5.0.1)\n",
            "Requirement already satisfied: asgiref<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from channels==4.0.0->metagpt==0.5.2) (3.7.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython==3.1.40->metagpt==0.5.2) (4.0.11)\n",
            "Requirement already satisfied: pylance==0.5.10 in /usr/local/lib/python3.10/dist-packages (from lancedb==0.1.16->metagpt==0.5.2) (0.5.10)\n",
            "Requirement already satisfied: ratelimiter in /usr/local/lib/python3.10/dist-packages (from lancedb==0.1.16->metagpt==0.5.2) (1.2.0.post0)\n",
            "Requirement already satisfied: retry in /usr/local/lib/python3.10/dist-packages (from lancedb==0.1.16->metagpt==0.5.2) (0.9.2)\n",
            "Requirement already satisfied: attr in /usr/local/lib/python3.10/dist-packages (from lancedb==0.1.16->metagpt==0.5.2) (0.3.2)\n",
            "Requirement already satisfied: semver in /usr/local/lib/python3.10/dist-packages (from lancedb==0.1.16->metagpt==0.5.2) (3.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231->metagpt==0.5.2) (2.0.24)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231->metagpt==0.5.2) (0.5.14)\n",
            "Requirement already satisfied: langchainplus-sdk<0.0.21,>=0.0.20 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231->metagpt==0.5.2) (0.0.20)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231->metagpt==0.5.2) (2.8.8)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231->metagpt==0.5.2) (1.2.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231->metagpt==0.5.2) (2.31.0)\n",
            "Requirement already satisfied: camel-converter[pydantic] in /usr/local/lib/python3.10/dist-packages (from meilisearch==0.21.0->metagpt==0.5.2) (3.1.1)\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from open-interpreter==0.1.7->metagpt==0.5.2) (1.4.4)\n",
            "Requirement already satisfied: astor<0.9.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from open-interpreter==0.1.7->metagpt==0.5.2) (0.8.1)\n",
            "Requirement already satisfied: git-python<2.0.0,>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from open-interpreter==0.1.7->metagpt==0.5.2) (1.0.3)\n",
            "Requirement already satisfied: huggingface-hub<0.17.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from open-interpreter==0.1.7->metagpt==0.5.2) (0.16.4)\n",
            "Requirement already satisfied: inquirer<4.0.0,>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from open-interpreter==0.1.7->metagpt==0.5.2) (3.2.1)\n",
            "Requirement already satisfied: litellm<0.2.0,>=0.1.590 in /usr/local/lib/python3.10/dist-packages (from open-interpreter==0.1.7->metagpt==0.5.2) (0.1.824)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from open-interpreter==0.1.7->metagpt==0.5.2) (1.0.0)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.4.2 in /usr/local/lib/python3.10/dist-packages (from open-interpreter==0.1.7->metagpt==0.5.2) (13.7.0)\n",
            "Requirement already satisfied: semgrep<2.0.0,>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from open-interpreter==0.1.7->metagpt==0.5.2) (1.52.0)\n",
            "Requirement already satisfied: six<2.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from open-interpreter==0.1.7->metagpt==0.5.2) (1.16.0)\n",
            "Requirement already satisfied: tokentrim<0.2.0,>=0.1.9 in /usr/local/lib/python3.10/dist-packages (from open-interpreter==0.1.7->metagpt==0.5.2) (0.1.13)\n",
            "Requirement already satisfied: wget<4.0,>=3.2 in /usr/local/lib/python3.10/dist-packages (from open-interpreter==0.1.7->metagpt==0.5.2) (3.2)\n",
            "Requirement already satisfied: yaspin<4.0.0,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from open-interpreter==0.1.7->metagpt==0.5.2) (3.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->metagpt==0.5.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->metagpt==0.5.2) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->metagpt==0.5.2) (2023.4)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest==7.2.2->metagpt==0.5.2) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest==7.2.2->metagpt==0.5.2) (23.2)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest==7.2.2->metagpt==0.5.2) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest==7.2.2->metagpt==0.5.2) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest==7.2.2->metagpt==0.5.2) (2.0.1)\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from python-docx==0.8.11->metagpt==0.5.2) (4.9.4)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client==1.4.0->metagpt==0.5.2) (1.60.0)\n",
            "Requirement already satisfied: grpcio-tools>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client==1.4.0->metagpt==0.5.2) (1.60.0)\n",
            "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client==1.4.0->metagpt==0.5.2) (2.8.2)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.26.14 in /usr/local/lib/python3.10/dist-packages (from qdrant-client==1.4.0->metagpt==0.5.2) (1.26.18)\n",
            "Requirement already satisfied: openapi_core<0.19.0,>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (0.18.2)\n",
            "Requirement already satisfied: prance<24.0.0.0,>=23.6.21.0 in /usr/local/lib/python3.10/dist-packages (from semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (23.6.21.0)\n",
            "Requirement already satisfied: regex<2024.0.0,>=2023.6.3 in /usr/local/lib/python3.10/dist-packages (from semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (2023.6.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect==0.8.0->metagpt==0.5.2) (1.0.0)\n",
            "Requirement already satisfied: PyJWT in /usr/lib/python3/dist-packages (from zhipuai==1.0.7->metagpt==0.5.2) (2.3.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.10/dist-packages (from zhipuai==1.0.7->metagpt==0.5.2) (0.6)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from zhipuai==1.0.7->metagpt==0.5.2) (5.3.2)\n",
            "Requirement already satisfied: pyarrow>=10 in /usr/local/lib/python3.10/dist-packages (from pylance==0.5.10->lancedb==0.1.16->metagpt==0.5.2) (10.0.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->metagpt==0.5.2) (1.1.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer->metagpt==0.5.2) (8.1.7)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->anthropic==0.3.6->metagpt==0.5.2) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->anthropic==0.3.6->metagpt==0.5.2) (1.3.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.231->metagpt==0.5.2) (3.20.2)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from Django>=3.2->channels==4.0.0->metagpt==0.5.2) (0.4.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython==3.1.40->metagpt==0.5.2) (5.0.1)\n",
            "Requirement already satisfied: protobuf<5.0dev,>=4.21.6 in /usr/local/lib/python3.10/dist-packages (from grpcio-tools>=1.41.0->qdrant-client==1.4.0->metagpt==0.5.2) (4.25.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic==0.3.6->metagpt==0.5.2) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic==0.3.6->metagpt==0.5.2) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic==0.3.6->metagpt==0.5.2) (0.14.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic==0.3.6->metagpt==0.5.2) (4.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.17.0,>=0.16.4->open-interpreter==0.1.7->metagpt==0.5.2) (3.13.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.17.0,>=0.16.4->open-interpreter==0.1.7->metagpt==0.5.2) (2023.6.0)\n",
            "Requirement already satisfied: blessed>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from inquirer<4.0.0,>=3.1.3->open-interpreter==0.1.7->metagpt==0.5.2) (1.20.0)\n",
            "Requirement already satisfied: editor>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from inquirer<4.0.0,>=3.1.3->open-interpreter==0.1.7->metagpt==0.5.2) (1.6.5)\n",
            "Requirement already satisfied: readchar>=3.0.6 in /usr/local/lib/python3.10/dist-packages (from inquirer<4.0.0,>=3.1.3->open-interpreter==0.1.7->metagpt==0.5.2) (4.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.10/dist-packages (from litellm<0.2.0,>=0.1.590->open-interpreter==0.1.7->metagpt==0.5.2) (7.0.1)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from litellm<0.2.0,>=0.1.590->open-interpreter==0.1.7->metagpt==0.5.2) (3.1.3)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.10/dist-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (0.6.1)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (4.19.2)\n",
            "Requirement already satisfied: jsonschema-spec<0.3.0,>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (0.2.4)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (10.1.0)\n",
            "Requirement already satisfied: openapi-schema-validator<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (0.6.2)\n",
            "Requirement already satisfied: openapi-spec-validator<0.8.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (0.7.1)\n",
            "Requirement already satisfied: parse in /usr/local/lib/python3.10/dist-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (1.20.0)\n",
            "Requirement already satisfied: werkzeug in /usr/local/lib/python3.10/dist-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (3.0.1)\n",
            "Requirement already satisfied: chardet>=3.0 in /usr/local/lib/python3.10/dist-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (5.2.0)\n",
            "Requirement already satisfied: ruamel.yaml>=0.17.10 in /usr/local/lib/python3.10/dist-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (0.17.40)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.4.2->open-interpreter==0.1.7->metagpt==0.5.2) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.4.2->open-interpreter==0.1.7->metagpt==0.5.2) (2.16.1)\n",
            "Requirement already satisfied: boltons~=21.0 in /usr/local/lib/python3.10/dist-packages (from semgrep<2.0.0,>=1.41.0->open-interpreter==0.1.7->metagpt==0.5.2) (21.0.0)\n",
            "Requirement already satisfied: click-option-group~=0.5 in /usr/local/lib/python3.10/dist-packages (from semgrep<2.0.0,>=1.41.0->open-interpreter==0.1.7->metagpt==0.5.2) (0.5.6)\n",
            "Requirement already satisfied: colorama~=0.4.0 in /usr/local/lib/python3.10/dist-packages (from semgrep<2.0.0,>=1.41.0->open-interpreter==0.1.7->metagpt==0.5.2) (0.4.6)\n",
            "Requirement already satisfied: defusedxml~=0.7.1 in /usr/local/lib/python3.10/dist-packages (from semgrep<2.0.0,>=1.41.0->open-interpreter==0.1.7->metagpt==0.5.2) (0.7.1)\n",
            "Requirement already satisfied: glom~=22.1 in /usr/local/lib/python3.10/dist-packages (from semgrep<2.0.0,>=1.41.0->open-interpreter==0.1.7->metagpt==0.5.2) (22.1.0)\n",
            "Requirement already satisfied: peewee~=3.14 in /usr/local/lib/python3.10/dist-packages (from semgrep<2.0.0,>=1.41.0->open-interpreter==0.1.7->metagpt==0.5.2) (3.17.0)\n",
            "Requirement already satisfied: wcmatch~=8.3 in /usr/local/lib/python3.10/dist-packages (from semgrep<2.0.0,>=1.41.0->open-interpreter==0.1.7->metagpt==0.5.2) (8.5)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.231->metagpt==0.5.2) (3.0.3)\n",
            "Requirement already satisfied: termcolor<3.0,>=2.3 in /usr/local/lib/python3.10/dist-packages (from yaspin<4.0.0,>=3.0.1->open-interpreter==0.1.7->metagpt==0.5.2) (2.4.0)\n",
            "Requirement already satisfied: decorator>=3.4.2 in /usr/local/lib/python3.10/dist-packages (from retry->lancedb==0.1.16->metagpt==0.5.2) (4.4.2)\n",
            "Requirement already satisfied: py<2.0.0,>=1.4.26 in /usr/local/lib/python3.10/dist-packages (from retry->lancedb==0.1.16->metagpt==0.5.2) (1.11.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.19.0->inquirer<4.0.0,>=3.1.3->open-interpreter==0.1.7->metagpt==0.5.2) (0.2.13)\n",
            "Requirement already satisfied: runs in /usr/local/lib/python3.10/dist-packages (from editor>=1.6.0->inquirer<4.0.0,>=3.1.3->open-interpreter==0.1.7->metagpt==0.5.2) (1.2.0)\n",
            "Requirement already satisfied: xmod in /usr/local/lib/python3.10/dist-packages (from editor>=1.6.0->inquirer<4.0.0,>=3.1.3->open-interpreter==0.1.7->metagpt==0.5.2) (1.8.1)\n",
            "Requirement already satisfied: face>=20.1.0 in /usr/local/lib/python3.10/dist-packages (from glom~=22.1->semgrep<2.0.0,>=1.41.0->open-interpreter==0.1.7->metagpt==0.5.2) (22.0.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx<1,>=0.23.0->anthropic==0.3.6->metagpt==0.5.2) (6.0.1)\n",
            "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx<1,>=0.23.0->anthropic==0.3.6->metagpt==0.5.2) (4.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.8.0->litellm<0.2.0,>=0.1.590->open-interpreter==0.1.7->metagpt==0.5.2) (3.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm<0.2.0,>=0.1.590->open-interpreter==0.1.7->metagpt==0.5.2) (2.1.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (0.16.2)\n",
            "Requirement already satisfied: pathable<0.5.0,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema-spec<0.3.0,>=0.2.3->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (0.4.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.4.2->open-interpreter==0.1.7->metagpt==0.5.2) (0.1.2)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.10/dist-packages (from openapi-schema-validator<0.7.0,>=0.6.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (0.1.4)\n",
            "Requirement already satisfied: jsonschema-path<0.4.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (0.3.2)\n",
            "Requirement already satisfied: lazy-object-proxy<2.0.0,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (1.10.0)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from ruamel.yaml>=0.17.10->prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.3.13.dev0->metagpt==0.5.2) (0.2.8)\n",
            "Requirement already satisfied: bracex>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from wcmatch~=8.3->semgrep<2.0.0,>=1.41.0->open-interpreter==0.1.7->metagpt==0.5.2) (2.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install aiocron discord"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRxaG0NV8LX-",
        "outputId": "a2303ded-ebf8-49dc-c5e7-032fab32fdfa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aiocron in /usr/local/lib/python3.10/dist-packages (1.8)\n",
            "Requirement already satisfied: discord in /usr/local/lib/python3.10/dist-packages (2.3.2)\n",
            "Requirement already satisfied: croniter in /usr/local/lib/python3.10/dist-packages (from aiocron) (2.0.1)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from aiocron) (5.2)\n",
            "Requirement already satisfied: discord.py>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from discord) (2.3.2)\n",
            "Requirement already satisfied: aiohttp<4,>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from discord.py>=2.3.2->discord) (3.8.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from croniter->aiocron) (2.8.2)\n",
            "Requirement already satisfied: pytz>2021.1 in /usr/local/lib/python3.10/dist-packages (from croniter->aiocron) (2023.3.post1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.7.4->discord.py>=2.3.2->discord) (23.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.7.4->discord.py>=2.3.2->discord) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.7.4->discord.py>=2.3.2->discord) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.7.4->discord.py>=2.3.2->discord) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.7.4->discord.py>=2.3.2->discord) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.7.4->discord.py>=2.3.2->discord) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.7.4->discord.py>=2.3.2->discord) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->croniter->aiocron) (1.16.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp<4,>=3.7.4->discord.py>=2.3.2->discord) (3.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RJdjWa2D8fn6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import aiohttp\n",
        "import asyncio\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "async def fetch_html(url):\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        async with session.get(url) as response:\n",
        "            return await response.text()\n",
        "\n",
        "async def parse_main_page(html):\n",
        "    title_list = []\n",
        "    href_list = []\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    # 更新查找标签的逻辑以匹配当前网页结构\n",
        "    title_tags = soup.find_all('h3', class_='mb-1 text-lg font-semibold leading-[1.2] hover:underline peer-hover:underline md:text-2xl')\n",
        "    for title_tag in title_tags:\n",
        "        a_tag = title_tag.find('a')  # 标题内的<a>标签\n",
        "        if a_tag:\n",
        "            title = a_tag.text.strip()  # 清除空白字符得到标题文本\n",
        "            href = a_tag['href']  # 提取href属性\n",
        "            title_list.append(title)  # 添加标题到列表\n",
        "            href_list.append(href)  # 添加链接到列表\n",
        "    return title_list, href_list\n",
        "\n",
        "async def parse_sub_page(html):\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    abstract = soup.find('div', class_=\"pb-8 pr-4 md:pr-16\").p.text\n",
        "    arxiv_url = soup.find('a', class_=\"btn inline-flex h-9 items-center\", href=True)['href']\n",
        "    return abstract, arxiv_url\n",
        "\n",
        "async def main():\n",
        "    url = 'https://huggingface.co/papers'\n",
        "    base_url = 'https://huggingface.co'\n",
        "    repositories = []\n",
        "    try:\n",
        "        html = await fetch_html(url)\n",
        "        title_list, href_list = await parse_main_page(html)\n",
        "\n",
        "        for title, href in zip(title_list, href_list):\n",
        "            repo_info = {}\n",
        "            repo_info['title'] = title\n",
        "            # repo_info['href'] = href\n",
        "            repositories.append(repo_info)\n",
        "            # print(title, href)\n",
        "            sub_html = await fetch_html(base_url + href)\n",
        "            abstract, arxiv_url = await parse_sub_page(sub_html)\n",
        "            # print(abstract, arxiv_url)\n",
        "            repo_info['abstract'] = abstract\n",
        "            repo_info['arxiv_url'] = arxiv_url\n",
        "            repositories.append(repo_info)\n",
        "        return repositories\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5cAtKz7GLh21"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repositories = await main()\n",
        "repositories"
      ],
      "metadata": {
        "id": "skQN364GNqZ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3bc3791-eb4a-4b48-9d17-247515c0e56e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'InstantID: Zero-shot Identity-Preserving Generation in Seconds',\n",
              "  'abstract': 'There has been significant progress in personalized image synthesis with\\nmethods such as Textual Inversion, DreamBooth, and LoRA. Yet, their real-world\\napplicability is hindered by high storage demands, lengthy fine-tuning\\nprocesses, and the need for multiple reference images. Conversely, existing ID\\nembedding-based methods, while requiring only a single forward inference, face\\nchallenges: they either necessitate extensive fine-tuning across numerous model\\nparameters, lack compatibility with community pre-trained models, or fail to\\nmaintain high face fidelity. Addressing these limitations, we introduce\\nInstantID, a powerful diffusion model-based solution. Our plug-and-play module\\nadeptly handles image personalization in various styles using just a single\\nfacial image, while ensuring high fidelity. To achieve this, we design a novel\\nIdentityNet by imposing strong semantic and weak spatial conditions,\\nintegrating facial and landmark images with textual prompts to steer the image\\ngeneration. InstantID demonstrates exceptional performance and efficiency,\\nproving highly beneficial in real-world applications where identity\\npreservation is paramount. Moreover, our work seamlessly integrates with\\npopular pre-trained text-to-image diffusion models like SD1.5 and SDXL, serving\\nas an adaptable plugin. Our codes and pre-trained checkpoints will be available\\nat https://github.com/InstantID/InstantID.',\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.07519'},\n",
              " {'title': 'InstantID: Zero-shot Identity-Preserving Generation in Seconds',\n",
              "  'abstract': 'There has been significant progress in personalized image synthesis with\\nmethods such as Textual Inversion, DreamBooth, and LoRA. Yet, their real-world\\napplicability is hindered by high storage demands, lengthy fine-tuning\\nprocesses, and the need for multiple reference images. Conversely, existing ID\\nembedding-based methods, while requiring only a single forward inference, face\\nchallenges: they either necessitate extensive fine-tuning across numerous model\\nparameters, lack compatibility with community pre-trained models, or fail to\\nmaintain high face fidelity. Addressing these limitations, we introduce\\nInstantID, a powerful diffusion model-based solution. Our plug-and-play module\\nadeptly handles image personalization in various styles using just a single\\nfacial image, while ensuring high fidelity. To achieve this, we design a novel\\nIdentityNet by imposing strong semantic and weak spatial conditions,\\nintegrating facial and landmark images with textual prompts to steer the image\\ngeneration. InstantID demonstrates exceptional performance and efficiency,\\nproving highly beneficial in real-world applications where identity\\npreservation is paramount. Moreover, our work seamlessly integrates with\\npopular pre-trained text-to-image diffusion models like SD1.5 and SDXL, serving\\nas an adaptable plugin. Our codes and pre-trained checkpoints will be available\\nat https://github.com/InstantID/InstantID.',\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.07519'},\n",
              " {'title': 'HexaGen3D: StableDiffusion is just one step away from Fast and Diverse Text-to-3D Generation',\n",
              "  'abstract': 'Despite the latest remarkable advances in generative modeling, efficient\\ngeneration of high-quality 3D assets from textual prompts remains a difficult\\ntask. A key challenge lies in data scarcity: the most extensive 3D datasets\\nencompass merely millions of assets, while their 2D counterparts contain\\nbillions of text-image pairs. To address this, we propose a novel approach\\nwhich harnesses the power of large, pretrained 2D diffusion models. More\\nspecifically, our approach, HexaGen3D, fine-tunes a pretrained text-to-image\\nmodel to jointly predict 6 orthographic projections and the corresponding\\nlatent triplane. We then decode these latents to generate a textured mesh.\\nHexaGen3D does not require per-sample optimization, and can infer high-quality\\nand diverse objects from textual prompts in 7 seconds, offering significantly\\nbetter quality-to-latency trade-offs when comparing to existing approaches.\\nFurthermore, HexaGen3D demonstrates strong generalization to new objects or\\ncompositions.',\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.07727'},\n",
              " {'title': 'HexaGen3D: StableDiffusion is just one step away from Fast and Diverse Text-to-3D Generation',\n",
              "  'abstract': 'Despite the latest remarkable advances in generative modeling, efficient\\ngeneration of high-quality 3D assets from textual prompts remains a difficult\\ntask. A key challenge lies in data scarcity: the most extensive 3D datasets\\nencompass merely millions of assets, while their 2D counterparts contain\\nbillions of text-image pairs. To address this, we propose a novel approach\\nwhich harnesses the power of large, pretrained 2D diffusion models. More\\nspecifically, our approach, HexaGen3D, fine-tunes a pretrained text-to-image\\nmodel to jointly predict 6 orthographic projections and the corresponding\\nlatent triplane. We then decode these latents to generate a textured mesh.\\nHexaGen3D does not require per-sample optimization, and can infer high-quality\\nand diverse objects from textual prompts in 7 seconds, offering significantly\\nbetter quality-to-latency trade-offs when comparing to existing approaches.\\nFurthermore, HexaGen3D demonstrates strong generalization to new objects or\\ncompositions.',\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.07727'},\n",
              " {'title': 'Quantum Denoising Diffusion Models',\n",
              "  'abstract': 'In recent years, machine learning models like DALL-E, Craiyon, and Stable\\nDiffusion have gained significant attention for their ability to generate\\nhigh-resolution images from concise descriptions. Concurrently, quantum\\ncomputing is showing promising advances, especially with quantum machine\\nlearning which capitalizes on quantum mechanics to meet the increasing\\ncomputational requirements of traditional machine learning algorithms. This\\npaper explores the integration of quantum machine learning and variational\\nquantum circuits to augment the efficacy of diffusion-based image generation\\nmodels. Specifically, we address two challenges of classical diffusion models:\\ntheir low sampling speed and the extensive parameter requirements. We introduce\\ntwo quantum diffusion models and benchmark their capabilities against their\\nclassical counterparts using MNIST digits, Fashion MNIST, and CIFAR-10. Our\\nmodels surpass the classical models with similar parameter counts in terms of\\nperformance metrics FID, SSIM, and PSNR. Moreover, we introduce a consistency\\nmodel unitary single sampling architecture that combines the diffusion\\nprocedure into a single step, enabling a fast one-step image generation.',\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.07049'},\n",
              " {'title': 'Quantum Denoising Diffusion Models',\n",
              "  'abstract': 'In recent years, machine learning models like DALL-E, Craiyon, and Stable\\nDiffusion have gained significant attention for their ability to generate\\nhigh-resolution images from concise descriptions. Concurrently, quantum\\ncomputing is showing promising advances, especially with quantum machine\\nlearning which capitalizes on quantum mechanics to meet the increasing\\ncomputational requirements of traditional machine learning algorithms. This\\npaper explores the integration of quantum machine learning and variational\\nquantum circuits to augment the efficacy of diffusion-based image generation\\nmodels. Specifically, we address two challenges of classical diffusion models:\\ntheir low sampling speed and the extensive parameter requirements. We introduce\\ntwo quantum diffusion models and benchmark their capabilities against their\\nclassical counterparts using MNIST digits, Fashion MNIST, and CIFAR-10. Our\\nmodels surpass the classical models with similar parameter counts in terms of\\nperformance metrics FID, SSIM, and PSNR. Moreover, we introduce a consistency\\nmodel unitary single sampling architecture that combines the diffusion\\nprocedure into a single step, enabling a fast one-step image generation.',\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.07049'},\n",
              " {'title': 'Towards A Better Metric for Text-to-Video Generation',\n",
              "  'abstract': \"Generative models have demonstrated remarkable capability in synthesizing\\nhigh-quality text, images, and videos. For video generation, contemporary\\ntext-to-video models exhibit impressive capabilities, crafting visually\\nstunning videos. Nonetheless, evaluating such videos poses significant\\nchallenges. Current research predominantly employs automated metrics such as\\nFVD, IS, and CLIP Score. However, these metrics provide an incomplete analysis,\\nparticularly in the temporal assessment of video content, thus rendering them\\nunreliable indicators of true video quality. Furthermore, while user studies\\nhave the potential to reflect human perception accurately, they are hampered by\\ntheir time-intensive and laborious nature, with outcomes that are often tainted\\nby subjective bias. In this paper, we investigate the limitations inherent in\\nexisting metrics and introduce a novel evaluation pipeline, the Text-to-Video\\nScore (T2VScore). This metric integrates two pivotal criteria: (1) Text-Video\\nAlignment, which scrutinizes the fidelity of the video in representing the\\ngiven text description, and (2) Video Quality, which evaluates the video's\\noverall production caliber with a mixture of experts. Moreover, to evaluate the\\nproposed metrics and facilitate future improvements on them, we present the\\nTVGE dataset, collecting human judgements of 2,543 text-to-video generated\\nvideos on the two criteria. Experiments on the TVGE dataset demonstrate the\\nsuperiority of the proposed T2VScore on offering a better metric for\\ntext-to-video generation.\",\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.07781'},\n",
              " {'title': 'Towards A Better Metric for Text-to-Video Generation',\n",
              "  'abstract': \"Generative models have demonstrated remarkable capability in synthesizing\\nhigh-quality text, images, and videos. For video generation, contemporary\\ntext-to-video models exhibit impressive capabilities, crafting visually\\nstunning videos. Nonetheless, evaluating such videos poses significant\\nchallenges. Current research predominantly employs automated metrics such as\\nFVD, IS, and CLIP Score. However, these metrics provide an incomplete analysis,\\nparticularly in the temporal assessment of video content, thus rendering them\\nunreliable indicators of true video quality. Furthermore, while user studies\\nhave the potential to reflect human perception accurately, they are hampered by\\ntheir time-intensive and laborious nature, with outcomes that are often tainted\\nby subjective bias. In this paper, we investigate the limitations inherent in\\nexisting metrics and introduce a novel evaluation pipeline, the Text-to-Video\\nScore (T2VScore). This metric integrates two pivotal criteria: (1) Text-Video\\nAlignment, which scrutinizes the fidelity of the video in representing the\\ngiven text description, and (2) Video Quality, which evaluates the video's\\noverall production caliber with a mixture of experts. Moreover, to evaluate the\\nproposed metrics and facilitate future improvements on them, we present the\\nTVGE dataset, collecting human judgements of 2,543 text-to-video generated\\nvideos on the two criteria. Experiments on the TVGE dataset demonstrate the\\nsuperiority of the proposed T2VScore on offering a better metric for\\ntext-to-video generation.\",\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.07781'}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Action: CrawlHuggingfaceDailyPaper"
      ],
      "metadata": {
        "id": "fUIpO-X34zm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from metagpt.actions.action import Action\n",
        "from metagpt.config import CONFIG\n",
        "\n",
        "class CrawlHuggingfaceDailyPaper(Action):\n",
        "    \"\"\"\n",
        "    This class specifically targets the daily papers section of the Huggingface website.\n",
        "    Its main functionality includes asynchronously fetching and parsing the latest research papers\n",
        "    published on Huggingface, extracting relevant details such as titles, abstracts, and arXiv URLs.\n",
        "    It can be utilized in applications where up-to-date research information from Huggingface\n",
        "    is required, making it a valuable tool for researchers and developers in AI and machine learning.\n",
        "    \"\"\"\n",
        "\n",
        "    async def run(self, url: str = \"https://huggingface.co/papers\"):\n",
        "        async with aiohttp.ClientSession() as client:\n",
        "            async with client.get(url, proxy=CONFIG.global_proxy) as response:\n",
        "                response.raise_for_status()\n",
        "                html = await response.text()\n",
        "\n",
        "        title_list, href_list = await parse_main_page(html)\n",
        "\n",
        "        repositories = []\n",
        "        base_url = 'https://huggingface.co'\n",
        "\n",
        "        for title, href in zip(title_list, href_list):\n",
        "            repo_info = {'title': title}\n",
        "            sub_html = await fetch_html(base_url + href)\n",
        "            abstract, arxiv_url = await parse_sub_page(sub_html)\n",
        "            repo_info['abstract'] = abstract\n",
        "            repo_info['arxiv_url'] = arxiv_url\n",
        "\n",
        "            repositories.append(repo_info)\n",
        "\n",
        "        return repositories\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp1W0OiAOU_6",
        "outputId": "4377ac31-0b27-4fe8-e9fa-813bcd4e2038"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-17 06:09:21.841 | INFO     | metagpt.const:get_metagpt_package_root:32 - Package root set to /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "craw_paper_action = CrawlHuggingfaceDailyPaper()\n",
        "resp = await craw_paper_action.run()\n",
        "resp"
      ],
      "metadata": {
        "id": "s2sDPcbEQaDr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "614b8c67-9075-42bc-d9e3-5f847c02cb85"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'InstantID: Zero-shot Identity-Preserving Generation in Seconds',\n",
              "  'abstract': 'There has been significant progress in personalized image synthesis with\\nmethods such as Textual Inversion, DreamBooth, and LoRA. Yet, their real-world\\napplicability is hindered by high storage demands, lengthy fine-tuning\\nprocesses, and the need for multiple reference images. Conversely, existing ID\\nembedding-based methods, while requiring only a single forward inference, face\\nchallenges: they either necessitate extensive fine-tuning across numerous model\\nparameters, lack compatibility with community pre-trained models, or fail to\\nmaintain high face fidelity. Addressing these limitations, we introduce\\nInstantID, a powerful diffusion model-based solution. Our plug-and-play module\\nadeptly handles image personalization in various styles using just a single\\nfacial image, while ensuring high fidelity. To achieve this, we design a novel\\nIdentityNet by imposing strong semantic and weak spatial conditions,\\nintegrating facial and landmark images with textual prompts to steer the image\\ngeneration. InstantID demonstrates exceptional performance and efficiency,\\nproving highly beneficial in real-world applications where identity\\npreservation is paramount. Moreover, our work seamlessly integrates with\\npopular pre-trained text-to-image diffusion models like SD1.5 and SDXL, serving\\nas an adaptable plugin. Our codes and pre-trained checkpoints will be available\\nat https://github.com/InstantID/InstantID.',\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.07519'},\n",
              " {'title': 'HexaGen3D: StableDiffusion is just one step away from Fast and Diverse Text-to-3D Generation',\n",
              "  'abstract': 'Despite the latest remarkable advances in generative modeling, efficient\\ngeneration of high-quality 3D assets from textual prompts remains a difficult\\ntask. A key challenge lies in data scarcity: the most extensive 3D datasets\\nencompass merely millions of assets, while their 2D counterparts contain\\nbillions of text-image pairs. To address this, we propose a novel approach\\nwhich harnesses the power of large, pretrained 2D diffusion models. More\\nspecifically, our approach, HexaGen3D, fine-tunes a pretrained text-to-image\\nmodel to jointly predict 6 orthographic projections and the corresponding\\nlatent triplane. We then decode these latents to generate a textured mesh.\\nHexaGen3D does not require per-sample optimization, and can infer high-quality\\nand diverse objects from textual prompts in 7 seconds, offering significantly\\nbetter quality-to-latency trade-offs when comparing to existing approaches.\\nFurthermore, HexaGen3D demonstrates strong generalization to new objects or\\ncompositions.',\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.07727'},\n",
              " {'title': 'Quantum Denoising Diffusion Models',\n",
              "  'abstract': 'In recent years, machine learning models like DALL-E, Craiyon, and Stable\\nDiffusion have gained significant attention for their ability to generate\\nhigh-resolution images from concise descriptions. Concurrently, quantum\\ncomputing is showing promising advances, especially with quantum machine\\nlearning which capitalizes on quantum mechanics to meet the increasing\\ncomputational requirements of traditional machine learning algorithms. This\\npaper explores the integration of quantum machine learning and variational\\nquantum circuits to augment the efficacy of diffusion-based image generation\\nmodels. Specifically, we address two challenges of classical diffusion models:\\ntheir low sampling speed and the extensive parameter requirements. We introduce\\ntwo quantum diffusion models and benchmark their capabilities against their\\nclassical counterparts using MNIST digits, Fashion MNIST, and CIFAR-10. Our\\nmodels surpass the classical models with similar parameter counts in terms of\\nperformance metrics FID, SSIM, and PSNR. Moreover, we introduce a consistency\\nmodel unitary single sampling architecture that combines the diffusion\\nprocedure into a single step, enabling a fast one-step image generation.',\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.07049'},\n",
              " {'title': 'Towards A Better Metric for Text-to-Video Generation',\n",
              "  'abstract': \"Generative models have demonstrated remarkable capability in synthesizing\\nhigh-quality text, images, and videos. For video generation, contemporary\\ntext-to-video models exhibit impressive capabilities, crafting visually\\nstunning videos. Nonetheless, evaluating such videos poses significant\\nchallenges. Current research predominantly employs automated metrics such as\\nFVD, IS, and CLIP Score. However, these metrics provide an incomplete analysis,\\nparticularly in the temporal assessment of video content, thus rendering them\\nunreliable indicators of true video quality. Furthermore, while user studies\\nhave the potential to reflect human perception accurately, they are hampered by\\ntheir time-intensive and laborious nature, with outcomes that are often tainted\\nby subjective bias. In this paper, we investigate the limitations inherent in\\nexisting metrics and introduce a novel evaluation pipeline, the Text-to-Video\\nScore (T2VScore). This metric integrates two pivotal criteria: (1) Text-Video\\nAlignment, which scrutinizes the fidelity of the video in representing the\\ngiven text description, and (2) Video Quality, which evaluates the video's\\noverall production caliber with a mixture of experts. Moreover, to evaluate the\\nproposed metrics and facilitate future improvements on them, we present the\\nTVGE dataset, collecting human judgements of 2,543 text-to-video generated\\nvideos on the two criteria. Experiments on the TVGE dataset demonstrate the\\nsuperiority of the proposed T2VScore on offering a better metric for\\ntext-to-video generation.\",\n",
              "  'arxiv_url': 'https://arxiv.org/abs/2401.07781'}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TRdrSQwoR4b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o973Grm6R60f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Action: SummaryDailyPaper\n",
        "\n",
        "summary each daily paper, add five keywords by LLM"
      ],
      "metadata": {
        "id": "vKItU_B76eQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any\n",
        "PAPER_SUMMARY_PROMPT = \"\"\"\n",
        "    Transform the given data about a research paper into a neat Markdown format. Also, identify and include five relevant keywords that best represent the core themes of the paper.\n",
        "    The provided data is:\n",
        "    ```\n",
        "    {data}\n",
        "    ```\n",
        "    Please create a markdown summary and suggest five keywords related to this paper.\n",
        "    \"\"\"\n",
        "class SummaryDailyPaper(Action):\n",
        "\n",
        "    async def run(\n",
        "        self,\n",
        "        data: Any\n",
        "    ):\n",
        "        return await self._aask(PAPER_SUMMARY_PROMPT.format(data=data))"
      ],
      "metadata": {
        "id": "Q_3G_iGbUSxN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "await SummaryDailyPaper().run(resp[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "mIBsrwmEUN2o",
        "outputId": "497d0834-2944-4258-9945-60abbc865217"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# InstantID: Zero-shot Identity-Preserving Generation in Seconds\n",
            "\n",
            "**Abstract:** There has been significant progress in personalized image synthesis with methods such as Textual Inversion, DreamBooth, and LoRA. Yet, their real-world applicability is hindered by high storage demands, lengthy fine-tuning processes, and the need for multiple reference images. Conversely, existing ID embedding-based methods, while requiring only a single forward inference, face challenges: they either necessitate extensive fine-tuning across numerous model parameters, lack compatibility with community pre-trained models, or fail to maintain high face fidelity. Addressing these limitations, we introduce InstantID, a powerful diffusion model-based solution. Our plug-and-play module adeptly handles image personalization in various styles using just a single facial image, while ensuring high fidelity. To achieve this, we design a novel IdentityNet by imposing strong semantic and weak spatial conditions, integrating facial and landmark images with textual prompts to steer the image generation. InstantID demonstrates exceptional performance and efficiency, proving highly beneficial in real-world applications where identity preservation is paramount. Moreover, our work seamlessly integrates with popular pre-trained text-to-image diffusion models like SD1.5 and SDXL, serving as an adaptable plugin. Our codes and pre-trained checkpoints will be available at [https://github.com/InstantID/InstantID](https://github.com/InstantID/InstantID).\n",
            "\n",
            "**Keywords:** personalized image synthesis, ID embedding-based methods, diffusion model, image personalization, identity preservation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# InstantID: Zero-shot Identity-Preserving Generation in Seconds\\n\\n**Abstract:** There has been significant progress in personalized image synthesis with methods such as Textual Inversion, DreamBooth, and LoRA. Yet, their real-world applicability is hindered by high storage demands, lengthy fine-tuning processes, and the need for multiple reference images. Conversely, existing ID embedding-based methods, while requiring only a single forward inference, face challenges: they either necessitate extensive fine-tuning across numerous model parameters, lack compatibility with community pre-trained models, or fail to maintain high face fidelity. Addressing these limitations, we introduce InstantID, a powerful diffusion model-based solution. Our plug-and-play module adeptly handles image personalization in various styles using just a single facial image, while ensuring high fidelity. To achieve this, we design a novel IdentityNet by imposing strong semantic and weak spatial conditions, integrating facial and landmark images with textual prompts to steer the image generation. InstantID demonstrates exceptional performance and efficiency, proving highly beneficial in real-world applications where identity preservation is paramount. Moreover, our work seamlessly integrates with popular pre-trained text-to-image diffusion models like SD1.5 and SDXL, serving as an adaptable plugin. Our codes and pre-trained checkpoints will be available at [https://github.com/InstantID/InstantID](https://github.com/InstantID/InstantID).\\n\\n**Keywords:** personalized image synthesis, ID embedding-based methods, diffusion model, image personalization, identity preservation.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4nkNi32EU9eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KVrQLINjZc0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Role: DailyPaperWatcher\n",
        "\n",
        "for analyze huggingfacce daily papers, and summary."
      ],
      "metadata": {
        "id": "6jeGEoi46ueO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List\n",
        "from metagpt.utils.common import OutputParser\n",
        "from metagpt.roles import Role\n",
        "from metagpt.schema import Message\n",
        "from metagpt.logs import logger\n",
        "\n",
        "class DailyPaperWatcher(Role):\n",
        "    def __init__(\n",
        "        self,\n",
        "        name=\"Huggy\",\n",
        "        profile=\"DailyPaperWatcher\",\n",
        "        goal=\"Generate a summary of Huggingface daily papers.\",\n",
        "        constraints=\"Only analyze based on the provided Huggingface daily papers.\",\n",
        "    ):\n",
        "        super().__init__(name, profile, goal, constraints)\n",
        "        self._init_actions([CrawlHuggingfaceDailyPaper])\n",
        "\n",
        "    # async def _act(self) -> Message:\n",
        "    #     logger.info(f\"{self._setting}: ready to {self._rc.todo}\")\n",
        "\n",
        "    #     todo = self._rc.todo\n",
        "\n",
        "    #     # Retrieve the last message from memory\n",
        "    #     try:\n",
        "    #         msg = self.get_memories(k=1)[0]\n",
        "    #     except IndexError:\n",
        "    #         logger.error(\"No messages in memory\")\n",
        "    #         return Message(content=\"Error: No messages in memory\", role=self.profile)\n",
        "\n",
        "    #     # Perform the action\n",
        "    #     try:\n",
        "    #         result = await todo.run(msg.content)\n",
        "    #         msg = Message(content=str(result), role=self.profile, cause_by=type(todo))\n",
        "    #         self._rc.memory.add(msg)\n",
        "\n",
        "    #         if isinstance(todo, (CrawlHuggingfaceDailyPaper, SummaryDailyPaper)):\n",
        "    #             await self._handle_paper(result)\n",
        "    #     except Exception as e:\n",
        "    #         logger.error(f\"Error during action execution: {e}\")\n",
        "    #         return Message(content=f\"Error: {e}\", role=self.profile)\n",
        "\n",
        "    #     return result\n",
        "    async def _act(self) -> Message:\n",
        "        logger.info(f\"{self._setting}: ready to {self._rc.todo}\")\n",
        "\n",
        "        todo = self._rc.todo\n",
        "\n",
        "        try:\n",
        "            msg = self.get_memories(k=1)[0]\n",
        "        except IndexError:\n",
        "            logger.error(\"No messages in memory\")\n",
        "            return Message(content=\"Error: No messages in memory\", role=self.profile)\n",
        "\n",
        "        try:\n",
        "            result = await todo.run(msg.content)\n",
        "            if isinstance(todo, CrawlHuggingfaceDailyPaper):\n",
        "                # 针对每篇论文创建并执行 SummaryDailyPaper 动作\n",
        "                logger.info(f\"Preparing to summarize {len(result)} papers\")\n",
        "                msg_content = ''\n",
        "                for paper in result:\n",
        "                    summary_action = SummaryDailyPaper(paper)\n",
        "                    summary_result = await summary_action.run(paper)\n",
        "                    summary_msg = Message(content=str(summary_result), role=self.profile, cause_by=type(summary_action))\n",
        "                    self._rc.memory.add(summary_msg)\n",
        "                    msg_content += str(summary_result)\n",
        "                    msg_content += '\\n'\n",
        "\n",
        "            else:\n",
        "                msg = Message(content=str(result), role=self.profile, cause_by=type(todo))\n",
        "                self._rc.memory.add(msg)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during action execution: {e}\")\n",
        "            return Message(content=f\"Error: {e}\", role=self.profile)\n",
        "\n",
        "        return Message(content=str(msg_content), role=self.profile, cause_by=type(todo))\n",
        "\n",
        "    async def _think(self) -> None:\n",
        "        \"\"\"Determine the next action to be taken by the role.\"\"\"\n",
        "        if self._rc.todo is None:\n",
        "            self._set_state(0)\n",
        "            return\n",
        "\n",
        "        if self._rc.state + 1 < len(self._states):\n",
        "            self._set_state(self._rc.state + 1)\n",
        "        else:\n",
        "            self._rc.todo = None\n",
        "\n",
        "    async def _react(self) -> Message:\n",
        "        \"\"\"Execute the assistant's think and actions.\"\"\"\n",
        "        while True:\n",
        "            await self._think()\n",
        "            if self._rc.todo is None:\n",
        "                break\n",
        "            msg = await self._act()\n",
        "\n",
        "        return msg\n",
        "\n",
        "    async def _handle_paper(self, paper_info) -> None:\n",
        "        actions = []\n",
        "        # Enhanced logging for debugging\n",
        "        logger.debug(f\"Handling paper with info: {paper_info}\")\n",
        "\n",
        "        for paper in paper_info:\n",
        "            actions.append(SummaryDailyPaper(paper))\n",
        "            logger.info(f\"Preparing to summarize paper: {paper['title']}\")\n",
        "\n",
        "        self._init_actions(actions)\n",
        "        self._rc.todo = None\n",
        "\n"
      ],
      "metadata": {
        "id": "d9EZfKlAdFNV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def main():\n",
        "\n",
        "    role = DailyPaperWatcher()\n",
        "    result = await role.run(\"https://huggingface.co/papers\")\n",
        "    logger.info(result)\n",
        "\n",
        "\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNHFxtLkdBKB",
        "outputId": "5f95198c-727c-4296-c3f2-7490513d97f1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-17 06:15:47.538 | INFO     | __main__:_act:44 - Huggy(DailyPaperWatcher): ready to CrawlHuggingfaceDailyPaper\n",
            "2024-01-17 06:15:48.725 | INFO     | __main__:_act:58 - Preparing to summarize 4 papers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# InstantID: Zero-shot Identity-Preserving Generation in Seconds\n",
            "\n",
            "**Abstract:** There has been significant progress in personalized image synthesis with methods such as Textual Inversion, DreamBooth, and LoRA. Yet, their real-world applicability is hindered by high storage demands, lengthy fine-tuning processes, and the need for multiple reference images. Conversely, existing ID embedding-based methods, while requiring only a single forward inference, face challenges: they either necessitate extensive fine-tuning across numerous model parameters, lack compatibility with community pre-trained models, or fail to maintain high face fidelity. Addressing these limitations, we introduce InstantID, a powerful diffusion model-based solution. Our plug-and-play module adeptly handles image personalization in various styles using just a single facial image, while ensuring high fidelity. To achieve this, we design a novel IdentityNet by imposing strong semantic and weak spatial conditions, integrating facial and landmark images with textual prompts to steer the image generation. InstantID demonstrates exceptional performance and efficiency, proving highly beneficial in real-world applications where identity preservation is paramount. Moreover, our work seamlessly integrates with popular pre-trained text-to-image diffusion models like SD1.5 and SDXL, serving as an adaptable plugin. Our codes and pre-trained checkpoints will be available at [https://github.com/InstantID/InstantID](https://github.com/InstantID/InstantID).\n",
            "\n",
            "**Keywords:** personalized image synthesis, identity preservation, diffusion model, facial image, image generation\n",
            "# Quantum Denoising Diffusion Models\n",
            "\n",
            "**Abstract:** In recent years, machine learning models like DALL-E, Craiyon, and Stable Diffusion have gained significant attention for their ability to generate high-resolution images from concise descriptions. Concurrently, quantum computing is showing promising advances, especially with quantum machine learning which capitalizes on quantum mechanics to meet the increasing computational requirements of traditional machine learning algorithms. This paper explores the integration of quantum machine learning and variational quantum circuits to augment the efficacy of diffusion-based image generation models. Specifically, we address two challenges of classical diffusion models: their low sampling speed and the extensive parameter requirements. We introduce two quantum diffusion models and benchmark their capabilities against their classical counterparts using MNIST digits, Fashion MNIST, and CIFAR-10. Our models surpass the classical models with similar parameter counts in terms of performance metrics FID, SSIM, and PSNR. Moreover, we introduce a consistency model unitary single sampling architecture that combines the diffusion procedure into a single step, enabling a fast one-step image generation.\n",
            "\n",
            "**Keywords:** quantum machine learning, variational quantum circuits, diffusion-based image generation, quantum diffusion models, performance metrics\n",
            "# Towards A Better Metric for Text-to-Video Generation\n",
            "\n",
            "**Abstract:** Generative models have demonstrated remarkable capability in synthesizing high-quality text, images, and videos. For video generation, contemporary text-to-video models exhibit impressive capabilities, crafting visually stunning videos. Nonetheless, evaluating such videos poses significant challenges. Current research predominantly employs automated metrics such as FVD, IS, and CLIP Score. However, these metrics provide an incomplete analysis, particularly in the temporal assessment of video content, thus rendering them unreliable indicators of true video quality. Furthermore, while user studies have the potential to reflect human perception accurately, they are hampered by their time-intensive and laborious nature, with outcomes that are often tainted by subjective bias. In this paper, we investigate the limitations inherent in existing metrics and introduce a novel evaluation pipeline, the Text-to-Video Score (T2VScore). This metric integrates two pivotal criteria: (1) Text-Video Alignment, which scrutinizes the fidelity of the video in representing the given text description, and (2) Video Quality, which evaluates the video's overall production caliber with a mixture of experts. Moreover, to evaluate the proposed metrics and facilitate future improvements on them, we present the TVGE dataset, collecting human judgments of 2,543 text-to-video generated videos on the two criteria. Experiments on the TVGE dataset demonstrate the superiority of the proposed T2VScore on offering a better metric for text-to-video generation.\n",
            "\n",
            "**Keywords:** generative models, text-to-video generation, evaluation metrics, video quality, text-video alignment\n",
            "# HexaGen3D: StableDiffusion is just one step away from Fast and Diverse Text-to-3D Generation\n",
            "\n",
            "**Abstract:** Despite the latest remarkable advances in generative modeling, efficient generation of high-quality 3D assets from textual prompts remains a difficult task. A key challenge lies in data scarcity: the most extensive 3D datasets encompass merely millions of assets, while their 2D counterparts contain billions of text-image pairs. To address this, we propose a novel approach which harnesses the power of large, pretrained 2D diffusion models. More specifically, our approach, HexaGen3D, fine-tunes a pretrained text-to-image model to jointly predict 6 orthographic projections and the corresponding latent triplane. We then decode these latents to generate a textured mesh. HexaGen3D does not require per-sample optimization, and can infer high-quality and diverse objects from textual prompts in 7 seconds, offering significantly better quality-to-latency trade-offs when comparing to existing approaches. Furthermore, HexaGen3D demonstrates strong"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-17 06:16:17.635 | INFO     | __main__:main:5 - DailyPaperWatcher: # InstantID: Zero-shot Identity-Preserving Generation in Seconds\n",
            "\n",
            "**Abstract:** There has been significant progress in personalized image synthesis with methods such as Textual Inversion, DreamBooth, and LoRA. Yet, their real-world applicability is hindered by high storage demands, lengthy fine-tuning processes, and the need for multiple reference images. Conversely, existing ID embedding-based methods, while requiring only a single forward inference, face challenges: they either necessitate extensive fine-tuning across numerous model parameters, lack compatibility with community pre-trained models, or fail to maintain high face fidelity. Addressing these limitations, we introduce InstantID, a powerful diffusion model-based solution. Our plug-and-play module adeptly handles image personalization in various styles using just a single facial image, while ensuring high fidelity. To achieve this, we design a novel IdentityNet by imposing strong semantic and weak spatial conditions, integrating facial and landmark images with textual prompts to steer the image generation. InstantID demonstrates exceptional performance and efficiency, proving highly beneficial in real-world applications where identity preservation is paramount. Moreover, our work seamlessly integrates with popular pre-trained text-to-image diffusion models like SD1.5 and SDXL, serving as an adaptable plugin. Our codes and pre-trained checkpoints will be available at [https://github.com/InstantID/InstantID](https://github.com/InstantID/InstantID).\n",
            "\n",
            "**Keywords:** personalized image synthesis, identity preservation, diffusion model, facial image, image generation\n",
            "# Quantum Denoising Diffusion Models\n",
            "\n",
            "**Abstract:** In recent years, machine learning models like DALL-E, Craiyon, and Stable Diffusion have gained significant attention for their ability to generate high-resolution images from concise descriptions. Concurrently, quantum computing is showing promising advances, especially with quantum machine learning which capitalizes on quantum mechanics to meet the increasing computational requirements of traditional machine learning algorithms. This paper explores the integration of quantum machine learning and variational quantum circuits to augment the efficacy of diffusion-based image generation models. Specifically, we address two challenges of classical diffusion models: their low sampling speed and the extensive parameter requirements. We introduce two quantum diffusion models and benchmark their capabilities against their classical counterparts using MNIST digits, Fashion MNIST, and CIFAR-10. Our models surpass the classical models with similar parameter counts in terms of performance metrics FID, SSIM, and PSNR. Moreover, we introduce a consistency model unitary single sampling architecture that combines the diffusion procedure into a single step, enabling a fast one-step image generation.\n",
            "\n",
            "**Keywords:** quantum machine learning, variational quantum circuits, diffusion-based image generation, quantum diffusion models, performance metrics\n",
            "# Towards A Better Metric for Text-to-Video Generation\n",
            "\n",
            "**Abstract:** Generative models have demonstrated remarkable capability in synthesizing high-quality text, images, and videos. For video generation, contemporary text-to-video models exhibit impressive capabilities, crafting visually stunning videos. Nonetheless, evaluating such videos poses significant challenges. Current research predominantly employs automated metrics such as FVD, IS, and CLIP Score. However, these metrics provide an incomplete analysis, particularly in the temporal assessment of video content, thus rendering them unreliable indicators of true video quality. Furthermore, while user studies have the potential to reflect human perception accurately, they are hampered by their time-intensive and laborious nature, with outcomes that are often tainted by subjective bias. In this paper, we investigate the limitations inherent in existing metrics and introduce a novel evaluation pipeline, the Text-to-Video Score (T2VScore). This metric integrates two pivotal criteria: (1) Text-Video Alignment, which scrutinizes the fidelity of the video in representing the given text description, and (2) Video Quality, which evaluates the video's overall production caliber with a mixture of experts. Moreover, to evaluate the proposed metrics and facilitate future improvements on them, we present the TVGE dataset, collecting human judgments of 2,543 text-to-video generated videos on the two criteria. Experiments on the TVGE dataset demonstrate the superiority of the proposed T2VScore on offering a better metric for text-to-video generation.\n",
            "\n",
            "**Keywords:** generative models, text-to-video generation, evaluation metrics, video quality, text-video alignment\n",
            "# HexaGen3D: StableDiffusion is just one step away from Fast and Diverse Text-to-3D Generation\n",
            "\n",
            "**Abstract:** Despite the latest remarkable advances in generative modeling, efficient generation of high-quality 3D assets from textual prompts remains a difficult task. A key challenge lies in data scarcity: the most extensive 3D datasets encompass merely millions of assets, while their 2D counterparts contain billions of text-image pairs. To address this, we propose a novel approach which harnesses the power of large, pretrained 2D diffusion models. More specifically, our approach, HexaGen3D, fine-tunes a pretrained text-to-image model to jointly predict 6 orthographic projections and the corresponding latent triplane. We then decode these latents to generate a textured mesh. HexaGen3D does not require per-sample optimization, and can infer high-quality and diverse objects from textual prompts in 7 seconds, offering significantly better quality-to-latency trade-offs when comparing to existing approaches. Furthermore, HexaGen3D demonstrates strong generalization to new objects or compositions.\n",
            "\n",
            "**Keywords:** generative modeling, 3D assets, text-to-3D generation, diffusion models, HexaGen3D\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " generalization to new objects or compositions.\n",
            "\n",
            "**Keywords:** generative modeling, 3D assets, text-to-3D generation, diffusion models, HexaGen3D\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M6vUzJwMdKNt"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trigger"
      ],
      "metadata": {
        "id": "23biLFK_T7Ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from aiocron import crontab\n",
        "from typing import Optional\n",
        "from pytz import BaseTzInfo\n",
        "from pydantic import BaseModel, Field\n",
        "from metagpt.schema import Message\n",
        "\n",
        "class DailyPaperInfo(BaseModel):\n",
        "    url: str\n",
        "    timestamp: float = Field(default_factory=time.time)\n",
        "\n",
        "\n",
        "\n",
        "class HuggingfaceDailyPaperCronTrigger():\n",
        "\n",
        "    def __init__(self, spec: str, tz: Optional[BaseTzInfo] = None, url: str = \"https://huggingface.co/papers\") -> None:\n",
        "        self.crontab = crontab(spec, tz=tz)\n",
        "        self.url = url\n",
        "\n",
        "    def __aiter__(self):\n",
        "        return self\n",
        "\n",
        "    async def __anext__(self):\n",
        "        await self.crontab.next()\n",
        "        return Message(self.url, DailyPaperInfo(url=self.url))\n"
      ],
      "metadata": {
        "id": "XJJ3A80uUWVP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dVsVCUCIUl0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Callback"
      ],
      "metadata": {
        "id": "n8k-cg5j8TfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Discord\n",
        "from google.colab import userdata\n",
        "\n",
        "TOKEN = userdata.get('DISCORD_TOKEN')\n",
        "CHANNEL_ID = userdata.get('DISCORD_CHANNEL_ID')"
      ],
      "metadata": {
        "id": "G_rBWBVC8UEk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# callback\n",
        "import os\n",
        "import discord\n",
        "async def discord_callback(msg: Message):\n",
        "    intents = discord.Intents.default()\n",
        "    intents.message_content = True\n",
        "    intents.members = True\n",
        "\n",
        "    client = discord.Client(intents=intents)\n",
        "    token = TOKEN\n",
        "    channel_id = int(CHANNEL_ID)\n",
        "\n",
        "    async with client:\n",
        "        await client.login(token)\n",
        "        channel = await client.fetch_channel(channel_id)\n",
        "        lines = []\n",
        "        for i in msg.content.splitlines():\n",
        "            if i.startswith((\"# \", \"## \", \"### \")):\n",
        "                if lines:\n",
        "                    await channel.send(\"\\n\".join(lines))\n",
        "                    lines = []\n",
        "            lines.append(i)\n",
        "\n",
        "        if lines:\n",
        "            await channel.send(\"\\n\".join(lines))"
      ],
      "metadata": {
        "id": "LWjA0Zon8qIq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dCzf1g3F8_ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "FljHE6qg9BXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from metagpt.subscription import SubscriptionRunner\n",
        "# 运行入口，\n",
        "async def main(spec: str = \"54 16 * * *\", discord: bool = True, wxpusher: bool = False):\n",
        "    callbacks = []\n",
        "    if discord:\n",
        "        callbacks.append(discord_callback)\n",
        "\n",
        "    if wxpusher:\n",
        "        callbacks.append(wxpusher_callback)\n",
        "\n",
        "    if not callbacks:\n",
        "        async def _print(msg: Message):\n",
        "            print(msg.content)\n",
        "        callbacks.append(_print)\n",
        "\n",
        "    async def callback(msg):\n",
        "        await asyncio.gather(*(call(msg) for call in callbacks))\n",
        "\n",
        "    runner = SubscriptionRunner()\n",
        "    await runner.subscribe(DailyPaperWatcher(), HuggingfaceDailyPaperCronTrigger(spec), callback)\n",
        "    await runner.run()"
      ],
      "metadata": {
        "id": "MFaDBk0I87KK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytz import timezone\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "current_time = datetime.now()\n",
        "target_time = current_time + timedelta(minutes=1)\n",
        "cron_expression = target_time.strftime('%M %H %d %m %w')\n",
        "print(cron_expression)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V53N81N79RAe",
        "outputId": "1358dd03-e6a2-4d9b-ee10-638de4155d49"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17 06 17 01 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "await main(cron_expression)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779
        },
        "id": "uIAuOp1L9TXI",
        "outputId": "de8f7b72-6ae4-4009-e021-0ef2d0111836"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-17 06:17:00.002 | INFO     | __main__:_act:44 - Huggy(DailyPaperWatcher): ready to CrawlHuggingfaceDailyPaper\n",
            "2024-01-17 06:17:01.171 | INFO     | __main__:_act:58 - Preparing to summarize 4 papers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# InstantID: Zero-shot Identity-Preserving Generation in Seconds\n",
            "\n",
            "**Abstract:** There has been significant progress in personalized image synthesis with methods such as Textual Inversion, DreamBooth, and LoRA. Yet, their real-world applicability is hindered by high storage demands, lengthy fine-tuning processes, and the need for multiple reference images. Conversely, existing ID embedding-based methods, while requiring only a single forward inference, face challenges: they either necessitate extensive fine-tuning across numerous model parameters, lack compatibility with community pre-trained models, or fail to maintain high face fidelity. Addressing these limitations, we introduce InstantID, a powerful diffusion model-based solution. Our plug-and-play module adeptly handles image personalization in various styles using just a single facial image, while ensuring high fidelity. To achieve this, we design a novel IdentityNet by imposing strong semantic and weak spatial conditions, integrating facial and landmark images with textual prompts to steer the image generation. InstantID demonstrates exceptional performance and efficiency, proving highly beneficial in real-world applications where identity preservation is paramount. Moreover, our work seamlessly integrates with popular pre-trained text-to-image diffusion models like SD1.5 and SDXL, serving as an adaptable plugin. Our codes and pre-trained checkpoints will be available at [https://github.com/InstantID/InstantID](https://github.com/InstantID/InstantID).\n",
            "\n",
            "**Keywords:** personalized image synthesis, ID embedding-based methods, diffusion model, image personalization, identity preservation\n",
            "# Towards A Better Metric for Text-to-Video Generation\n",
            "\n",
            "**Abstract:** Generative models have demonstrated remarkable capability in synthesizing high-quality text, images, and videos. For video generation, contemporary text-to-video models exhibit impressive capabilities, crafting visually stunning videos. Nonetheless, evaluating such videos poses significant challenges. Current research predominantly employs automated metrics such as FVD, IS, and CLIP Score. However, these metrics provide an incomplete analysis, particularly in the temporal assessment of video content, thus rendering them unreliable indicators of true video quality. Furthermore, while user studies have the potential to reflect human perception accurately, they are hampered by their time-intensive and laborious nature, with outcomes that are often tainted by subjective bias. \n",
            "\n",
            "In this paper, we investigate the limitations inherent in existing metrics and introduce a novel evaluation pipeline, the Text-to-Video Score (T2VScore). This metric integrates two pivotal criteria: (1) Text-Video Alignment, which scrutinizes the fidelity of the video in representing the given text description, and (2) Video Quality, which evaluates the video's overall production caliber with a mixture of experts. Moreover, to evaluate the proposed metrics and facilitate future improvements on them, we present the TVGE dataset, collecting human judgments of 2,543 text-to-video generated videos on the two criteria. Experiments on the TVGE dataset demonstrate the superiority of the proposed T2VScore on offering a better metric for text-to-video generation.\n",
            "\n",
            "**Keywords:** generative models, text-to-video generation, evaluation metrics, video quality, text-video alignment\n",
            "# Quantum Denoising Diffusion Models\n",
            "\n",
            "**Abstract:** In recent years, machine learning models like DALL-E, Craiyon, and Stable Diffusion have gained significant attention for their ability to generate high-resolution images from concise descriptions. Concurrently, quantum computing is showing promising advances, especially with quantum machine learning which capitalizes on quantum mechanics to meet the increasing computational requirements of traditional machine learning algorithms. This paper explores the integration of quantum machine learning and variational quantum circuits to augment the efficacy of diffusion-based image generation models. Specifically, we address two challenges of classical diffusion models: their low sampling speed and the extensive parameter requirements. We introduce two quantum diffusion models and benchmark their capabilities against their classical counterparts using MNIST digits, Fashion MNIST, and CIFAR-10. Our models surpass the classical models with similar parameter counts in terms of performance metrics FID, SSIM, and PSNR. Moreover, we introduce a consistency model unitary single sampling architecture that combines the diffusion procedure into a single step, enabling a fast one-step image generation.\n",
            "\n",
            "**Keywords:** Quantum machine learning, variational quantum circuits, diffusion-based image generation, quantum computing, performance metrics\n",
            "# HexaGen3D: StableDiffusion is just one step away from Fast and Diverse Text-to-3D Generation\n",
            "\n",
            "**Abstract:** Despite the latest remarkable advances in generative modeling, efficient generation of high-quality 3D assets from textual prompts remains a difficult task. A key challenge lies in data scarcity: the most extensive 3D datasets encompass merely millions of assets, while their 2D counterparts contain billions of text-image pairs. To address this, we propose a novel approach which harnesses the power of large, pretrained 2D diffusion models. More specifically, our approach, HexaGen3D, fine-tunes a pretrained text-to-image model to jointly predict 6 orthographic projections and the corresponding latent triplane. We then decode these latents to generate a textured mesh. HexaGen3D does not require per-sample optimization, and can infer high-quality and diverse objects from textual prompts in 7 seconds, offering significantly better quality-to-latency trade-offs when comparing to existing approaches. Furthermore, HexaGen3D demonstrates strong generalization to new objects or compositions.\n",
            "\n",
            "**Keywords:** generative modeling, 3D assets, textual prompts, data scarcity, pretrained models.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CancelledError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-5b28dced21a8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mawait\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcron_expression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-52185846b04d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(spec, discord, wxpusher)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSubscriptionRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mawait\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubscribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDailyPaperWatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHuggingfaceDailyPaperCronTrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mawait\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/metagpt/subscription.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, raise_exception)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36msleep\u001b[0;34m(delay, result)\u001b[0m\n\u001b[1;32m    603\u001b[0m                         future, result)\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCancelledError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lBBGDSL4_tCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vWREx52KEwZ5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}